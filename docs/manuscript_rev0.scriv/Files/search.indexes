<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="7266B95C-B76F-4EE4-81A3-7B72F1341DC5">
            <Title>Second fine-tuning round</Title>
        </Document>
        <Document ID="C9C0CDC2-1BC0-4397-B489-57919C672AC8">
            <Title>Deep_convolutional_neural_networks_for_pan-specifi</Title>
            <Text>Han and Kim BMC Bioinformatics (2017) 18:585 DOI 10.1186/s12859-017-1997-x
METHODOLOGY ARTICLE
Deep convolutional neural networks for pan-specific peptide-MHC class I binding prediction
Youngmahn Han1,2 and Dongsup Kim1*
Abstract
Open Access
     Background: Computational scanning of peptide candidates that bind to a specific major histocompatibility complex (MHC) can speed up the peptide-based vaccine development process and therefore various methods are being actively developed. Recently, machine-learning-based methods have generated successful results by training large amounts of experimental data. However, many machine learning-based methods are generally less sensitive in recognizing locally-clustered interactions, which can synergistically stabilize peptide binding. Deep convolutional neural network (DCNN) is a deep learning method inspired by visual recognition process of animal brain and it is known to be able to capture meaningful local patterns from 2D images. Once the peptide-MHC interactions can be encoded into image-like array(ILA) data, DCNN can be employed to build a predictive model for peptide-MHC binding prediction. In this study, we demonstrated that DCNN is able to not only reliably predict peptide-MHC binding, but also sensitively detect locally-clustered interactions.
Results: Nonapeptide-HLA-A and -B binding data were encoded into ILA data. A DCNN, as a pan-specific prediction model, was trained on the ILA data. The DCNN showed higher performance than other prediction tools for the latest benchmark datasets, which consist of 43 datasets for 15 HLA-A alleles and 25 datasets for 10 HLA-B alleles. In particular, the DCNN outperformed other tools for alleles belonging to the HLA-A3 supertype. The F1 scores of the DCNN were 0. 86, 0.94, and 0.67 for HLA-A*31:01, HLA-A*03:01, and HLA-A*68:01 alleles, respectively, which were significantly higher than those of other tools. We found that the DCNN was able to recognize locally-clustered interactions that could synergistically stabilize peptide binding. We developed ConvMHC, a web server to provide user-friendly web interfaces for peptide-MHC class I binding predictions using the DCNN. ConvMHC web server can be accessible via http:// jumong.kaist.ac.kr:8080/convmhc.
Conclusions: We developed a novel method for peptide-HLA-I binding predictions using DCNN trained on ILA data that encode peptide binding data and demonstrated the reliable performance of the DCNN in nonapeptide binding predictions through the independent evaluation on the latest IEDB benchmark datasets. Our approaches can be applied to characterize locally-clustered patterns in molecular interactions, such as protein/DNA, protein/RNA, and drug/protein interactions.
Keywords: T cell epitope prediction, Peptide-based vaccine development, Peptide-MHC class I binding prediction, Deep learning, Convolutional neural network
* Correspondence: kds@kaist.ac.kr
1Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
  
Han and Kim BMC Bioinformatics (2017) 18:585
Page 2 of 9
Background
Cytotoxic T lymphocytes (CTLs) play a key role in elimin- ating infections caused by intracellular pathogens. Since the CTL T-cell receptor recognizes foreign peptides in complex with major histocompatibility complex (MHC) molecules on the infected cell surface, the response of the host immune system to pathogens can be activated by peptide binding of MHC molecules. Determining peptides that bind specific MHC molecules is important for identi- fying T cell epitopes and can facilitate the development of peptide-based vaccines and design of immunotherapies. However, experimental identification of peptide-MHC is time-consuming and laborious; computer-assisted binding predictions can be a cost-effective and practical alternative and various methods have been developed [1].
Sette and Sidney grouped HLA class I (HLA-I) mole- cules into HLA supertypes using binding specificities char- acterized by the binding motifs of peptides [2]. Early peptide binding prediction methods were based on search- ing for allele-specific peptide binding motifs [3, 4]. As more experimental data became available, statistical methods have been developed using positional scoring matrixes that utilize amino acid occurrence frequencies at each position [5, 6]. Recently, more sophisticated machine learning methods [7–9] have generated the most successful results by training large amount of experimental data derived from public databases, such as the Immune Epitope Database [10]. Allele-specific machine learning methods generally achieve more accurate predictions as more data are learned for each HLA-I allele. A significant portion of currently available data was biased towards a limited number of common alleles [11], and this makes it difficult to predict peptide bindings for rare alleles. Sequence-based pan- specific methods have been proposed to overcome this problem and transfer the knowledge of other peptide- MHC binding information to improve the predictions for rare and even new alleles [12–14].
The pan-specific methods utilize information on not only the peptide sequence but also the MHC residues in peptide-MHC contact sites derived from the crystal struc- tures of peptide-MHC complexes. The contact sites are clustered around the peptide anchor positions and the binding pockets of MHC molecules [14–16]. The amino acids of a peptide interact with MHC molecules in com- pensatory and synergistic manner rather than independ- ently [17–19]. A large-scale structural simulation study of the peptide-MHC binding landscapes revealed statistically significant pairwise correlations in amino acid preferences at different positions of a peptide [15]. Many machine learning-based methods have a risk of learning the features associated with amino acids of peptide and the HLA-I mol- ecule independently. Therefore, they could be less sensitive in recognizing the locally-clustered interactions, which could synergistically produce peptide-HLA-I binding.
Deep convolutional neural network (DCNN) is a branch of deep learning methods that extract and learn high-level representations (features or patterns) from the low-level raw data through nonlinear transformations of multiple layers. It was originally designed to process the spatial and temporal data, particularly two-dimensional images with multiple color channels. DCNNs are inspired by the ani- mal visual cortex and imitate cognitive functions of the cortex using three key concepts: capturing local motifs of highly connected pixels, invariance to the motif location, and hierarchical composition of the local motifs [20]. DCNNs have achieved successful results in many object recognition and detection tasks [21–23]. Recent studies have proposed bioinformatics applications of DCNNs in- cluding protein contact predictions [24] and small mol- ecule bioactivity predictions [25, 26].
In this study, we propose a novel method for pan- specific peptide-HLA-I binding prediction using DCNN. The peptide-HLA-I binding structure can be encoded into two-dimensional image-like array (ILA) data. A contact site between the peptide and MHC molecule is corre- sponded to a “pixel” of the ILA data. For each “pixel”, physicochemical property values of the amino acid pair at the contact site are assigned to its channels. The locally- clustered contact sites at peptide anchor positions and binding pockets of the HLA-I molecule form local motifs on the ILA data, which can be captured by DCNN. The resultant multi-channel ILA data were used to train the DCNN for peptide-HLA-I binding prediction. The DCNN showed a reliable performance for the independent bench- mark datasets. In particular, we report that the DCNN sig- nificantly outperformed other tools in peptide binding predictions for alleles belonging to the HLA-A3 supertype. We also highlight the ability of DCNN to recognize the locally-clustered interactions in three peptides that bind to HLA-I molecules in synergistic manner.
Methods
Figure 1 shows the schematic representation of overall training process of our DCNN. Each peptide binding in- formation was encoded into ILA. The DCNN extracts low-level features from the ILA and combines them into high-level features(motifs) through multiple convolu- tional and pooling layers. The DCNN learns these high- level features to be used for classifying the ILA into binder or non-binder through fully connected layers.
Training datasets
For benchmark with other tools, including NetMHCPan [14], SMM [5], ANN [7], and PickPocket [6], we used the same training dataset used in these tools. The data- set was compiled from three sources (the IEDB and the Sette and Buus laboratories) contained BD2009 and BD2013 data from [27] and additional binding data,

Han and Kim BMC Bioinformatics (2017) 18:585
Page 3 of 9
                     Fig. 1 Schematic representation of overall training process of the DCNN. An ILA is converted from peptide binding information of training dataset. The DCNN extracts low-level features from the ILA and combines them into high-level features(motifs) through multiple convolutional and pooling layers. The DCNN learns these high-level features to be used for classifying the input ILA into binder or non-binder through fully connected layers
 which can be downloaded from the IEDB website (http://tools.iedb.org/mhci/download/). We used nona- peptide binding data for HLA-A and -B to generate a pan-specific prediction model. For the binary classifica- tion of peptide binding affinities, peptides with a half- maximal inhibitory concentration (IC50) value of less than 500 nM were designated as binders. In total, the training dataset consisted of 118,174 binding data cover- ing 76 alleles: 37 HLA-A (72,551) and 39 HLA-B
(45,623). Additional file 1: Table S1 shows the detailed description of the training dataset.
Encoding peptide binding data into ILA data
As depicted in Fig. 2, a peptide binding structure can be encoded into a width (W) × height (H) ILA with C chan- nels. The ILA width and height were the number of con- tact residue of the HLA molecule and the number of amino acids of the peptide, respectively. A contact site
            Fig. 2 Encoding a peptide binding structure into an ILA. The left panel shows the nonapeptide (green)-HLA-A*02:01 (magenta) complex (PDB entry 1qsf). HLA residues at contact sites are depicted in cyan. The right panel shows the ILA data. The ILA width and height were the number of contact residue of the HLA molecule and the number of amino acids of the peptide, respectively. A contact site between the peptide and MHC molecule is corresponded to a “pixel” of the ILA. For each “pixel”, physicochemical property values of the amino acid pair at the contact site are assigned to its channels
 
Han and Kim BMC Bioinformatics (2017) 18:585
Page 4 of 9
between the peptide and MHC molecule is corre- sponded to a “pixel” of the ILA. For each “pixel”, physi- cochemical property values of the amino acid pair at the contact site are assigned to its channels. We used 9 physicochemical scores out of 11 physicochemical scores suggested by [28] excluding two highly correlated scores (pairwise correlation, R2&gt;0.8) as the physicochemical property values of an amino acid; the channel size C is 18, the sum of the number of physiochemical scores of the amino acid pair at the contact site.
We used 34 HLA-I contact residues proposed by NetMHCPan [14]. Consequently, the nonapeptide-HLA- I binding data were encoded into ILA data with the di- mension of 34 (width) × 9 (height) with 18 channels.
Constructing and training the DCNN
As shown in Fig. 3, our DCNN architecture is closely based on of the popular DCNN architecture proposed by Simonyan and Zisserma [23], which uses very small filters for capturing fine details of images and allows more elab- orate data transformations through increased depth of the network. We concatenated three convolution blocks with two convolution layers and a max pooling layer, and then connected three dense layers to the ends of the network. In all convolution layers, convolution filters of 3 × 3 were used, and the numbers of filters for the convolution blocks were 32, 64, and 128, respectively. In order to avoid overfitting, we applied the dropout [29] acting as a regularization next to each convolution block. The ReLU [30] activation function was used for nonlinear transform- ation of the output value of each convolution layer. We used the Adam optimizer [31] with learning rate 0.001 for 200 epochs.
The DCNN was trained on the ILA data converted from 118,174 binding data covering 76 HLA-I alleles. In order to prevent the DCNN from overfitting the training data, the DCNN training was performed using leave-one-out and 5-fold cross-validations. The ILA data were split into 76 allele subsets in leave-one-out cross-validation and 5 equal sized subsets in 5-fold cross-validation, respectively. For each cross-validation round, a single subset was retained as the validation data for testing the DCNN, and the remaining subsets were used as training data. The cross-validation was repeated for the number of subsets: i.e., 76 times in leave-one-out cross-validation and 5 times in 5-fold cross-validation, with each subset used exactly once as the validation data. In a single cross-validation round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were mea- sured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs [32]. We implemented the DCNN using Keras library(https:// github.com/fchollet/keras).
Independent evaluation of the DCNN
Trolle et al. [33] developed a framework for automatic- ally benchmarking the performance of peptide-MHC binding prediction tools. Based on this framework, the IEDB has evaluated the performance of participating prediction tools on IEDB experimental datasets, which are updated weekly, and published the results via the website (http://tools.iedb.org/auto_bench/mhci/weekly/). We performed a blind test of the DCNN using the latest experimental IEDB data accumulated since March 21, 2014. The accumulated data were grouped by IEDB ref- erences, alleles, and measurement types and split into 68 test subsets consisting of 43 subsets for 15 HLA-A al- leles and 25 subsets for 10 HLA-B alleles (Additional file 2: Table S4). We performed the benchmark with other participating tools, including NetMHCPan, SMM, ANN, and PickPocket, for each subset. For the reliable bench- mark, we used the latest standalone version of the pre- diction tools downloaded from the IEDB website (http://
                                 Fig. 3 The DCNN architecture. The DCNN architecture is closely based on of the popular DCNN architecture proposed by Simonyan and Zisserman. Three convolution blocks with two convolution layers and a max pooling layer are concatenated, and three classification layers are then connected to the ends of the network. The dropout was next applied to each convolution block as a regularization. ReLU was used for the nonlinear transformation of the output value of each convolution layer
 
Han and Kim BMC Bioinformatics (2017) 18:585
Page 5 of 9
tools.iedb.org/mhci/download/), which were trained on the same training data as that of our DCNN. The F1 score, the harmonic mean of precision and recall, was used to quantify the prediction performance, where an F1 score reaches its best value at 1 and worst value at 0. The F1 score is defined as:
F1 1⁄4 2 􏰀 precision 􏰀 recall ; precision þ recall
Table 1 Summary of cross-validation results Average accuracy
Leave-one-out 0.855 5-fold 0.892
Independent evaluation of the DCNN
Average loss 0.318
0.254
      TP ; TP þ FP
;
false positives, and false negatives, respectively.
Identifying informative pixels recognized by the DCNN
In order to find locally-clustered interactions, informative pixels captured by the DCNN on the ILA classified as a binder were investigated. This was enabled due to the development of several recent methods that identify informative pixels of DCNN inputs, including Deconvnet [34], guided backpropagation [35], and DeepLIFT [36]. The informative pixels were found by using high- resolution DeepLIFT method in this study.
Results and discussion
Training results
In order to compare the prediction performance of the DCNN and other prediction methods, the DCNN was trained on the dataset that was used in other tools. The 118,174 nonapeptide-HLA-I binding data for 76 HLA-A alleles (72,551) and 37 HLA-B alleles (45,623) were encoded into the two-dimensional ILA data. The pre- dictive performance was evaluated with leave-one-out and 5-fold cross-validation approaches. DCNN models were trained up to 200 epochs with early stopping con- dition. The mean validation losses were 0.318 in leave- one-out and 0.254 in 5-fold cross-validation, and the mean validation accuracies were 0.855 and 0.892, re- spectively (Table 1), and this indicate that our DCNN was able to be generally trained on the ILA data without much overfitting problems. Additional file 3: Table S2 and Additional file 4: Table S3 show the detailed cross- validation results.
We performed a blind test of the DCNN using the latest IEDB experimental data accumulated since March 21, 2014. The data were grouped by IEDB references, alleles, and measurement types and split into 68 test subsets con- sisting of 43 subsets for 15 HLA-A alleles and 25 subsets for 10 HLA-B alleles. For each subset, the prediction per- formances of other prediction tools, including NetMHC- Pan, SMM, ANN, and PickPocket, were measured. The F1 scores were used to quantify their predictive perfor- mances. Table 2A and 2B summarize the prediction re- sults for HLA-A and HLA-B test subsets, respectively, and Additional file 2: Table S4 shows the detailed predic- tion results. The mean and median of the F1 scores of the DCNN were 0.638 and 0.696, respectively; these values were slightly higher than those of other tools, suggesting that the DCNN was more reliable in nonapeptide-HLA-A binding predictions (Table 2A). The mean of the F1 scores of the DCNN was 0.593, which was almost the same as those of other tools; however, the median was 0.667, which was higher than that of the other tools, indicating that the DCNN was also reliable in nonapeptide-HLA-B binding predictions (Table 2B).
In particular, our DCNN showed significantly higher pre- diction performance than other prediction tools for the subsets for HLA-A*31:01, HLA-A*03:01, and HLA-A*68:01 alleles belonging to the HLA-A3 supertype (Table 3).
The HLA-A3 supertype were known to have import- ant locally-clustered interactions that synergistically sta- bilizes the peptide-MHC complexes [26]. We thus investigated whether the trained DCNN was captured this features by inspecting its informative sites or pixels for three peptide-MHC complex pairs that were cor- rectly predicted by our method but were failed in other
precision 1⁄4
 TP TP þ FN
recall 1⁄4
where TP, FP, and FN are the numbers of true positives,
 Table 2 Prediction results for HLA-I test subsets
(A) Summary of prediction results for 43 HLA-A test subsets
DCNN NetMHCPan
PickPocket 0.561 0.625 0.318
PickPocket 0.560 0.593 0.277
 Mean 0.638
Median 0.696
Standard Deviation 0.230
(B) Summary of prediction results for 25 HLA-B test subsets
Mean 0.593 Median 0.667 Standard Deviation 0.286
0.606 0.625 0.286
0.608 0.667 0.267
DCNN NetMHCPan
SMM ANN 0.578 0.606 0.615 0.643 0.302 0.290
SMM ANN 0.601 0.579 0.667 0.667 0.250 0.286
 
Han and Kim BMC Bioinformatics (2017) 18:585
Page 6 of 9
Table 3 Prediction results for HLA-A*31:01, HLA-A*03:01, and HLA-A*68:01 alleles
 IEDB ID
315312 1031253 1026840 1026840
Allele
HLA-A*31:01 HLA-A*03:01 HLA-A*68:01 HLA-A*68:01 Mean
Meas. Type
binary ic50 binary ic50
DCNN NetMHCPan
0.857 0.667 0.941 0.875 0.340 0.275 0.667 0.600 0.701 0.604
SMM ANN
0.571 0.667 0.667 0.875 0.456 0.208 0.583 0.444 0.569 0.549
by the DCNN to identify
positions 1, 2, and 3. Fig.
pixels with higher red intensities (red and blue intensities indicated the degree of contribution to the binder and non-binder, respectively) were dominant and locally- clustered at the positions 1, 2, and 3, whereas the inform- ative pixels with higher blue intensities were located at position 9. These findings were consistent with the fact that the locally-clustered patterns recognized by the
PickPocket
0.400 0.941 0.045 0.143 0.382
      methods: KVFGPIHEL for HLA-A*31:01, RAAPPPPPR for HLA-A*03:01, and LPQWLSANR for HLA-A*68:01. In KVFGPIHEL-HLA-A*31:01, the amino acids K, V,
and F of the peptide were preferred at the primary and second anchor positions 1, 2, and 3, respectively, but the nonpolar and hydrophobic L was deleterious at the pri- mary anchor position 9, and the charged H was tolerated at the secondary anchor position 7. We investigated the informative pixels on the transformed ILA data captured
the locally-clustered motifs at 4a shows that the informative
    Fig. 4 Informative pixels on the ILA data. (a) In KVFGPIHEL-HLA-A*31:01, the informative pixels with higher red intensities (red and blue intensities indicated the degree of contribution to the binder and non-binder, respectively) were dominant and locally-clustered at the positions 1, 2, and 3 (b) In RAAPPPPPR-HLA-A*03:01, informative pixels with higher red intensities were dominant and locally-clustered at the peptide positions 1 and 2 (c) In LPQWLSANR-HLA-A*68:01, informative pixels with red intensities were slightly dominant at positions 4, 5, and 6 and at the primary anchor position 9, with clustering at position 9
 
Han and Kim BMC Bioinformatics (2017) 18:585
Page 7 of 9
DCNN were informative when the KVFGPIHEL was clas- sified as a binder.
In RAAPPPPPR-HLA-A*03:01, the positively charged amino acid R of the peptide was preferred at the secondary anchor position 1, but the amino acids A, and R at the pri- mary and secondary anchor positions 2, 3, and 9, respect- ively, were tolerated. Considering binding contributions of the individual amino acids at the primary and secondary anchor positions, the peptide could not be a binder. Fig. 4b shows that the informative pixels with higher red intensities were dominant and locally-clustered at the peptide posi- tions 1 and 2, thus suggesting that the locally-clustered in- teractions between the amino acids at the peptide positions could produce stable binding together.
In LPQWLSANR-HLA-A*68:01, the positively charged R of the peptide was preferred at the primary anchor pos- ition 9, but the L, P, and Q were not preferred at the pri- mary and secondary anchor positions 1, 2, and 3, respectively. The amino acids at positions 4, 5, 6, and 7 were tolerated. As shown in Fig. 4c, informative pixels with red intensities were slightly dominant at positions 4, 5, and 6 and at the primary anchor position 9, with clustering at position 9, thus indicating that amino acids at positions 4, 5, 6, and 9 synergistically induced stable binding.
We found that our DCNN was able to correctly predict the three binder peptides KVFGPIHEL, RAAPPPPPR, and LPQWLSANR with preferred amino acids only at some primary and secondary anchor positions but with
amino acids that could synergistically induce stable bind- ing. This small number of cases are insufficient to support the general higher prediction performance of DCNN ap- proach for the HLA-A3 supertype, but these cases provide the possibilities that the DCNN can capture the locally- clustered interaction patterns in the peptide-HLA-A3 binding structures, which cannot be easily captured by other methods.
Web server
We developed ConvMHC(http://jumong.kaist.ac.kr:8080/ convmhc), a web server to provide user-friendly web inter- faces for peptide-MHC class I binding predictions using our DCNN. The main web interface consists of the input form panel (left) and the result list panel (right) as shown in Fig. 5. Users can submit multiple peptide sequences and a HLA-I allele in the input form panel. Once the pre- diction process is completed, the user can see the predic- tion results of the input peptides in the result list panel. For each prediction result, the user can also identify the informative pixels captured by the DCNN on the ILA data through a pop-up panel.
Conclusions
In this study, we developed a novel method for pan-specific peptide-HLA-I binding prediction using DCNN trained on ILA data that were converted from experimental binding data and demonstrated the reliable performance of the
        Fig. 5 ConMHC Web Server. The main web interface of ConvMHC consists of the input form panel (left) and the result list panel (right). Users can submit multiple peptide sequences and a HLA-I allele in the input form panel. Once the prediction process is completed, the user can see the prediction results of the input peptides in the result list panel. For each prediction result, the user can also identify the informative pixels captured by the DCNN on the transformed binding ILA data through a pop-up panel
 
Han and Kim BMC Bioinformatics (2017) 18:585
Page 8 of 9
DCNN in nonapeptide binding predictions through the in- dependent evaluation on IEDB external datasets. In particu- lar, the DCNN significantly outperformed other tools in peptide binding predictions for alleles belonging to the HLA-A3 supertype. By investigating the informative pixels captured by the DCNN on the ILA data converted from the binder nonapeptides that were predicted correctly by the DCNN but were failed in other methods, we found that the DCNN was better able to capture locally-clustered interac- tions that could synergistically produce stable binding in the peptide-HLA-A3 complexes: KVFGPIHEL-HLA-A*31:01, RAAPPPPPR-HLA-A*03:01, and LPQWLSANR-HLA- A*68:01.
We anticipate that our DCNN would become more reli- able in peptide binding predictions for HLA-A3 alleles through further training and evaluations on more experi- mental data. DCNNs for MHC class II will be generated and evaluated in further studies. Moreover, our approaches described herein will be useful for identifying locally- clustered patterns in molecular binding structures, such as protein/DNA, protein/RNA, and drug/protein interactions. However, it is not easy to build a reliable prediction model using DCNNs because deep learning tasks require large amounts of training data to extract high-level and general- ized representations from the data. Currently, in order to overcome the limited training data, state-of-the-art learning technologies, such as generative adversarial nets [37] and transfer learning [38] are attracting attentions. These tech- nologies can be effectively applied to generate more reliable binding prediction models.
Additional files
Abbreviations
DCNN: Deep Convolutional Neural Network; HLA: Human Leukocyte Antigen, the human version of MHC; ILA: Image-Like Array; MHC: Major Histocompatibility Complex
Acknowledgements
The authors would like to thank Dr. S. Hong for helpful discussions and comments.
Funding
This work was supported by the Bio &amp; Medical Technology Development Program of the NRF funded by the Korean government, MSIP(2016M3A9B6915714), the National Research Council of Science &amp; Technology (NST) grant by the Korea government (MSIP) (No. CRC-16- 01-KRICT) and the KAIST Future Systems Healthcare Project funded by the Korea government(MSIP).
Availability of data and materials
ConvMHC web server can be accessible via http://jumong.kaist.ac.kr:8080/ convmhc. Python source codes and all the datasets supporting this work can be downloaded from https://github.com/ihansyou/convmhc.
Authors’ contributions
YH designed the method, conducted the experiments, and wrote the manuscript. DK gave research ideas and supervised this project. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Author details
1Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea. 2Department of Convergence Technology Research, Korea Institute of Science and Technology Information, Daejeon, Republic of Korea.
Received: 17 September 2017 Accepted: 12 December 2017
References
1. Lundegaard C, Hoof I, Lund O. State of the art and challenges in sequence based T-cell epitope prediction. Immunome Research. 2010;6(Suppl 2):S3.
2. Sette A, Sidney J. Nine major HLA class I supertypes account for the vast preponderance of HLA-A and-B polymorphism. Immunogenetics. 1999;50:3–4.
3. Rötzschke O, Falk K, Stevanović S. Exact prediction of a natural T cell epitope. Eur J Immunol. 1991;21:2891–4.
4. Sette A, et al. Prediction of major histocompatibility complex binding regions of protein antigens by sequence pattern analysis. Proc Natl Acad Sci U S A. 1989;86:3296–300.
5. Peters B, Tong W, Sidney J, Sette A, Weng Z. Generating quantitative models describing the sequence specificity of biological processes with the stabilized matrix method. BMC Bioinformatics. 2005;6:132.
6. Zhang H, Lund O, Nielsen M. The PickPocket method for predicting binding specificities for receptors based on receptor pocket similarities: application to MHC-peptide binding. Bioinformatics. 2009;25:1293–9.
7. Nielsen M, Lundegaard C, Worning P, Lauemøller SL, Lamberth K, Buus S, et al. Reliable prediction of T-cell epitopes using neural networks with novel sequence representations. Protein Sci. 2003;12(5):1007–17.
8. Andreatta M, Nielsen M. Gapped sequence alignment using artificial neural networks: application to the MHC class I system. Bioinformatics. 2016;32:511–7.
9. Saethang T, Hirose O, Kimkong I, Tran VA, Dang XT, Nguyen L, et al. EpicCapo: epitope prediction using combined information of amino acid pairwise contact potentials and HLA-peptide contact site information. BMC bioinformatics. 2012;13(1):313.
10. Vita R, Overton JA, Greenbaum JA, Ponomarenko J, Clark JD, Cantrell JR, et al. The immune epitope database (IEDB) 3.0. Nucleic Acids Res. 2014;43(D1): 405–12.
11. Sette A, Fleri W, Peters B, Sathiamurthy M, Bui HH, Wilson S. A roadmap for the immunomics of category A–C pathogens. Immunity. 2005;22(2):155–61.
12. Zhang GL, Khan AM, Srinivasan KN, August JT, Brusic V. MULTIPRED: a computational system for prediction of promiscuous HLA binding peptides. Nucleic Acids Research. 2005;33(suppl_2):172–9.
13. Jojic N, Reyes-Gomez M, Heckerman D, Kadie C, Schueler-Furman O. Learning MHC I—peptide binding. Bioinformatics. 2006;22(14):227–35.
14. Nielsen M, Lundegaard C, Blicher T, Lamberth K, Harndahl M, Justesen S, et al. NetMHCpan, a method for quantitative predictions of peptide binding to any HLA-A and-B locus protein of known sequence. PLoS One. 2007;2(8):e796.
    Additional file 1: Table S1. Detailed description of the training dataset. (XLSX 16 kb)
Additional file 2: Table S4. Detailed prediction results for the IEDB HLA-I benchmark datasets. (XLSX 18 kb)
Additional file 3: Table S2. Detailed results for leave-one-out cross- validation. (XLSX 16 kb)
Additional file 4: Table S3. Detailed results for 5-fold cross-validation. (XLSX 11 kb)
 
Han and Kim BMC Bioinformatics (2017) 18:585
Page 9 of 9
15. Yanover C, Bradley P. Large-scale characterization of peptide-MHC binding landscapes with structural simulations. Proc Natl Acad Sci. 2011;108(17):6981–6.
16. Ehrenmann F, Kaas Q, Lefranc MP. IMGT/3Dstructure-DB and IMGT/ DomainGapAlign: a database and a tool for immunoglobulins or antibodies, T cell receptors, MHC, IgSF and MhcSF. Nucleic Acids Research. 2009; 38(suppl_1):301–7.
17. Guan P, Doytchinova IA, Flower DR. HLA-A3 supermotif defined by quantitative structure–activity relationship analysis. Protein Eng. 2003;16(1):11–8.
18. DiBrino M, Parker KC, Shiloach J, Knierman M, Lukszo J, Turner, et al. Endogenous peptides bound to HLA-A3 possess a specific combination of anchor residues that permit identification of potential antigenic peptides. Proc. Natl. Acad. Sci. 1993;90(4):1508–12.
19. Sidney J, Grey HM, Southwood S, Celis E, Wentworth PA, del Guercio MF, et al. Definition of an HLA-A3-like supermotif demonstrates the overlapping peptide-binding repertoires of common HLA molecules. Hum Immunol. 1996;45(2):79–93.
20. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.
21. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep
convolutional neural networks. In: Advances in neural information
processing systems; 2012. p. 1097-105
22. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, et al. Going
deeper with convolutions. In: Proceedings of the IEEE conference on
computer vision and pattern recognition; 2015. p. 1–9.
23. Simonyan, K., &amp; Zisserman, A. Very deep convolutional networks for large-
scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
24. Wang S, Sun S, Li Z, Zhang R, Xu J. Accurate de novo prediction of
protein contact map by ultra-deep learning model. PLoS Comput Biol.
2017;13(1):e1005324.
25. Wallach, I., Dzamba, M., &amp; Heifets, A. AtomNet: a deep convolutional neural
network for bioactivity prediction in structure-based drug discovery. arXiv
preprint arXiv:1510.02855, 2015.
26. Ragoza M, Hochuli J, Idrobo E, Sunseri J, Koes DR. Protein–Ligand scoring
with Convolutional neural networks. J Chem Inf Model. 2017;57(4):942–57.
27. Kim Y, Sidney J, Buus S, Sette A, Nielsen M, Peters B. Dataset size and
composition impact the reliability of performance benchmarks for peptide-
MHC binding predictions. BMC Bioinformatics. 2014;15(1):241.
28. Liu W, Meng X, Xu Q, Flower DR, Li T. Quantitative prediction of mouse
class I MHC peptide binding affinity using support vector machine
regression (SVR) models. BMC Bioinformatics. 2006;7(1):182.
29. Srivastava N, Hinton GE, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout:
a simple way to prevent neural networks from overfitting. J Mach Learn Res.
2014;15(1):1929–58.
30. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann
machines. In: Proceedings of the 27th international conference on machine
learning (ICML-10); 2010. p. 807–14.
31. Kingma, D., &amp; Ba, J. Adam: A method for stochastic optimization.arXiv
preprint arXiv:1412.6980. 2014.
32. Prechelt L. Early stopping—but when? In: Neural networks: tricks of the
trade. Berlin Heidelberg: Springer; 2012. p. 53–67.
33. Trolle T, Metushi IG, Greenbaum JA, Kim Y, Sidney J, et al. Automated
benchmarking of peptide-MHC class I binding predictions. Bioinformatics.
2015;31(13):2174–81.
34. Zeiler MD, Fergus R. Visualizing and understanding convolutional networks.
In: European conference on computer vision; 2014. p. 818-33.
35. Springenberg, J. T., Dosovitskiy, A., Brox, T., &amp; Riedmiller, M. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
36. Shrikumar, A., Greenside, P., Shcherbina, A., &amp; Kundaje, A. Not just a black
box: Learning important features through propagating activation
differences. arXiv preprint arXiv:1605.01713, 2016.
37. Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets. In:
Advances in neural information processing systems; 2014. p. 2672–80.
38. Greenspan H, van Ginneken B, Summers RM. Guest editorial deep learning in medical imaging: overview and future promise of an exciting new
technique. IEEE Trans Med Imaging. 2016l;35(5):1153–9.
 Submit your next manuscript to BioMed Central and we will help you at every step:
• We accept pre-submission inquiries
• Our selector tool helps you to find the most relevant journal • We provide round the clock customer support
• Convenient online submission
• Thorough peer review
• Inclusion in PubMed and all major indexing services
• Maximum visibility for your research
Submit your manuscript at www.biomedcentral.com/submit
</Text>
        </Document>
        <Document ID="ECE93083-6494-448F-9274-7E28C927375D">
            <Title>Introduction</Title>
        </Document>
        <Document ID="B6C91DB2-8FC4-45CC-8695-8AC6F5F8DD23">
            <Title>전체 학습과정</Title>
            <Synopsis>￼
</Synopsis>
            <Text>#


2) BA 데이터셋을 semi-supervised하게 pretraining 하여 펩타이드-MHC 결합에서의 아미노산 상호작용 패턴에 대한contextual language model을 구축한다. 이 때 언어모델의 범용성(generality)을 위해 입력  펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 삭제 또는 치환하여 선행학습을 수행한다. 펩타이드와 MHC 서열에서의 아미노산 삭제 또는 치환은 임의로 특정 비율의 서열 단편에 대해 수행하되 펩타이드 서열의 경우는 알려진 위치특이적 점수행렬(PSSM: Position-Specific Scoring Matrix)에 기반한 편향성을 주고, MHC 서열의 경우는 MHC 결합사이트의 진화적 보존성에 기반한 편향성을 부여한다. 
3) The pre-trained model을 특정 MS-identified natural ligand 데이터셋을 사용한 fine-tuning을 통해 최종 (Neo-) 펩타이드 예측 모델을 구축하고 cross-validation을 통해 성능을 검증한다.</Text>
        </Document>
        <Document ID="1AA48FE0-2A82-491D-84F8-53642D499202">
            <Title>BERTMHC</Title>
        </Document>
        <Document ID="1CDDBE2E-EEDC-4CC8-9ECF-DE5FE35DAE02">
            <Title>Pre-training the initial model</Title>
            <Text>For pre-training the initial TAPE model, the positive dataset containing epitope-specific TCR CDR3beta sequences was compiled from three data sources on May 2021: Dash et al[{Dash:2017go}] containing epitope-specific paired TCRα and TCRβ chains for  three epitopes from humans and for seven epitopes from mice, two manually curated databases that contains pathology-associated TCR sequences, such as VDJdb[{Bagaev:2019hf}](https://vdjdb.cdr3.net) and McPAS-TCR[{Tickotsky:2017bo}](http://friedmanlab.weizmann.ac.il/McPAS-TCR/).  Every entry in VDJdb has been given a confidence score between 0  and 3 (0: critical information missing, 1: medium confidence, 2: high confidence, 3: very high  confidence). We selected all VDJdb entries with a confidence score at least 1. After selecting all epitopes that have at least 20 CDR3beta sequences and removing duplicates with the same {epitope, CDR3beta} from the positive dataset,  the dataset contained 12,619 positive data points covering 80 epitopes. To increase the specificity of our model, it was necessary to add more epitope-specific CDR3b sequence data to training dataset as negative examples which were expected to interact between TCRs and epitopes. The background CDR3b sequences were obtained from Howie at al., who have collected blood from two healthy donors. A negative example generated by combining the epitopes from the positive dataset and randomly selected TCR CDR3beta sequences derived from two healthy donors[{Howie:2015}]. Table 1 summarizes the epitope-specific CDR3beta sequence data for the pre-training dataset(see Table S1 for the pre-training dataset in detail). 
The initial TAPE model was trained while freezing the embedding layer and top two encoding layers, where the weights of the layers were not updated. The freezing layers was extended to top six encoding layers in the next fine-tuning tasks. The training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs[{Prechelt:2012 }].  We used the Adam optimizer[{Kingma D:2014}] with learning rate 0.0001 and 128 batch size in all epochs. PyTorch deep learning library(https://pytorch.org) were used for implementing our models.</Text>
        </Document>
        <Document ID="AF59BA5F-BC3F-48C5-8575-E3638F32A08C">
            <Title>MS-based immunopeptidome studies</Title>
            <Text>Abelin, Bassni의 MS-based direct identification of presented peptides

Thus, identification of MHC-bound peptides by MS holds great promise for the generation of large-scale data sets characterizing the peptidome specific for individual MHC molecules (15, 17) and potentially for the identification of T cell epitopes (18). However, it is clear that, within the foreseeable future, the number of MHC molecules characterized by such MS studies will remain limited. In this context, significant efforts over the last decades have been dedicated to experimentally characterizing the peptide-binding space of MHC molecules using semi–high-throughput MHC– peptide BA assays (19, 20), enabling binding-specificity characterization of a large set of MHC molecules from different species. 

MS 방법의 단점들
데이터 수의 한계,allele coverage가 낮다
시간과 비용 cost?

Deconvolution for assigning HLA allele
a key obstacle is the ambiguity that arises from the co-expression of multiple HLA alleles. 

The majority of LC-MS/MS studies of the HLA peptidome have used cells expressing multiple HLA molecules, which requires peptides to be assigned to one of up to six class I alleles through the use of pre-existing bioinformatics predictors, or ‘‘deconvolu- tion’’ (Bassani-Sternberg and Gfeller, 2016). Thus, peptides that do not closely match known motifs cannot confidently be reported as binders to a given HLA allele. By contrast, we used a rapid approach to generate a high-quality LC-MS/MS dataset of &gt;24,000 endogenous peptides whose assignment to specific HLA alleles was unambiguous. Because we knew the allele assignment a priori, we greatly enhanced our analyses depth. 
 
However, historically liquid chromatography-tandem mass spectrometry (LC- MS/MS) methods have required large cellular input, which limits throughput, and the multi-allelic nature of the data complicates productive motif learning. 

the potential impact of any biases associated with ligands identified by MS, such as depletion of cysteines 
We note that certain biases (Klont et al., 2018; Mahoney et al., 2011) are known to exist in the sequences identified from mass spectrometry, which could limit the power of computational models trained on such datasets. 

The predicted MS observability of the HLA peptides and frequencies of individual amino acids between MS and IEDB peptides were highly similar, aside from underrepresentation of cysteine (Figures 1E and 1F). Free cysteine, which interferes with precursor fragmentation during LC-MS/MS, is underrepre- sented in other MS-based HLA-peptide datasets (Bassani-Stern- berg et al., 2015; Trolle et al., 2016) (Figure S1F). We recovered cysteine-containing peptides when a third round of database search accounted for cysteinylation (Table S1C). </Text>
        </Document>
        <Document ID="D1ED545E-FA3B-4A3F-A4D1-3F1F4E6E6E13">
            <Title>적응면역시스템에서의 T-cell의 중요성</Title>
            <Text>대부분의 백신 후보 물질 개발은 중화항체를 생성하는 B cell에 촛점을 맞추고 있지만, 기본적인 접근법은 T cell을 기반으로 하고 있다. 혈액에서 순환하는 T 세포는 종종 증상이 나타나기 전에 감염된 세포를 탐지하고 면역 반응을 일으키거나 감염된 세포를 직접 제거하는 적응 면역 체계의 핵심적인 역할을 담담한다. 따라서, 암 면역치료와 효과적인 백신 개발에 있어 T cell 면역 반응을 효과적으로 유도할 수 있는 면역원성을 갖는 에피토프를 식별하는 일은 매우 중요한 일이다.
Beyond neutralizing antibodies produced by B cells, cytotoxic CD8 T cells and the helper CD4 T cells are essential to clear viruses. Circulating in the blood, T cells are leading the first response to any virus in adaptive immune system that detects infected cells and mount an immune response or directly clear the infected cells, often before symptoms appear. Several studies have shown that T cells were essential for effective early immune response and virus clearance[{Oh:2019hx}, {Channappanavar:2014gj}, {Channappanavar:2014we},{Yang:2007ei}]. Therefore, it is important to identify the T cell epitopes that can induce T cell immune responses in effective COVID-19 vaccine development.
</Text>
        </Document>
        <Document ID="9736EDCF-B577-4C0D-ABB3-66C1EAE01F7C">
            <Title>Hi-TIDe : Immunopeptidomics - DOF</Title>
            <Text>By continuing your navigation on the sites and applications "unil.ch", you accept the use of cookies allowing us to optimize your user experience. Read the legal information
OK
Department of Oncology

You are here:  UNIL  &gt; Department of Oncology &gt; Our research groups &gt; Bassani (Hi-TIDe)
 Our research groups 
Hi-TIDe : Immunopeptidomics

| Research interest | Research group projects | Selected publications 
 

 
Michal BASSANI-STERNBERG
Group leader
Human integrated tumor immunology discovery engine (Hi-TIDe)

Department of oncology UNIL CHUV
Ludwig Institute for Cancer Research Lausanne

Head of clinical mass spectrometry unit
Center of experimental therapeutics

Phone +41 79 900 55 30
Email Michal.bassani@chuv.ch



 
 

Research interest
Our main goal is to identify clinically relevant cancer specific Human Leukocyte Antigen (HLA) ligands that will guide the development of personalized cancer immunotherapy using mass-spectrometry (MS), currently the only methodology to unbiasedly identify HLA binding peptides that are presented in vivo to cytotoxic T cells.

Research group projects
Proteogenomics and MS-based immunopeptidomics approaches to identify HLA ligands derived from tumor-associated proteins, mutated neoantigens, non-canonical ORFs and post translationally modified peptides.



We developed a high-throughput and in depth MS-based immunopeptidomics pipeline that now enables robust and reproducible sample preparation and measurement of HLA/MHC class I and class II peptides. We are currently applying this methodology to identify tumor-associated ligands extracted from cell lines and tumor tissues from humans and nice models. 
We have initiated fundamental discovery work to elucidate how tumor cells present antigens and what are the bases of tumor immunogenicity.
We are investigating the differences between tumor types in terms of antigen presentation and how drugs modulate the immunopeptidome.
In collaboration with the Vital-IT group (SIB), we have established a continuous bio-informatics pipeline enabling direct identification of neoantigens by combining genomic information derived from exome-seq analysis with measured immunopeptidomics data.
In collaboration with Pr David Gfeller, we are improving the performance of HLA class I and class II binding prediction tools by training them with our measured immunopeptidomics data.
We apply proteogenomics approaches to identify personalized neo-antigens from patient tumor samples. These tumor-specific antigens will be further developed into personalized cancer vaccines or to enrich tumor-reactive and antigen-specific T cells for adoptive T cell-based therapies.
Selected publications
Julien Racle, Justine Michaux, Georg Alexander Rockinger, Marion Arnaud, Philippe Guillaume, Sara Bobisse, George Coukos, Alexandre Harari, Camilla Jandus, Michal Bassani-Sternberg* and David Gfeller* (2019). Deep motif deconvolution of HLA-II peptidomes for robust epitope predictions. BioRxiv * Co-corresponding authors.

Bassani-Sternberg, M. *, Digklia, A., Huber, F., Wagner, D., Sempoux, C., Stevenson, B. J., Thierry, A.-C., Michaux, J., Pak, H., Racle, J., Boudousquie, C., Balint, K., Coukos, G., Gfeller, D., Martin Lluesma, S., Harari, A., Demartines, N., and Kandalaft, L. E.* (2019). A phase Ib study of the combination of personalized autologous dendritic cell vaccine, aspirin and standard of care adjuvant chemotherapy followed by nivolumab for resected pancreatic adenocarcinoma - a proof of antigen discovery feasibility in three patients. Frontiers in Immunology doi:10.3389/fimmu.2019.01832. *Co-corresponding authors

Ebrahimi-Nik, H., Michaux, J., Corwin, W.L., Keller, G.L., Shcheglova, T., Pak, H., Coukos, G., Baker, B.M., Mandoiu, II, Bassani-Sternberg, M.* and Srivastava, P.K* (2019). Mass spectrometry driven exploration reveals nuances of neoepitope-driven tumor rejection. JCI Insight 5, doi:10.1172/jci.insight.129152. *Co-corresponding authors.

Chong, C., Marino, F., Pak, H. S., Racle, J., Daniel, R. T., Muller, M., Gfeller, D., Coukos, G., &amp; Bassani-Sternberg, M. (2018). High-throughput and sensitive immunopeptidomics platform reveals profound IFNgamma-mediated remodeling of the HLA ligandome. Mol Cell Proteomics. doi:10.1074/mcp.TIR117.000383

*Bassani-Sternberg, M., *Braunlein, E., Klar, R., Engleitner, T., Sinitcyn, P., Audehm, S., Straub, M., Weber, J., Slotta-Huspenina, J., Specht, K., Martignoni, M. E., Werner, A., Hein, R., D, H. Busch, Peschel, C., Rad, R., Cox, J., Mann, M., &amp; Krackhardt, A. M. (2016). Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry. Nat Commun, 7, 13404. doi:10.1038/ncomms13404

Group members
Chloe Chong
Post doctoral fellow
Humberto Ferreira
Post doctoral fellow
Elodie Lauret Marie Joseph
Post doctoral fellow
Justine Michaux
Laboratory Technician
HuiSong Pak
MS operator
Florian Huber
Bioinformatician
 
Markus Müller 
Senior bioinformatician (SIB)
Brian Stevenson 
Senior bioinformatician (SIB)
 

Share:    
Ch. des Boveresses 155 - CH-1066 Epalinges
Switzerland
Tel. +41 21 692 59 92 
Fax +41 21 692 59 95
Contact 
Directories
Legal information
Sitemap
Edition
 
  
 </Text>
        </Document>
        <Document ID="D12B2365-D2DE-46C4-8888-189EE79914C4">
            <Title>Deep learning methods</Title>
            <Text>ANN
ConvNet

DeepLigand
</Text>
        </Document>
        <Document ID="8C6D75BE-6C76-4828-9540-B0D93B916DDB">
            <Title>Evaluation results</Title>
            <Text>The final fine-tuned model was evaluated using two external test datasets containing COVID-19 epitope(YLQPRTFLL)-specific TCR CDR3beta sequence data. The F1 scores were significantly high, 0.938 and 0.968 for Shomuradova dataset and ImmuneCODE dataset, respectively. In particular , our model outperformed TCRGP model[{Emmi Jokinen, 2021}] for ImmuneCODE dataset .</Text>
        </Document>
        <Document ID="C5CAFA7A-9BA8-408E-8306-C5640C771104">
            <Title>NetMHCPan40</Title>
            <Text>#

NetMHCpan4.0[{Jurtz:2017bw}] is a modified affinity model that includes an additional output node for training on natural ligand observations from mass spectrometry data. However, it assumes that affinity and uncorrelated display selection processes share the same sequence features[{Zeng:2019ds}]. 
NetMHCpan4.0은 전형적인 Multi-Task learning 방법을 사용하여 BA와 EL 데이터의 동시에 저레벨의 sequence 패턴을 공유하며 예측 모델을 학습시킨다. 이를 통하여 BA 데이터에 편향되지 않고(오버피팅 되지 않고) EL 데이터의 feature가 가미된(반영된) 좀 더 범용적인 예측 모델을 만들어 낼 수 있다.
그러나, 문제는 BA 데이터를 학습한 예측 모델의 false positive이다. 따라서 BA 데이터로부터 세포 표면으로 제시되는 후보 신항원을 selectively하게 선별해내는 예측 모델을 구축하는데에 한계가 있을 수 있다.</Text>
        </Document>
        <Document ID="5D7401F2-26EC-4813-AE73-8E683D948602">
            <Title>Fine-tuning datasets</Title>
        </Document>
        <Document ID="3AF7D8EA-0A64-417D-8A1D-432B6272B8AB">
            <Title>Fine-tuning and evaluation datasets</Title>
            <Text>임상학적 신생항원 샘플들에 대한 선행학습 된 모델의 미세튜닝과 예측 성능을 평가하기 위해 [{Sarkizova:2019fu}]의 예측 모델 성능 평가에서 사용된 데이터셋을 utility 하였다. The evaluation dataset consisted of  51,531(n) HLA-bound ligands identified by LC-MS/MS from 11 patient-derived tumor cell lines and 999 x n random decoys from the human proteome. To estimate prediction power, we used the fraction of correctly predicted binders in the top 0.1% of the dataset(that is, PPV = true positive call / all positive call). We advocated for the PPV evaluation metric over the commonly used area under the receiver operating characteristic curve(AUROC), because it is better suited for the HLA presentation prediction where a relatively small number of true binders within an excess of non binders. A MHC molecule is expected to present approximately only 0.1% peptides among the 9-mer peptides in the human proteome[{Abelin:2017cn}, {BassaniSternberg:2015js}, {Sarkizova:2019fu].  
</Text>
            <Notes>The MS model selection dataset consists of 226,684 ligands formed by combining 186,415 ligands from IEDB with 39,741 additional ligands from SysteMHC Atlas (Shao et al., 2018) and 530 additional ligands from ref. (Abelin et al., 2017). The unprocessed SysteMHC and Abelin et al. datasets are much larger but most entries are already present in IEDB, are duplicates, or report on alleles for which training was not attempted. The ligands from SysteMHC Atlas were first filtered to remove entries with low confidence (prob &lt; 0.99). Of the 112 alleles supported by the predictor, 57 had at least 100 MS ligands available for model selection (Table S1). 
</Notes>
        </Document>
        <Document ID="DC1448AA-AC33-4695-BAEB-8E8BE85AA8DA">
            <Title>Google AI Blog: Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</Title>
            <Text>Blog
The latest news from Google AI
Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing
Friday, November 2, 2018
Posted by Jacob Devlin and Ming-Wei Chang, Research Scientists, Google AI Language


One of the biggest challenges in natural language processing (NLP) is the shortage of training data. Because NLP is a diversified field with many distinct tasks, most task-specific datasets contain only a few thousand or a few hundred thousand human-labeled training examples. However, modern deep learning-based NLP models see benefits from much larger amounts of data, improving when trained on millions, or billions, of annotated training examples. To help close this gap in data, researchers have developed a variety of techniques for training general purpose language representation models using the enormous amount of unannotated text on the web (known as pre-training). The pre-trained model can then be fine-tuned on small-data NLP tasks like question answering and sentiment analysis, resulting in substantial accuracy improvements compared to training on these datasets from scratch. 

This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. The release includes source code built on top of TensorFlow and a number of pre-trained language representation models. In our associated paper, we demonstrate state-of-the-art results on 11 NLP tasks, including the very competitive Stanford Question Answering Dataset (SQuAD v1.1). 

What Makes BERT Different?
BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).

Why does this matter? Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” However, BERT represents “bank” using both its previous and next context — “I accessed the ... account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.

A visualization of BERT’s neural network architecture compared to previous state-of-the-art contextual pre-training methods is shown below. The arrows indicate the information flow from one layer to the next. The green boxes at the top indicate the final contextualized representation of each input word:

BERT is deeply bidirectional, OpenAI GPT is unidirectional, and ELMo is shallowly bidirectional.
The Strength of Bidirectionality
If bidirectionality is so powerful, why hasn’t it been done before? To understand why, consider that unidirectional models are efficiently trained by predicting each word conditioned on the previous words in the sentence. However, it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model. 

To solve this problem, we use the straightforward technique of masking out some of the words in the input and then condition each word bidirectionally to predict the masked words. For example:

While this idea has been around for a very long time, BERT is the first time it was successfully used to pre-train a deep neural network.

BERT also learns to model relationships between sentences by pre-training on a very simple task that can be generated from any text corpus: Given two sentences A and B, is B the actual next sentence that comes after A in the corpus, or just a random sentence? For example:

Training with Cloud TPUs
Everything that we’ve described so far might seem fairly straightforward, so what’s the missing piece that made it work so well? Cloud TPUs. Cloud TPUs gave us the freedom to quickly experiment, debug, and tweak our models, which was critical in allowing us to move beyond existing pre-training techniques. The Transformer model architecture, developed by researchers at Google in 2017, also gave us the foundation we needed to make BERT successful. The Transformer is implemented in our open source release, as well as the tensor2tensor library.

Results with BERT
To evaluate performance, we compared BERT to other state-of-the-art NLP systems. Importantly, BERT achieved all of its results with almost no task-specific changes to the neural network architecture. On SQuAD v1.1, BERT achieves 93.2% F1 score (a measure of accuracy), surpassing the previous state-of-the-art score of 91.6% and human-level score of 91.2%:

BERT also improves the state-of-the-art by 7.6% absolute on the very challenging GLUE benchmark, a set of 9 diverse Natural Language Understanding (NLU) tasks. The amount of human-labeled training data in these tasks ranges from 2,500 examples to 400,000 examples, and BERT substantially improves upon the state-of-the-art accuracy on all of them:

Making BERT Work for You
The models that we are releasing can be fine-tuned on a wide variety of NLP tasks in a few hours or less. The open source release also includes code to run pre-training, although we believe the majority of NLP researchers who use BERT will never need to pre-train their own models from scratch. The BERT models that we are releasing today are English-only, but we hope to release models which have been pre-trained on a variety of languages in the near future. 

The open source TensorFlow implementation and pointers to pre-trained BERT models can be found at http://goo.gl/language/bert. Alternatively, you can get started using BERT through Colab with the notebook “BERT FineTuning with Cloud TPUs.”

You can also read our paper "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" for more details.
   
    
 
 Google  Privacy  Terms</Text>
        </Document>
        <Document ID="D902C088-6B06-4815-98D9-BF1BA3336783">
            <Title>Materials and Methods</Title>
        </Document>
        <Document ID="ECFB98D7-197A-469D-AB04-62DA64EA8F41">
            <Title>Datasets</Title>
        </Document>
        <Document ID="A0F37104-9E1B-4A2A-8BF0-323AAEF06C7A">
            <Title>기존방법의 한계점</Title>
            <Text>현재까지 큐레이트된 데이터베이스에서 보고된 TCR-epitope interactions 데이터의 수는 매우 제한적이다: VDJdb[{Bagaev:2019hf}] and McPAS-TCR[{Tickotsky:2017bo}] databases contain ~20k and ~55k epitope-specific TCR sequences  as of October 2019, respectively. 특히, 최근의 딥러닝 기반의 방법들은 predictive power를 높이기 위해서 비교적 많은 수의 학습데이터를 필요로 하기 때문에 신뢰성 높은 예측 모델을 구축하는데 한계가 있다.</Text>
        </Document>
        <Document ID="C796E5F0-8538-4CBA-ADE6-26BB129254FB">
            <Title>NetMHCpan4.0</Title>
            <Text>
	

	
Services are gradually being migrated to https://services.healthtech.dtu.dk/.
In the near future, cbs.dtu.dk will be retired. Please try out the new site.


Home


Supplementary material

Here, you will find the data set used for training and testing of the NetMHCpan-4.0 method.

Training data

The training binding data are partitioned in 5 files to be used for cross-validation. For instance do the f000_ba and f000_el files contain the binding affinity and eluted ligand training data, and the c000_ba and c000_el files the binding affinity and eluted ligand test data for the first cross-validation partitioning. It is critical that this data partitioning is maintained.

The format for each of the files is

ARWLASTPL 0.589395 BoLA-D18.4 85.0
ASYAAAAAY 0.496594 BoLA-D18.4 232.0
GMMGGLWKY 0.439136 BoLA-D18.4 432.0
KMFHGGLRY 0.898463 BoLA-D18.4 3.0
KMLEASTIY 0.75609 BoLA-D18.4 14.0
KQLEYSWVL 0.481554 BoLA-D18.4 273.0
KQWSWFSLL 0.451477 BoLA-D18.4 378.0
MMFDAMGAL 0.935937 BoLA-D18.4 2.0
MMMSTAVAF 0.762939 BoLA-D18.4 13.0
MTFPVSLEY 0.485003 BoLA-D18.4 263.0
where the first column gives the peptide, the second column the log50k transformed binding affinity (i.e. 1 - log50k( aff nM)) or 1/0 for the eluted ligangd data, and the third column the class I allele.

When classifying BA peptides into binders and non-binders for calculation of the AUC values for instance, a threshold of 500 nM is used. This means that peptides with log50k transformed binding affinity values greater than 0.426 are classified as binders.

BA data

f000_ba (Train data) c000_ba (Test data)
f001_ba (Train data) c001_ba (Test data)
f002_ba (Train data) c002_ba (Test data)
f003_ba (Train data) c003_ba (Test data)
f004_ba (Train data) c004_ba (Test data)
EL data

f000_el (Train data) c000_el (Test data)
f001_el (Train data) c001_el (Test data)
f002_el (Train data) c002_el (Test data)
f003_el (Train data) c003_el (Test data)
f004_el (Train data) c004_el (Test data)
References

NetMHCpan, a method for MHC class I binding prediction beyond humans 
Ilka Hoof, Bjoern Peters, John Sidney, Lasse Eggers Pedersen, Ole Lund, Soren Buus, and Morten Nielsen 
Immunogenetics 61.1 (2009): 1-13 
PMID: 19002680   Full text 

This file was last modified Sunday 27th 2019f October 2019 13:08:50 GMT


</Text>
        </Document>
        <Document ID="3B20F4A6-3400-4B9C-8591-A326AF8F616B">
            <Title>Result and Discussion</Title>
        </Document>
        <Document ID="0F61C527-EEA6-4954-BC9D-5527562FE504">
            <Title>Datasets</Title>
            <Notes>￼</Notes>
        </Document>
        <Document ID="95384325-4873-420F-AE94-3C7E0A8FF5DD">
            <Title>The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time</Title>
            <Text>
Jay Alammar
Visualizing machine learning one concept at a time
Blog About
The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)

Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) 
Translations: Chinese (Simplified), Persian

The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).


(ULM-FiT has nothing to do with Cookie Monster. But I couldn’t think of anything else..)

One of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component – saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch.


The two steps of how BERT is developed. You can download the model pre-trained in step 1 (trained on un-annotated data), and only worry about fine-tuning it for step 2. [Source for book icon].
BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).

There are a number of concepts one needs to be aware of to properly wrap one’s head around what BERT is. So let’s start by looking at ways you can use BERT before looking at the concepts involved in the model itself.

Example: Sentence Classification

The most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:



To train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT.

For people not versed in the topic, since we’re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a labele (“spam” or “not spam” for each message).


Other examples for such a use-case include:

Sentiment analysis
Input: Movie/Product review. Output: is the review positive or negative?
Example dataset: SST
Fact-checking
Input: sentence. Output: “Claim” or “Not Claim”
More ambitious/futuristic example:
Input: Claim sentence. Output: “True” or “False”
Full Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, by with ML later, hopefully).
Video: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.
Model Architecture

Now that you have an example use-case in your head for how BERT can be used, let’s take a closer look at how it works.



The paper presents two model sizes for BERT:

BERT BASE – Comparable in size to the OpenAI Transformer in order to compare performance
BERT LARGE – A ridiculously huge model which achieved the state of the art results reported in the paper
BERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model – a foundational concept for BERT and the concepts we’ll discuss next.



Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).

Model Inputs



The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.

Just like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.



In terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge.

Model Outputs

Each position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).



That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.



If you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.

Parallels with Convolutional Nets

For those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network.



A New Age of Embedding

These new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let’s recap how those are used before pointing to what has now changed.

Word Embedding Recap

For words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).

The field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)


The GloVe word embedding of the word "stick" - a vector of 200 floats (rounded to two decimals). It goes on for two hundred values.
Since these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:

 
ELMo: Context Matters

If we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born.


Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams
Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.


ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.

What’s ELMo’s secret?

ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.


A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.

We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done.

ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.


Great slides on ELMo
ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).


ULM-FiT: Nailing down Transfer Learning in NLP

ULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.

NLP finally had a way to do transfer learning probably as well as Computer Vision could.

The Transformer: Going beyond LSTMs

The release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs.

The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).

OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling

It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.


The OpenAI Transformer is made up of the decoder stack from the Transformer
The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).

With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.


The OpenAI Transformer is now ready to be trained to predict the next word on a dataset made up of 7,000 books.
Transfer Learning to Downstream Tasks

Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):


How to use a pre-trained OpenAI transformer to do sentence clasification
The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.


Isn’t that clever?

BERT: From Decoders to Encoders

The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?

“Hold my beer”, said R-rated BERT.

Masked Language Model

“We’ll use transformer encoders”, said BERT.

“This is madness”, replied Ernie, “Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.”

“We’ll use masks”, said BERT confidently.


BERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.
Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).

Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.

Two-sentence Tasks

If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).

To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?


The second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT actually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.
Task specific-Models

The BERT paper shows a number of ways to use BERT for different tasks.


BERT for feature extraction

The fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.


Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):


Take BERT out for a spin

The best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.

The next step would be to look at the code in the BERT repo:

The model is constructed in modeling.py (class BertModel) and is pretty much identical to a vanilla Transformer encoder.
run_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file.

Several pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.

BERT doesn’t look at words as tokens. Rather, it looks at WordPieces. tokenization.py is the tokenizer that would turns your words into wordPieces appropriate for BERT.
You can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model.

Acknowledgements

Thanks to Jacob Devlin, Matt Gardner, Kenton Lee, Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post.

Written on December 3, 2018
Subscribe to get notified about upcoming posts by email

Email Address




This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 
Attribution example: 
Alammar, Jay (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/ 

Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.
  </Text>
        </Document>
        <Document ID="C8F98E66-7AC6-4300-B250-87783EE31C61">
            <Title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</Title>
            <Text>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
Google AI Language
{jacobdevlin,mingweichang,kentonl,kristout}@google.com
Abstract
We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be fine- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
1 Introduction
Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).
There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as addi- tional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.
We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the fine-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.
In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked
arXiv:1810.04805v2 [cs.CL] 24 May 2019

word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair representations. The contributions of our paper are as follows:
• We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.
• We show that pre-trained representations reduce the need for many heavily-engineered task- specific architectures. BERT is the first fine- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-specific architectures.
• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert.
2 Related Work
There is a long history of pre-training general lan- guage representations, and we briefly review the most widely-used approaches in this section.
2.1 Unsupervised Feature-based Approaches
Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013).
These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016).
ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els.
2.2 Unsupervised Fine-tuning Approaches
As with the feature-based approaches, the first works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008).
More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-

           NSP Mask LM Mask LM
MNLI NER SQuAD
Start/End Span
                       C T1 ... TN T[SEP] T1’ ... TM’ BERT
C T1 ... TN T[SEP] T1’
... TM’
...
          E
[CLS]
 E
1
 E
N
 E
[SEP]
 E’ 1
  E
[CLS]
 E
1
    E’ M
E
N
E
[SEP]
E’ 1
                            [CLS]
...
Tok 1 ... Tok N
Masked Sentence A
[SEP]
...
Tok 1 ...
TokM
[CLS]
BERT BERT ...
Tok 1 ... Tok N [SEP]
Tok 1 ... TokM
        Masked Sentence B
Question
Question Answer Pair
Fine-Tuning
Paragraph
E’ M
    Unlabeled Sentence A and B Pair Pre-training
  Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers).
ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).
2.3 Transfer Learning from Supervised Data
There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014).
3 BERT
We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and fine-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine- tuning, the BERT model is first initialized with the pre-trained parameters, and all of the param- eters are fine-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate fine-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.
A distinctive feature of BERT is its unified ar- chitecture across different tasks. There is mini-
mal difference between the pre-trained architec- ture and the final downstream architecture.
Model Architecture BERT’s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4
1 https://github.com/tensorflow/tensor2tensor
2 http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/filter size to be 4H,
i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-
 
Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one token sequence. Throughout this work, a “sentence” can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special clas- sification token ([CLS]). The final hidden state corresponding to this token is used as the ag- gregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ∈ RH, and the final hidden vector for the ith input token asTi ∈RH.
For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2.
3.1 Pre-training BERT
Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1.
Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly “see itself”, and the model could trivially predict the target word in a multi-layered context.
former is often referred to as a “Transformer encoder” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation.
In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input.
Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace “masked” words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.
Task #2: Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6
5The final model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.
  
           Input
[CLS]
my
dog
is
cute
[SEP]
he
likes
play ##ing
[SEP]
             Token Embeddings
Segment Embeddings
Position Embeddings
E
B
E
8
E[CLS]
Emy
Edog
Eis
Ecute
E[SEP]
Ehe
Elikes
Eplay
E##ing
E[SEP]
              EA
 EA
 EA
 EA
 EA
 EA
 EB
 EB
 EB
 EB
              E0
 E1
 E2
 E3
 E4
 E5
 E6
 E7
 E9
 E10
Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings.
The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters.
Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.
3.2 Fine-tuning BERT
Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences.
For each task, we simply plug in the task- specific inputs and outputs into BERT and fine- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and
(4) a degenerate text-∅ pair in text classification or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as en- tailment or sentiment analysis.
Compared to pre-training, fine-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-specific details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5.
4 Experiments
In this section, we present BERT fine-tuning re- sults on 11 NLP tasks.
4.1 GLUE
The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.
To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hid- den vector C ∈ RH corresponding to the first input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during fine-tuning are classification layer weights W ∈ RK ×H , where K is the number of labels. We com- pute a standard classification loss with C and W , i.e., log(softmax(C W T )).
7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.
8See (10) in https://gluebenchmark.com/faq.
 
 System
MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average
 Pre-OpenAI SOTA BiLSTM+ELMo+Attn OpenAI GPT BERTBASE BERTLARGE
392k 80.6/80.1 76.4/76.1 82.1/81.4 84.6/83.4 86.7/85.9
363k 108k 67k 8.5k 5.7k 66.1 82.3 93.2 35.0 81.0 64.8 79.8 90.4 36.0 73.3 70.3 87.4 91.3 45.4 80.0 71.2 90.5 93.5 52.1 85.8 72.1 92.7 94.9 60.5 86.5
3.5k 2.5k - 86.0 61.7 74.0 84.9 56.8 71.0 82.3 56.0 75.1 88.9 66.4 79.6 89.3 70.1 82.1
  Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The “Average” column is slightly different thantheofficialGLUEscore,sinceweexcludetheproblematicWNLIset.8 BERTandOpenAIGPTaresingle- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.
We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that fine- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different fine-tuning data shuffling and clas- sifier layer initialization.9
Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.
We find that BERTLARGE significantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.
4.2 SQuAD v1.1
The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from
9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.
10 https://gluebenchmark.com/leaderboard
Wikipedia containing the answer, the task is to predict the answer text span in the passage.
As shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- torS ∈ RH andanendvectorE ∈ RH during fine-tuning. The probability of word i being the start of the answer span is computed as a dot prod-
uct between Ti and S followed by a softmax over eS·Ti
all of the words in the paragraph: Pi = 􏰀j eS·Tj .
 The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as S ·Ti + E ·Tj , and the maximum scoring span where j ≥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.
Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.
Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA fine-
11QANet is described in Yu et al. (2018), but the system has improved substantially after publication.
  
  System Dev Test EM F1 EM F1
Top Leaderboard Systems (Dec 10th, 2018)
System Dev Test
ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0
BERTBASE 81.6 -
  Human
#1 Ensemble - nlnet #2 Ensemble - QANet
Published BiDAF+ELMo (Single)
- -
- -
- -
82.3
86.0 91.7 84.5 90.5
- 85.8 82.3 88.5
- - - -
91.2
 86.6 86.3
formance is measured with 100 samples, as reported in the SWAG paper.
sˆ = max S·T + E·T . We predict a non-null i,j j≥i i j
answer when sˆ &gt; s + τ , where the thresh- i,j null
old τ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.
The results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system.
4.4 SWAG
The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices.
When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer.
We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors’ baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%.
5 Ablation Studies
In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional
BERTLARGE Human (expert)†
Human (5 annotations)†
Table 4: SWAG Dev and Test accuracies. †Human per-
  R.M. Reader (Ensemble) Ours
BERTBASE (Single) BERTLARGE (Single) BERTLARGE (Ensemble) BERTLARGE (Sgl.+TriviaQA) BERTLARGE (Ens.+TriviaQA)
- 85.6 81.2 87.9
80.8 88.5 84.1 90.9 85.8 91.8 84.2 91.1 86.2 92.2
- 85.0 - 88.0
  Human
#1 Single - MIR-MRC (F-Net) #2 Single - nlnet
86.3 89.0
- -
- -
86.9 89.5 74.8 78.0 74.2 77.1
71.4 74.9 71.4 74.4
80.0 83.1
-
-
85.1 91.8 87.4 93.2
 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and fine-tuning seeds.
 System
Top Leaderboard Systems (Dec 10th, 2018)
Dev Test EM F1 EM F1
  unet (Ensemble) SLQA+ (Single)
BERTLARGE (Single)
Published
Ours
- - -
 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.
tuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12
4.3 SQuAD v2.0
The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic.
We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = S·C + E·C to the score of the best non-null span
12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.
78.7 81.9
  
 Tasks
BERTBASE
No NSP
LTR &amp; No NSP
+ BiLSTM
Dev Set
MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1)
results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.
We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.
5.2 Effect of Model Size
In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously.
Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such significant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters.
It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suffi- ciently pre-trained. Peters et al. (2018b) presented
 84.4 88.4 86.7 83.9 84.9 86.5 82.1 84.3 77.5 82.1 84.1 75.7
92.7 88.5 92.6 87.9
92.1
91.6 84.9
77.8
 Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. “No NSP” is trained without the next sentence prediction task. “LTR &amp; No NSP” is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. “+ BiLSTM” adds a ran- domly initialized BiLSTM on top of the “LTR + No NSP” model during fine-tuning.
ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks
We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, fine-tuning scheme, and hyperpa- rameters as BERTBASE:
No NSP: A bidirectional model which is trained using the “masked LM” (MLM) but without the “next sentence prediction” (NSP) task.
LTR &amp; No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our fine-tuning scheme.
We first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring “No NSP” to “LTR &amp; No NSP”. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the

 mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach — we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.
5.3 Feature-based Approach with BERT
All of the BERT results presented so far have used the fine-tuning approach, where a simple classifi- cation layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a down- stream task. However, the feature-based approach, where fixed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.
In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF
Hyperparams Dev Set Accuracy
System
ELMo (Peters et al., 2018a)
CVT (Clark et al., 2018) CSE (Akbik et al., 2018)
Fine-tuning approach BERTLARGE BERTBASE
Feature-based approach (BERTBASE) Embeddings
Second-to-Last Hidden
Last Hidden
Weighted Sum Last Four Hidden Concat Last Four Hidden Weighted Sum All 12 Layers
Dev F1
95.7 -
-
96.6 96.4
91.0 95.6 94.9 95.9 96.1 95.5
Test F1
92.2 92.6 93.1
92.8 92.4
- - - - - -
      #L
3 6 6
12 12 24
#H #A
LM(ppl) MNLI-m MRPC SST-2
Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.
layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.
To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without fine-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classification layer.
Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine- tuning and feature-based approaches.
6 Conclusion
Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architec- tures. Our major contribution is further general- izing these findings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks.
 768 12 5.84 768 3 5.24 768 12 4.68 768 12 3.99
1024 16 3.54 1024 16 3.23
77.9 79.8 88.4
80.6 82.2
81.9 84.8 91.3 84.4 86.7 92.9 85.7 86.9 93.3
86.6 87.8
93.7
90.7
 Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. “LM (ppl)” is the masked LM perplexity of held-out training data.

References
Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.
Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120–128. Association for Computa- tional Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancou- ver, Canada. Association for Computational Lin- guistics.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005.
Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.
Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL.
Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914– 1925.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160–167. ACM.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ̈ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670–680, Copen- hagen, Denmark. Association for Computational Linguistics.
Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079–3087.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.
William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).
William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the . arXiv preprint arXiv:1801.07736.
Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics.
Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics.
Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI.
Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557.
 
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.
Quoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188–1196.
Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.
Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence represen- tations. In International Conference on Learning Representations.
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS.
Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.
Andriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081–1088. Curran As- sociates, Inc.
Ankur P Parikh, Oscar Ta ̈ckstro ̈m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP.
Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532– 1543.
Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL.
Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499–1509.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383–2392.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.
Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638.
Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433.
Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL ’10, pages 384–394.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.
Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform

for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353–355.
Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics.
Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv:1805.12471.
Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.
Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”
We organize the appendix into three sections:
• Additional implementation details for BERT are presented in Appendix A;
• •
A A.1
Additional details for our experiments are presented in Appendix B; and
Additional ablation studies are presented in Appendix C.
We present additional ablation studies for BERT including:
– Effect of Number of Training Steps; and – Ablation for Different Masking Proce-
dures.
Additional Details for BERT Illustration of the Pre-training Tasks
We provide examples of the pre-training tasks in the following.
Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by
• 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy → my dog is [MASK]
• 10% of the time: Replace the word with a randomword,e.g.,my dog is hairy → my dog is apple
• 10% of the time: Keep the word un- changed,e.g.,my dog is hairy → my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.
The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.
Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model

       OpenAI GPT
T T ... T 12N
Trm Trm ... Trm
Trm Trm ... Trm
...
                       Lstm Lstm
Lstm Lstm
ELMo
T T ... T 12N
... Lstm Lstm Lstm ... Lstm ... Lstm Lstm Lstm ... Lstm
...
                                             E
1
E
2
E
N
E
1
E
2
E
N
  BERT (Ours)
T T ... T 12N
Trm Trm ... Trm
Trm Trm ... Trm
...
                  E
1
 E
2
 E
N
Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.
to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.
Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples.
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
Label = IsNext
Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
A.2 Pre-training Procedure
To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as “sentences” even though they are typ- ically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the “next sentence pre- diction” task. They are sampled such that the com- bined length is ≤ 512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces.
We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40
epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.
Training of BERTBASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete.
Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.
A.3 Fine-tuning Procedure
For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:
• Batch size: 16, 32
13 https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html
 
• Learning rate (Adam): 5e-5, 3e-5, 2e-5 • Number of epochs: 2, 3, 4
We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.
A.4 Comparison of BERT, ELMo ,and OpenAI GPT
Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are fine- tuning approaches, while ELMo is a feature-based approach.
The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:
• GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).
• GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only in- troduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training.
• GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.
• GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.
To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.
A.5 Illustrations of Fine-tuning on Different Tasks
The illustration of fine-tuning BERT on different tasks can be seen in Figure 4. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classification out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.
B B.1
Detailed Experimental Setup
Detailed Descriptions for the GLUE Benchmark Experiments.
Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a):
MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classifi- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the first one.
QQP Quora Question Pairs is a binary classifi- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018).
QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classification task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.

   Class Label
C T ... T T T’ ... T’
Class Label
[CLS] [CLS]
                    1
Tok 1N1M
Sentence 1 Sentence 2
Start/End Span
...
N [SEP] 1
M
BERT
        E
[CLS]
E
1
E
N
E
[SEP]
E’ 1
...
E’ M
                           [CLS]
...
Tok
[SEP]
Tok
...
Tok
Tok 1
O
Tok 1
Tok 2 ...
Single Sentence
B-PER ...
Tok 2 ...
Single Sentence
Tok N
O
Tok N
                                        C T ... T T T’ ... T’
1
N [SEP] 1 M
... ...
C T1 T2 ... TN
    BERT
         E
[CLS]
E
1
E
N
E
[SEP]
E’ 1
E’ M
E
[CLS]
E
1
BERT
 E
2
...
E
N
                       [CLS]
Tok ...
[CLS]
Tok [SEP] Tok ... Tok 1N1M
Question Paragraph
        Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
SST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).
CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not (Warstadt et al., 2018).
STS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.
MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations
for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005).
RTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14
WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that’s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-
14Note that we only report single-task fine-tuning results in this paper. A multitask fine-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.
15 https://gluebenchmark.com/faq
C T1 T2   TN
...
      E
[CLS]
E
1
BERT
 E
2
...
E
N
 
jority class.
C Additional Ablation Studies
C.1 Effect of Number of Training Steps
Figure 5 presents MNLI Dev accuracy after fine- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions:
Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never ap- pears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based ap- proaches, as we expect the mismatch will be am- plified for the feature-based approach as the model will not have the chance to adjust the representa- tions.
Masking Rates Dev Set Results
MASK SAME RND MNLI NER
Fine-tune Fine-tune Feature-based
1.
2.
Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high fine-tuning accuracy?
Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps.
Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word?
Answer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.
Ablation for Different Masking Procedures
80% 100% 80% 80% 0% 0%
10% 10% 84.2 95.4 94.9 0% 0% 84.3 94.9 94.0 0% 20% 84.1 95.2 94.6
20% 0% 84.4 95.2 94.7 20% 80% 83.7 94.8 94.6 0% 100% 83.6 94.9 94.6
       C.2
Table8: Ablationoverdifferentmaskingstrategies.
The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.
The numbers in the left part of the table repre- sent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.
From the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.
In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.
         84 82 80 78 76
                   200 400 600 800
Pre-training Steps (Thousands)
1,000
BERTBASE (Masked LM) BERTBASE (Left-to-Right)
      Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k.
MNLI Dev Accuracy
</Text>
        </Document>
        <Document ID="FACD0E37-678A-4AD1-9779-CBDE56EDB804">
            <Title>딥러닝 3단계: 머신러닝 프로젝트 구조화하기 &gt; 다중 작업 학습(Multitask Learning) : edwith</Title>
            <Text>로그인 바로가기
하위 메뉴 바로가기
본문 바로가기
전체강좌 부스트코스 파트너
강좌만들기
 로그인 / 회원가입
딥러닝 3단계: 머신러닝 프로젝트 구조화하기
 커넥트재단  edwith 좋아요 51 수강생 1208 
http://www.edwith.org/deeplearningai3/lecture/34892/
강의목록
공지게시판
다중 작업 학습(Multitask Learning)

학습목표
다중 작업 학습을 배운다.
핵심키워드
다중 작업 학습 (multi-task learning)
저레벨 특성 (low-level feature)

Multitask Learning 원본보기
학습내용
다중 작업 학습은 하나의 신경망이 여러작업을 동시에 할 수 있도록 학습하는 것입니다.
이미지 다중 분류 학습은 신경망 초기 특성들은 여러 물체에서 공유가 가능하기 때문에, 하나의 신경망을 학습시키는 것이 여러 신경망을 개별 학습시키는 것 보다 효율적입니다.
다중 작업 학습은 아래의 상황에서 많이 쓰입니다.
여러 문제들의 하나의 저레벨 특성을 공유할 때
데이터가 비슷할때 (항상 만족하는 것은 아닙니다.)
거대한 작업 세트들을 하나의 큰 신경망으로 한번에 학습 시키려고 할 때
다중 작업 학습보다는 전이학습이 더 많이 쓰이고 있습니다.
공유하기    2 수강완료
전이학습(Transfer Learning)End-to-End Deep Learning 은 무엇인가요?
목록
댓글

이미지 첨부 파일 첨부  수식 비공개   저장
서비스 소개 도움말 제휴 문의 서비스 문의
한국어

 이용약관   개인정보처리방침
상호: 재단법인 커넥트소재지: 경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층대표자명 : 조규찬사업자정보확인사업자 등록번호: 129-82-12249유선 번호: 1522-9182통신판매신고 번호: 제2015-경기성남-0754호고객센터: inquiries@edwith.org
구글플레이앱스토어
© CONNECT All Rights Reserved. Powered by NAVER
</Text>
        </Document>
        <Document ID="A4DA0DF3-3A08-4501-B2F8-F256AFDE0392">
            <Title>MHCflurry</Title>
            <Text>
#

MHCflurry[{ODonnell:2018fv}]는 펩타이드 결합 예측에서의 FP를 줄이기 위해 IEDB 결합데이터셋으로 학습한 모델을 MS elution 데이터셋을 사용하여 성능을 평가하여 높은 성능을 보이는 예측 모델들을 ensembl하여 최종 예측 모델을 구축하였다. 
MHCflurry only considers the natural ligands as peptides that also have high binding affinity to the MHCs, and thus it remains an affinity model that doesn’t consider other peptide features that influence peptide presentation. </Text>
        </Document>
        <Document ID="A69C96B8-4902-42F8-B1CF-586004916454">
            <Title>BERT-based 전이학습을 통해 데이터 수 한계 극복이 가능</Title>
            <Text>또한, BERT의 전이학습 접근 방식을 따라서, BA 데이터를 사용하여 선행 학습된 언어모델을 MS로 식별된 eluted ligand 데이터를 사용한 fine-tuning을 통하여 범용적이면서 동시에 정확한 reliable한 예측 모델을 구축해 낼 수 있을 것이다.
</Text>
            <Notes>

- 기존의 기계학습 또는 딥러닝 기반 신항원 예측 기술은 신뢰성 높은 예측 모델 구축을 위해 가공(레이블)된 많은 수의 학습데이터가 필요하나, 암 특이적 신항원 펩타이드 데이터 수가 적기 때문에 예측 정확도의 한계가 있음
- Allele-specific/Pan-specific 방법의 정화도/범용성에서의 Trade-off
- 본 발명은 이러한 신항원 펩타이드 데이터 수의 한계를 극복하기 위해 최근의 자연어 처리 분야에서 최고의 예측성능을 보이고 있는 양방향 언어모델을 기반으로 기존에 누적된 펩타이드 결합 데이터를 사용하여 선행 학습된 펩타이드 언어 모델(Pre-trained Peptide Language Model)을 구축하고 한국인 종양 특이적 신항원 펩타이드 데이터를 사용한 미세튜닝(Fine-tuning)을 통해 정확도 높은 신항원 펩타이드 예측 모델을 구축하는 기술임 
</Notes>
        </Document>
        <Document ID="FDC57CC5-64C9-4AB4-B489-B3377B58EEB1">
            <Title>Final fine-tuning datasets</Title>
            <Text>Table 1 summarizes the epitope-specific TCRβ CDR3 sequences for each data sources. After removing duplicates with the same {epitope sequence, TCRβ CDR3 sequence}, the positive dataset contained ? unique data points, spanning ?? peptides and ?? TCR beta CDR3 sequences(Table S1).
</Text>
        </Document>
        <Document ID="F25E8FE3-01A8-4ADB-9DDC-D28576DC73AF">
            <Title>Pre-training datasets</Title>
            <Notes># Binding affinity datasets
The affinity measurement dataset used for training and model selection was assembled from a snapshot of the Immune Epitope Database (IEDB) MHC ligands downloaded on Dec. 1, 2017 augmented with the BD2013 dataset (Kim et al., 2014). IEDB entries with non-class I, non-specific, mutant, or unparseable allele names were dropped, as were those with peptides identified by MS or containing post-translational modifications or noncanonical amino acids. This yielded an IEDB dataset of 143,898 quantitative and 43,978 qualitative affinity measurements. Of 179,692 measurements in the BD2013 dataset (Kim et al., 2014), 57,506 were not also present in the IEDB dataset. After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the combined dataset consists of 230,735 measurements across 130 alleles. 
# Output value of binding prediction model and Loss
As in the NetMHC tools, MHCflurry internally transforms binding affinities to values between 0.0 and 1.0, where 0.0 is a non-binder and 1.0 is a strong binder. The neural networks are trained using the transformed values and the inverse transform is used to return prediction results as nanomolar affinities. The transform is given by 1 log50000(x) where x is the nanomolar affinity. Affinities are capped at 50,000 nM.
 Training is attempted for all alleles with at least 25 affinity measurements. Ten percent of the training data is set aside for model selection. Each neural network is trained on a different 90% sample of the remaining data, with the other 10% used as a test set for early stopping. Training proceeds with the RMSprop optimizer using a minibatch size of 128 until the accuracy on the test set has not improved for 20 epochs. At each epoch, 25 synthetic negative peptides for each length 8–15 are randomly generated. These random negative peptides are sampled so as to have the same amino acid distribution as the training peptides and are assigned affinities &gt;20,000 nM. For the MHCflurry (train-MS) variant, the number of random peptides for each length is 0.2n + 25 where n is the number of training peptides. 
A modified mean squared error (MSE) loss function that supports data with inequalities is used for both the training loss and test set accuracy metric. For this loss function, measurements are associated with an inequality: (&lt;), (&gt;), or (=). The loss L is defined as: 
￼

where n is the total number of measurements, and ybi and yi are the predicted and measured values for measurement i, respectively. Quantitative affinity data is associated with an inequality of (=). For qualitative affinity data, we assigned the following inequalities and measurement values: positive-high, &lt; 100 nM; positive, &lt; 500 nM, positive-intermediate, &lt; 1,000 nM; positive-low, &lt; 5,000 nM; negative, &gt; 5,000 nM. In the MHCflurry (train-MS) variant, MS-identified ligands are assigned the value ‘‘&lt; 500 nM.’’ 
</Notes>
        </Document>
        <Document ID="13E839A7-03D8-484E-A943-7441773DAE39">
            <Title>NetTCR: sequence-based prediction of TCR binding
to peptide-MHC complexes using convolutional
neural networks</Title>
            <Text> bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
NetTCR: sequence-based prediction of TCR binding
to peptide-MHC complexes using convolutional
neural networks
Vanessa Isabell Jurtz1​ ,​ Leon Eyrich Jessen1​ ,2,​ Amalie Kai Bentzen2​ ,​ Martin Closter Jespersen1​ ,​ Swapnil Mahajan3​ ,​ Randi Vita3​ ,​ Kamilla Kjærgaard Jensen1​ ,​ Paolo Marcatili1​ ,​ Sine Reker Hadrup2​ ,​ Bjoern Peters3​ ,​ Morten Nielsen1​ ,4
1 DTU Bioinformatics, Technical University of Denmark
2 DTU Nanotechnology, Technical University of Denmark 3 La Jolla Institute for Allergy and Immunology, USA
4 University of San Martin, Argentina
Predicting epitopes recognized by cytotoxic T cells has been a long standing challenge within the field of immuno- and bioinformatics. While reliable predictions of peptide binding are available for most Major Histocompatibility Complex class I (MHCI) alleles, prediction models of T cell receptor (TCR) interactions with MHC class I-peptide complexes remain poor due to the limited amount of available training data. Recent next generation sequencing projects have however generated a considerable amount of data relating TCR sequences with their cognate HLA-peptide complex target. Here, we utilize such data to train a sequence-based predictor of the interaction between TCRs and peptides presented by the most common human MHCI allele, HLA-A*02:01. Our model is based on convolutional neural networks, which are especially designed to meet the challenges posed by the large length variations of TCRs. We show that such a sequence-based model allows for the identification of TCRs binding a given cognate peptide-MHC target out of a large pool of non-binding TCRs.
Introduction
Cytotoxic T cells (CTLs) scan MHC class I-peptide complexes presented on the cell surface of nucleated cells. CTLs are able to recognize and kill infected or malfunctioning cells, e.g. cancer cells ​(1)​. Given the central role of the CTLs in the immune system, it is of paramount importance to understand the interaction between the T cell receptor (TCR) of the CTLs and their cognate peptide-MHCI targets. A peptide recognised in this context is referred to as a T-cell epitope.
The vast majority of all peptides that can be generated from a protein will not be presented by MHC molecules ​(2–4)​. Therefore, prediction of peptide-MHC binding is very useful to limit the number of peptide candidates when looking for potential T cell epitopes. Computational models have been trained successfully to predict peptide-MHCI binding, current state of the art methods include NetMHCpan ​(3, 4)​, NetMHCcons ​(5)​, NetMHC ​(6)​, the IEDB consensus method ​(7)​ and

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
MHCflurry ​(8)​. Binding of peptides can be predicted with very high accuracy and precision for most human MHCI molecules ​(3)​.
However, not all MHC presented peptides are immunogenic. In order to predict which MHC restricted peptides do become T cell epitopes, the interaction between a TCR and its cognate target needs to be better understood. The TCR must be able to make contacts with the peptide as well as the MHC molecule to trigger an immune response. TCR and MHC interactions were reviewed by Gruta et al. ​(9)​. The focus of the here presented work is on the interactions between TCR and peptide.
Ample data are available linking peptides to the MHC molecules they bind, especially with the data obtained from mass spectrometry experiments ​(10–12)​. In contrast, there is much less data available linking specific TCRs to their cognate target. Recently developed high throughput sequencing methods are likely to change this situation and are already contributing increasing amounts of data ​(13, 14)​. Among those methods are the MIRA assay published by Klinger et al. (15)​ and the TCR barcoding technique published by Bentzen et al. ​(16)​. Additionally, two recent publications by Glanville et al.​(17)​ and Dash et al. ​(18)​ have made more high throughput data available. Furthermore, these works describe clustering algorithms able to group TCRs by their epitope specificity. In particular, the work by Glanville et al. suggests that relatively simple sequence-based models can be used to classify and define specificity groups shared by TCRs and individuals. This is in line with earlier work by Roomp and Domingues ​(19)​. Several structure-based approaches for modelling the structure and interactions of the TCR:p:MHC system have likewise been proposed including structural modeling ​(20, 21)​ and structure based prediction of TCR:p:MHC interactions ​(22)​.
The IEDB ​(23)​ as well as the VDJdb ​(24)​ collect sequenced TCRs with known specificity published in peer reviewed articles, thereby providing a useful data resource to the community.
Here we seek, based on such data, to go beyond the work by Glanville et al. and present NetTCR, a method to predict the interaction between TCRs and peptides presented by HLA-A*02:01. NetTCR is based on convolutional neural networks (CNNs) and depends only on the amino acid sequences of the peptide and CDR3 region of the TCR beta chain as input. CNNs scan their input with convolutional filters that cover only short continuous parts of the input. These filters detect patterns and the network is then able to integrate the information from the patterns discovered by different filters throughout the input sequence. This type of model has been very useful in image classification ​(25)​, and recently also for handling sequence data of variable length for prediction of for instance protein secondary structure ​(26, 27)​, kinase phosphorylation ​(28)​ and subcellular location ​(29)​. CNNs are also ideally suited to deal with unaligned peptide and TCR sequences differing in length. The here presented model is available as web-server under ​http://www.cbs.dtu.dk/services/NetTCR/​ and the underlying code can be downloaded here: ​https://github.com/mnielLab/netTCR​.
  
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Methods
Data set
A dataset of TCR beta chain CDR3 sequences and corresponding cognate peptide targets was downloaded from the IEDB ​(23)​ in April 2018. Only peptides presented by HLA-A*02:01 were selected. The training data consisted of 9015 unique data points, spanning 91 peptides and 8920 TCR sequences. Further, an additional dataset generated using the MIRA assay was kindly provided by Klinger et al. ​(15).​ This dataset consisted of 379 unique data points, spanning 16 peptides and 379 TCR sequences derived from 5 donors.
Since these data sets contain only positive interactions, negative data examples were generated by creating internal wrong combinations of TCRs and peptides, i.e. combining TCR sequences with peptides different from their cognate target. These combinations were made by extracting the list of peptide targets from the positive data set (keeping duplicates if a peptide was found to interact with multiple TCRs), and next pairing each TCR with a peptide different from the cognate target randomly drawn from this list of peptide targets. In this way, a data set with 50% positive and 50% negative data points was obtained.
To supplement the data sets with additional negative examples, eluted peptide ligands were retrieved from the IEDB, selecting only peptides derived from self (i.e human) proteins. Further, a set of 200,000 TCR CDR3 sequences from 20 healthy donors ​(30)​ was downloaded. Next, additional data was created by first replacing each TCR in the combined positive and negative mis-paired data sets three times with a random TCR drawn from the healthy donor TCR data set (proportions of additional negatives ranging from 2-5 were tested with limited variations in validation predictive performance, data not shown). Finally, the 3000 eluted ligands were paired with TCRs drawn randomly from the complete set of IEDB positives, IEDB mis-paired negatives, and additional negatives constructed from the TCRs of healthy donors. All the additional TCR peptide combinations were added as negatives to the data set, obtaining a final training data set consisting of 9012 (12%) positive and 66,102 (88%) negative TCR-peptide combinations.
To avoid model overfitting and overestimation of model performance, the entire data set was partitioned into 5 sets prior to model training. Prior to partitioning, TCR beta chain (TCRb) CDR3 sequences were compared to each other using blastp and TCRs sharing more than 90% sequence identity, determined by blastp, were kept in the same data partition. Otherwise data points were assigned to partitions randomly.
Model
A CNN model was implemented to predict whether or not a given TCR is able to recognize a specific peptide. The input to the network was the amino acid sequence of the peptide and the CDR3 region of the beta chain of the TCR. Both sequences were encoded using the

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
BLOSUM50 matrix as described earlier ​(31)​. Additionally N- and C-terminus start and stop signals were added to the TCR sequence and encoded using a vector containing only +/-0.1 for start and stop respectively. Peptide and TCR sequences were each processed by 100 convolutional filters of sizes 1, 3, 5, 7 and 9 amino acids (500 filters in total). The peptide and TCR convolutional layers were concatenated and processed by a second convolutional layer of 100 filters with size 1. Subsequently, global max pooling was performed to remove the sequence length dimension of peptide and TCRb CDR3 region. Global max pooling results were connected to a dense layer of 10 hidden neurons connected to the output neuron. Throughout the network, the sigmoid activation function was applied to all neurons. The network was trained using 5 fold cross validation with early stopping for 300 epochs. The weights were updated using the adam optimizer with a learning rate of 0.001. When additional negative examples were added to the training process, a subset of the negative examples was randomly selected in each training epoch to keep the frequency of positive data points at 50%. The validation error (to determine the early stopping epoch) however, was calculated on the full unbalanced data set.
All models were implemented in the Python programming language using the tensorflow library. The models were exclusively trained on the IEDB data, the MIRA data was used solely for performance evaluation purposes. Model performance was measured in AUC (area under the ROC curve, 0.5 corresponding to random predictions, 1.0 equals perfect predictions) or AUC10%, the partial AUC integrated upto a false positive rate of 10%.
Model evaluation
Performance on a large set of TCRs
To investigate whether the model was able to identify the TCRs interacting with a given peptide out of a large set of TCRs, we identified 3 peptides (GILGFVFTL, GLCTLVAML and NLVPMVATV), all frequently occurring in both the IEDB and MIRA data sets. Each TCR in the MIRA data was paired with each of the three selected peptides and the pair was annotated depending on whether this interaction is positive (i.e. observed in the MIRA experiments) or negative. With this setup, the TCRs can be partitioned into two groups: 1) TCRs with a target among the three analyzed peptides and 2) TCRs without a target among the analyzed peptides. The data were predicted using the models described above (trained on the IEDB data, with and without additional negative data). Maximum prediction values of TCRs not targeting any of the 3 peptides were compared to the prediction values obtained for positive peptide-TCR pairs. Further, the predictions of a given TCR to all 3 peptides were ranked and the rank of the true binding peptide was extracted.
Performance on a set of randomly assigned TCRs
To provide a random baseline performance, a data set was generated by randomly assigning the TCR sequences of the original data to the peptides in the data set. Subsequently a model was trained on this random data set as described above.

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
V+J gene bias
To investigate how much of the models performance could be explained by a potential bias in V and J gene usage, models were trained on the center and N- and C-terminal regions of the TCRb CDR3 sequences. To train models on only the central part of the CDR3 sequence, the first and last two amino acids were replaced with X, corresponding to unknown amino acid. The influence of the N- and C-terminal regions of the CDR3 sequence was investigated using two approaches: 1) the TCRb CDR3 sequence represented only by the first and last two amino acids and 2) all amino acids of the TCRb CDR3 sequence except the first and last two are replaced by X, thus conserving information about the length of the original CDR3 sequence. Separate models were trained on these three representations of the TCR sequences and compared to a model trained on the complete CDR3 sequence.
Experimental validation
Ethical approval. ​All healthy donor material was collected under approval by the Scientific Ethics Committee of the Capital Region of Denmark, and written informed consent was obtained according to the Declaration of Helsinki.
Peptides and MHC monomer production.​ Peptides were purchased from Pepscan (Pepscan Presto) and dissolved to 10 mM in DMSO. UV-sensitive ligands were synthesized as previously described ​(32–34)​. Recombinant HLA-A*02:01 heavy chain and human β2 microglobulin light chain were produced in Escherichia coli. HLA heavy and light chain were refolded with UV-sensitive ligands and purified as described in ​(35)​. Specific peptide-MHC complexes were generated by UV-mediated peptide MHC exchange ​(33)​.
Generation of fluorescently labeled pMHC tetramers. ​MHC tetramers were assembled as described previously ​(36, 37)​ onto one of two fluorescently-labeled streptavidin (SA) conjugates: SA-phycoerythrin (PE) or SA-allophycocyanin (APC) (BioLegend, Nordic Biosite, Denmark). Tetramers binding one of the four peptides NLVPMVATV, GLCTLVAML, YVLDHLIVV and GILGFVFTL were​ ​labeled with PE while the remaining tetramers were labeled with APC (see Table S2 for full list of included peptides). Tetramers were stored at -20 °C in 5% glycerol (vol/vol) and 0.5% BSA (wt/vol).
Frequencies of antigen-specific T cells (Table S2) were determined using combinatorial encoding of pMHC tetramers ​(36, 37)​ or DNA barcode-labeled MHC multimers ​(16)​.
Peptide-MHC tetramer staining.​ Cryopreserved PBMCs from four healthy donors were thawed and washed in RPMI + 10% FCS. Cells were washed in a cytometry buffer (PBS + 2% FCS). 5×106​ ​ cells were incubated, 15 min, 37 °C, with pooled PE and APC tetramers in a total volume of 100 μL (final concentration of each distinct pMHC, 23 nM). Next a 5× antibody mix composed of CD8-BV480 (BD 566121, clone RPA-T8) (final dilution 1/50), dump channel antibodies: CD4-FITC (BD 345768) (final dilution 1/80), CD14-FITC (BD 345784) (final dilution 1/32), CD19-FITC (BD 345776) (final dilution 1/16), CD40-FITC (Serotech MCA1590F) (final dilution

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
1/40), CD16-FITC (BD 335035) (final dilution 1/64) and a dead cell marker (LIVE/DEAD Fixable Near-IR; Invitrogen L10119) (final dilution 1/1000) was added and incubated 30 min, 4 °C. Cells were washed twice in cytometry buffer, resuspended in 100 μL cytometry buffer and sorted immediately.
Flow cytometry and cell sorting.​ Tetramer-stained cells were sorted on a FACSAriaFusion (Becton Dickinson) into tubes containing 100 μL PBS supplemented with BSA (0.5%), herring DNA (100 μg/mL) and EDTA (2 mM) (tubes were saturated with PBS + 2% BSA in advance). Using FACSDiva software, we gated on single, live CD8 positive and ‘dump’ (CD4, 14, 16, 19, and 40) negative lymphocytes and within this population sorted either all PE positive cells or all APC positive cells into separate tubes. The sorted cells were centrifuged 10 min, 5,000g, and the buffer was removed. The cell pellet was stored at −20 °C in a minimal amount of residual buffer (&lt;20 μL). DNA was isolated using QIAamp DNA Micro Kit according to manufacturer’s instructions (Qiagen) and the TCRb chains were sequenced and processed at Adaptive Biotechnologies (Seattle, WA) using the ImmunoSEQ platform.
In silico interaction predictions​ Binding to all four peptides was predicted for the PE (now referred to as positive) and the APC (negative) sorted populations (Table S3) using the TCRb CDR3 sequences and the model trained on a combination of IEDB and MIRA data sets with additional negative data. For a given TCR sequence the maximum scoring prediction was recorded and it was investigated whether a difference in these maximum prediction scores could be observed between the positive and negative TCRs.
Results
We here present a machine learning model to predict TCR-peptide interactions based on only the peptide and TCRb CDR3 amino acid sequences. Training data was obtained from the IEDB, the model was evaluated on data generated with the MIRA assay (for details see materials and methods). Both datasets cover several peptides presented by HLA-A*02:01 but are dominated by the same 3 peptides (Figure 1A). Prior to training the model, the data were partitioned keeping similar TCRs in the same partition to limit redundancy between data partitions. Figure 1B shows all TCRs in the IEDB data color coded according to their peptide target (TCRs recognizing a peptide different from the above mentioned 3 abundant peptides are colored gray), lines connect TCRs sharing more than 90% sequence identity as determined by BlastP. This threshold resulted in clusters largely specific for one given peptide, indicating it is appropriate to reduce data redundancy.
A convolutional neural network architecture used to predict interaction between TCRs and their cognate target is shown in Figure 1C. The two convolutional layers combined with global max pooling enable training the model on peptides and TCR sequences of different lengths. Models were trained using 5 fold cross-validation as described in materials and methods.

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 1: A) Frequency of peptides in IEDB and MIRA data sets. Only peptides that occur more than 10 times are shown, less frequent peptides are summarized in “other”. B) TCRb CDR3 clustering in the IEDB data set. The coloring corresponds to the TCRs peptide target, TCRs that share more than 90% BlastP sequence identity are connected C) Setup of the convolutional neural network.
Model performance
One important application of our model would be to identify binding TCRs specific to one or more of the peptides from a large data set of irrelevant TCRs obtained, for instance, by repertoire sequencing. We simulated this task by selecting the three most common peptides in the IEDB (GILGFVFTL, GLCTLVAML and NLVPMVATV) which are also part of the MIRA data. Subsequently, we predicted binding of all TCRs in the MIRA data to each of those three peptides, using two different models: one trained on the IEDB data with internal negative data and another trained with additional negative data (derived from TCR sequencing projects and eluted peptide ligands, for details see methods). Subsequently we evaluated how the models could separate positive TCRs binding one of the three peptides from the negative TCRs.
Figures 2A-C give the results of this analysis, comparing the performance of the two models in terms of ROC, sensitivity and specificity curves. The AUC value of the model trained with additional negative data was slightly increased (0.697 to 0.727) compared to the model trained with only internal negative examples. As shown in Figure 2A, the AUC10% increased substantially to 0.48 from 0.27 with additional negative data. Figure 2B reveals that additional negative training data increased the specificity of the model while decreasing the sensitivity (Figure 2C).
 
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 2: A) ROC curves, B) specificity and C) sensitivity for models trained on the IEDB data with internal negative data and additional negative data. Predictions were made on three common peptides shared between IEDB and MIRA data, combined with all MIRA TCRs.
Next, we investigated to what degree our model was able to identify the correct cognate peptide target for a given TCR. For each TCR binding to one of the three peptides included in the evaluation data set we predicted binding to all three peptides, and calculated the rank of the true cognate target. This rank is 1 if the cognate peptide target achieved the strongest predicted binding value among the three peptides. Figure 3A shows the histogram of these rank values for the two models. The model trained with additional negative data predicted the target peptide as rank 1 for 112 (59.3%) TCRs out of 189 compared to 90 (47.6%) for the model trained only on IEDB data. These results thus confirm the predictive power of the method in identifying the correct cognate target for a given TCR.
Apart from producing high predictions for the interaction between TCR and cognate target peptide, in order to be useful, a model is also required to make low predictions for negative TCRs with no cognate target among the peptides covered by the model. We tested to which degree this was the case for the models trained with or without additional negative data. For this we compared the highest prediction values obtained for TCRs without a target among the three selected peptides, to the predictions made for TCRs paired with their correct target. The result of this comparison is shown in Figure 3B and revealed that the model trained only on the IEDB data often assigned negative TCRs higher prediction values compared to observed peptide-TCR pairs. In contrast, for the model trained with additional negative examples, we found that the true cognate targets received higher median prediction values than the maximal predictions made for TCRs without cognate targets among the selected peptides. Similar observations were made when considering the predictions made for a specific peptide (Figure S1). Also in this case, the model with additional negative training data predicted noticeably lower values for negative compared to positive TCRs.
In conclusion, these results demonstrate that the model trained on additional negative data outperformed the model trained only on IEDB data in terms of assigning the highest rank to true
 
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
target peptides. Further, this model was able to accomplish the task of finding few interacting TCRs out of a pool of many non-interacting TCRs, due to increased specificity.
Figure 3: A) For the TCRs binding one of the 3 peptides in the evaluation set the rank of this peptide is shown (rank 1 = highest prediction, rank 3=lowest prediction). B) For TCRs binding to one of the three peptides in the evaluation set the prediction value to this peptide is shown. For TCRs not binding to one of the peptides in the evaluation set all prediction values are shown. Subsequently only the maximum prediction to an evaluation peptide is shown. The intention is to visualize if one can separate TCRs that have a cognate target among the peptides the model is trained on, from those that do not recognize any of the peptides in the model, based the model’s prediction values. The red line at 0.25 indicates such a classification threshold.
Performance on unknown peptides and randomly assigned TCRs
The IEDB data set is dominated by only 2 peptides, as shown above in Figure 1A. Still, we aimed to investigate if our model is able to learn general interaction rules that allow for accurate predictions for peptides not included in training. To get an estimate of the model’s performance on unknown peptides, we predicted the MIRA data and calculated the performance per peptide. The result of this analysis is shown in table S1 and demonstrated clearly that the performance on the peptides unique to the MIRA data was considerably lower than the performance on
 
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
peptides shared with the training data. This indicates that the model has limited potential for extrapolating predictions to unknown peptides, most likely due to the limited amount of peptides in the training data.
To obtain a baseline performance and ensure that this would be random, we trained a model where all TCRs in the data set were randomly re-assigned to a peptide. As expected this model achieved a random performance of AUC=0.538 on the test set and AUC=0.4 on the external MIRA data.
Influence of V and J genes on model performance
The current data sets linking TCRs to peptide epitopes are small and further limited by being derived from a small number of donors. This could possibly lead to a bias in the V and J gene usage of the donors to which a model predicting peptide and TCR interaction might overfit. As the N- and C-terminal parts of the CDR3 sequence are defined mostly by the V and J genes, we tested the potential V and J gene bias in the data by comparing the performance of a model trained on the full TCRb CDR3 sequence to models trained only on the central part of the CDR3 sequence or the N- and C-terminal parts, (see figure 4A). For evaluating the amount of information captured in the terminal parts, we represented the TCRs as only the first two N and C terminal residues or with the central part of the sequence masked as X for unknown amino acid, thereby conserving information about the loop length of the original CDR3 sequence (for details see materials and methods).
Figure 4B shows the test set performance of the different models. Training only on the two N- and C-terminal amino acids of the CDR3 sequence resulted in a marked drop in test set performance (from AUC=0.676 to AUC=0.605), indicating there is not enough information contained in the terminal residues only to train a prediction model. When informing the model of the N and C terminal amino acids along with the loop length of the CDR3 region, a model can be trained with a test performance comparable to when using the complete CDR3 sequence (AUC=0.655). This is also the case when training only on the central part of the CDR3 sequence, masking the N- and C-terminal residue (AUC=0.664). However, when turning to the external MIRA evaluation set (see figure 4C), only models trained on the the complete or central part of the CDR3 sequence generalize well. This result suggests that models trained only on the N- and C-terminal regions of the CDR3 sequences likely overfits to the V and J gene distribution in the IEDB and therefore do not achieve good performance on the MIRA data set.

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 4: A) To investigate V+J gene bias, models with different representations of the TCRb CDR3 sequence were trained. B+C) Performance of the different models on the test and MIRA datasets given in AUC, the red line denotes random performance.
Experimental validation of the model
Next, we set out to validate our model in a real-life biological setting. T cells from four donors were sorted into a positive subset, containing TCRs responsive to the four HLA-A*02:01 restricted peptides (GILGFVFTL, GLCTLVAML, NLVPMVATV, YVLDHLIVV) and a negative subset, containing TCRs responsive to 92 other HLA-A*02:01-restricted peptides (table S3). The CDR3 sequences were obtained from the TCR beta chains of each of these subsets. In parallel, we trained a model on the combined IEDB and MIRA data. This combined data set has a large amount of TCR data for the four positive peptides (GILGFVFTL, GLCTLVAML, NLVPMVATV and YVLDHLIVV), and we would hence expect to be able to predict the interactions to one or more of the peptides for the TCRs in the positive subset and lack of interaction for the TCRs in the negative subset. To validate that this was indeed the case, we predicted the interaction between each TCR and the four positive peptides and identified the maximum prediction value for each TCR among those four peptides. Figure 5 compares these maximum prediction values for the positive and negative samples, showing higher prediction values for the TCRs that are able to recognize one of the four peptides (p-value &lt; 0.01, student T-test). In line with what we observed in Figure 3B, the positive TCRs achieve a median prediction value close to 0.25, while the vast majority of negative TCRs result in predictions below 0.25. 67 of the 314 unique positive TCRs are identical with TCRs in the IEDB and MIRA data sets, likely representing public TCR sequences. No identical TCRs can be found between the negative fraction and IEDB or MIRA data.
 
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Figure 5: Prediction values for TCRs recognizing one of four tested peptides (positive) and TCRs not recognizing any of the tested TCRs (negative).
Discussion
We present a model capable of predicting the cognate target of a given TCR based on the amino acid sequences of the peptide and CDR3 region of the TCR beta chain. The underlying model is a convolutional neural network (CNN). The performance of the model was evaluated in several benchmarks demonstrating a high ability both to separate T cell receptors specific for the set of peptides included in the training data from T cells specific to irrelevant peptides, and to identify the correct cognate target for a given TCR.
It has been suggested previously that learning the rules of TCR antigen recognition is extremely difficult since TCRs can rearrange themselves upon contact with the peptide-MHC complex ​(38) and thereby gain immense cross-reactivity ​(39, 40)​. While the necessity and role of TCR cross-reactivity in the immune systems function remains to be elucidated ​(41)​, growing evidence suggests that T cell receptors specific to a common target share common properties ​(17, 18)​. Increasing amounts of data are now available linking TCR sequences to their cognate targets. The predictive power of the model proposed here is in line with these observations.
Given the fact that most TCR sequencing projects focus on characterizing the CDR3 region of the T cell receptor beta chain sequence, we chose to train a model based on this part of the TCR only. It is clear that this potentially has limited the predictive power of our model, and that future extension of the model would benefit from being trained on paired T cell sequence data
 
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
covering both the alpha and beta chain and possibly including information describing the V and J genes of the rearrangement.
The training data used here was obtained from the IEDB. Alternatively this data could have been obtained from the VDJdb, another database curating and providing TCR sequences and their cognate targets published in literature. Both databases are an extremely valuable resource to the community.
When aiming to train a model to predict TCR specificity, negative data is needed but not readily available in resources such as the IEDB and VDJdb. This is because the underlying experiments identify interacting TCRs and do not specifically report non-interacting TCRs.
Our approach to resolve this, was to make mismatching combinations between peptides and TCR sequences, keeping the frequency of peptides in positive and negative data equal. The advantage of this approach is that the model is prevented from simply learning interacting peptides or TCRs by heart, due to the equal amount of positive and negative examples.
We envision that our model would likely be used to filter out TCRs from repertoire sequencing that are able to interact with a given peptide. This task of identifying very few sequences out of a pool of many requires a model of great specificity. To increase the specificity of our model it was necessary to add more TCR sequences and peptides as negative examples to the training. The negative TCR sequences were obtained by repertoire sequencing of healthy individuals and paired with the peptides in the IEDB and further self peptides identified by ligand elution assays to be presented by HLA-A*02:01. Combining TCR sequences of healthy individuals with human self peptides should result in true negative examples in the vast majority of cases, but when combining those TCRs with the peptides present in the IEDB data, which are to a large extent well studied influenza and herpes virus epitopes ​(42)​, they might result in some false negatives. However, as the chances of combining the right TCR with the right peptide in this setting are slim, in the vast majority of cases we expect to obtain true negatives with this approach. In the case of the MIRA data, the experimental setup requires TCRs to be specific to only one of the 16 assayed peptides, hence eliminating the possibility of discovering cross-reactive TCRs ​(15)​.
We chose to set up our model as a CNN. This type of model is highly flexible and has earlier been demonstrated to be highly suited to discover motifs in unaligned input sequences of varying length ​(28, 29)​. Additionally convolutional networks are often faster and easier to train than recurrent long short-term memory networks (LSTMs). Future work will tell if improved predictive power can be obtained combining different network architectures ​(43)​.
Currently available datasets of TCR-peptide interactions contain many more TCR sequences than peptides and the peptides share very limited similarity in general. It is therefore expected that a model trained on these data will have limited power to extrapolate predictions to unknown peptides. This is also what we observed when evaluating the performance of the model on peptides not present in the IEDB training data. Given these observations, we

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
recommend that the model should presently only be used to make predictions for the four peptides covered by abundant TCR data in the combined IEDB and MIRA data.
Due to the limited amount of TCR donors in the current data set, it is a concern that models might overfit to a bias in V and J gene usage in the donors. To investigate the extent of this in our data set, we trained different models masking the N- and C-terminal regions or the center of the TCR beta CDR3 regions. We found that we could indeed train models that performed well on the test set when supplying information about the N- and C-terminal regions and the length of the CDR3 loop. When partitioning the data sets, we did not account for reducing redundancy between partitions based on the V and J genes, we only compared the entire CDR3 sequences in our approach to redundancy reduction. Therefore the same bias in V and J gene usage is likely present throughout all partitions, explaining the high performance on the test set. When evaluating models on the independent MIRA data set, obtained from a different set of donors, we find that only models trained on the central part of the CDR3 sequence generalize well. This indicates that there is indeed a bias in V and J gene usage to which a model can overfit, but there is also a signal in the central part of the CDR3 region, defining the specificity of the TCR which is consistent across several data sets and donors.
Substantial efforts have recently been dedicated to elucidate what properties of a TCR dictate its specificity, and publications suggest TCRs sharing a common target are characterized by sharing, to some degree, a common motif ​(17, 18)​. This is also the underlying assumption of the model presented here. To further validate our approach, we performed a TCR sequencing experiment where we obtained two sets of TCRs: one specific to the four peptides covered by our model, and one non-specific to any of these peptides. As expected, the model achieved higher interaction predictions for TCRs recognizing one of the four known cognate target peptides. More detailed results could have been achieved by determining the exact specificity of each TCR. This was however not possible due to limited funds.
In conclusion, we have successfully trained a model to predict interactions between TCRs and their cognate, HLA-A*02:01 restricted peptide target. Our results indicate that accurate prediction based only on the TCR beta chains CDR3 region and amino acid sequence of the peptide is feasible. Due to the small amount of training peptides, the model can however at present only be applied to the limited set of peptides included in the training data. However as more data becomes available, we expect the predictive power of the model to increase, and allow for accurate predictions also for uncharacterized peptides as has been observed earlier for the pan-specific prediction models of peptide-MHC interactions ​(44)​. Finally, the presented model framework is highly flexible and allows for the straight forward integration of the MHC molecule or TCR alpha chain in the future when data becomes available, to train a truly global prediction method.

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
References
1. Zhang N, Bevan MJ (2011) CD8(+) T cells: foot soldiers of the immune system. ​Immunity 35(2):161–168.
2. Yewdell JW, Bennink JR (1999) Immunodominance in major histocompatibility complex class I-restricted T lymphocyte responses. ​Annu Rev Immunol​ 17:51–88.
3. Jurtz V, et al. (2017) NetMHCpan-4.0: Improved Peptide-MHC Class I Interaction Predictions Integrating Eluted Ligand and Peptide Binding Affinity Data. ​J Immunol 199(9):3360–3368.
4. Nielsen M, Andreatta M (2016) NetMHCpan-3.0; improved prediction of binding to MHC class I molecules integrating information from multiple receptor and peptide length datasets. Genome Med​ 8(1):33.
5. Karosiene E, Lundegaard C, Lund O, Nielsen M (2012) NetMHCcons: a consensus method for the major histocompatibility complex class I predictions. ​Immunogenetics 64(3):177–186.
6. Andreatta M, Nielsen M (2016) Gapped sequence alignment using artificial neural networks: application to the MHC class I system. ​Bioinformatics​ 32(4):511–517.
7. Moutaftsi M, et al. (2006) A consensus epitope prediction approach identifies the breadth of murine TCD8 -cell responses to vaccinia virus. ​Nat Biotechnol​ 24(7):817–819.
8. O’Donnell T, Rubinsteyn A, Bonsack M, Riemer A, Hammerbacher J (2017) MHCflurry: open-source class I MHC binding affinity prediction. doi:​10.1101/174243​.
9. La Gruta NL, Gras S, Daley SR, Thomas PG, Rossjohn J (2018) Understanding the drivers of MHC restriction of T cell receptors. ​Nat Rev Immunol​ 18(7):467–478.
10. Bassani-Sternberg M, et al. (2017) Deciphering HLA-I motifs across HLA peptidomes improves neo-antigen predictions and identifies allostery regulating HLA specificity. doi:​10.1101/098780​.
11. Pearson H, et al. (2016) MHC class I–associated peptides derive from selective regions of the human genome. ​J Clin Invest​ 126(12):4690–4701.
12. Abelin JG, et al. (2017) Mass Spectrometry Profiling of HLA-Associated Peptidomes in Mono-allelic Cells Enables More Accurate Epitope Prediction. ​Immunity​ 46(2):315–326.
13. Breden F, et al. (2017) Reproducibility and Reuse of Adaptive Immune Receptor Repertoire Data. ​Front Immunol​ 8:1418.
14. Rubelt F, et al. (2017) Adaptive Immune Receptor Repertoire Community recommendations for sharing immune-repertoire sequencing data. ​Nat Immunol​ 18(12):1274–1278.
15. Klinger M, et al. (2015) Multiplex Identification of Antigen-Specific T Cell Receptors Using a

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Combination of Immune Assays and Immune Receptor Sequencing. ​PLoS One 10(10):e0141561.
16. Bentzen AK, et al. (2016) Large-scale detection of antigen-specific T cells using peptide-MHC-I multimers labeled with DNA barcodes. ​Nat Biotechnol​ 34(10):1037–1045.
17. Glanville J, et al. (2017) Identifying specificity groups in the T cell receptor repertoire. Nature​ 547(7661):94–98.
18. Dash P, et al. (2017) Quantifiable predictive features define epitope-specific T cell receptor repertoires. ​Nature​ 547(7661):89–93.
19. Roomp K, Domingues FS (2011) Predicting interactions between T cell receptors and MHC-peptide complexes. ​Mol Immunol​ 48(4):553–562.
20. Riley TP, Singh NK, Pierce BG, Weng Z, Baker BM (2016) Computational Modeling of T Cell Receptor Complexes. ​Methods Mol Biol​ 1414:319–340.
21. Pierce BG, Weng Z (2013) A flexible docking approach for prediction of T cell receptor-peptide-MHC complexes. ​Protein Sci​ 22(1):35–46.
22. Lanzarotti E, Marcatili P, Nielsen M (2018) Identification of the cognate peptide-MHC target of T cell receptors using molecular modeling and force field scoring. ​Mol Immunol 94:91–97.
23. Vita R, et al. (2015) The immune epitope database (IEDB) 3.0. ​Nucleic Acids Res 43(Database issue):D405–12.
24. Shugay M, et al. (2017) VDJdb: a curated database of T-cell receptor sequences with known antigen specificity. ​Nucleic Acids Res.​ doi:​10.1093/nar/gkx760​.
25. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. ​Nature​ 521(7553):436–444.
26. Zhang B, Li J, Lü Q (2018) Prediction of 8-state protein secondary structures by a novel
deep learning architecture. ​BMC Bioinformatics​ 19(1):293.
27. Klausen MS, et al. (2018) NetSurfP-2.0: improved prediction of protein structural features
by integrated deep learning. doi:​10.1101/311209​.
28. Fenoy E, Izarzugaza JMG, Jurtz V, Brunak S, Nielsen M (2018) A generic Deep Convolutional Neural Network framework for prediction of Receptor-ligand Interactions. NetPhosPan; Application to Kinase Phosphorylation prediction. ​Bioinformatics.​ doi:​10.1093/bioinformatics/bty715​.
29. Almagro Armenteros JJ, Sønderby CK, Sønderby SK, Nielsen H, Winther O (2017) DeepLoc: prediction of protein subcellular localization using deep learning. ​Bioinformatics 33(21):3387–3395.
30. Savola P, et al. (2017) Somatic mutations in clonally expanded cytotoxic T lymphocytes in patients with newly diagnosed rheumatoid arthritis. ​Nat Commun​ 8:15869.

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
31. Nielsen M, et al. (2003) Reliable prediction of T-cell epitopes using neural networks with novel sequence representations. ​Protein Sci​ 12(5):1007–1017.
32. Toebes M, et al. (2006) Design and use of conditional MHC class I ligands. ​Nat Med 12(2):246–251.
33. Rodenko B, et al. (2006) Generation of peptide-MHC class I complexes through UV-mediated ligand exchange. ​Nat Protoc​ 1(3):1120–1132.
34. Bakker AH, et al. (2008) Conditional MHC class I ligands and peptide exchange technology for the human MHC gene products HLA-A1, -A3, -A11, and -B7. ​Proc Natl Acad Sci U S A 105(10):3825–3830.
35. Hadrup SR, et al. (2009) High-throughput T-cell epitope discovery through MHC peptide exchange. ​Methods Mol Biol​ 524:383–405.
36. Hadrup SR, et al. (2009) Parallel detection of antigen-specific T-cell responses by multidimensional encoding of MHC multimers. ​Nat Methods​ 6(7):520–526.
37. Andersen RS, et al. (2012) Parallel detection of antigen-specific T cell responses by combinatorial encoding of MHC multimers. ​Nat Protoc​ 7(5):891–902.
38. Rossjohn J, et al. (2015) T Cell Antigen Receptor Recognition of Antigen-Presenting Molecules. ​Annu Rev Immunol​ 33(1):169–200.
39. Wooldridge L, et al. (2012) A single autoimmune T cell receptor recognizes more than a million different peptides. ​J Biol Chem​ 287(2):1168–1177.
40. Colf LA, et al. (2007) How a single T cell receptor recognizes both self and foreign MHC. Cell​ 129(1):135–146.
41. Sewell AK (2012) Why must T cells be cross-reactive? ​Nat Rev Immunol​ 12(9):669–677.
42. Fleri W, et al. (2017) The Immune Epitope Database: How Data Are Entered and Retrieved.
J Immunol Res​ 2017:5974574.
43. Jurtz VI, et al. (2017) An introduction to deep learning on biological sequence data:
examples and solutions. ​Bioinformatics​ 33(22):3685–3690.
44. Hoof I, et al. (2009) NetMHCpan, a method for MHC class I binding prediction beyond
humans. ​Immunogenetics​ 61(1):1–13.
Aknowledgements
We thank Michael Schantz Klausen for assistance in setting up the webserver.

 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Supplementary
Figure S1: Prediction values for each TCR-peptide combination among the 3 peptides in the evaluation data and all MIRA TCRs. A) IEDB internal negative data model B) IEDB model trained with additional negative data.
Table S1: AUC per peptide of the MIRA data predicted with a model trained on IEDB with additional negative data.
  
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
Table S2: Previously detected responses (% of CD8 T cells).
      Donor ID
    Peptide sequence
     BC-D83
   BC-D104
   BC-D108
  BC-D112
   GILGFVFTL
      0.017
    0.18
    0.06
   0
   GLCTLVAML
      0
    0.19*
    0.05
   1.44
 NLVPMVATV
  0
     0
      0.22
     3.41
   YVLDHLIVV
    0.36
 0.084
 0.04
 0.36
 CLGGLLTMV
  0
     0.034
      0.05
     0.09*
   FLYALALLL
    0.06
 0.07*
 0.02
 0.05*
   VLEETSVML
      0
    0
    0
   0
   ILKEPVHGV
      0
    0
    0
   0
   Donor HLA-type
                  HLA-A
  0101, 0201
     0201
      0101, 0201
     0201
   HLA-B
    4402, 5701
 5001, 5101
 0702, 1815
 0702, 4402
 HLA-C
  0501, 0602
    0602, 1502
     0702, 1203
    0501, 0702
 *estimated frequencies from a DNA barcode-based MHC multimer analysis (Bentzen et al. PMID: 27571370)
Table S3: List of peptides used to isolate TCR sequences. All peptides are HLA-A*02:01 restricted.
  Peptide
     Antigen origin
  Source protein
   NLVPMVATV
       CMV
    pp65
    GLCTLVAML
       EBV
    BMF1
    YVLDHLIVV
       EBV
    BRLF1
    GILGFVFTL
       FLU
    MP 58-66
    VLEETSVML
      CMV
   IE1
   CLGGLLTMV
      EBV
   LMP2
   FLYALALLL
      EBV
   LMP2
 ILKEPVHGV
  HIV
   Pol (C20)
 
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
     RVAALARDAP melanoma 707-AP
        RLDFNLIRV melanoma ATIC (AICRT)
        MVYDLYKTL melanoma ATIC (AICRT)
        NLFETPVEA melanoma BA46 (MFGE8)
        GLQHWVPEL melanoma BA46 (MFGE8)
        PLFDFSWLSL melanoma Bcl-2
        WLSLKTLLSL melanoma Bcl-2
        YLNDHLEPWI melanoma Bcl-xL
        CQWGRLWQL melanoma BING-4
        LATEKSRWSG melanoma B-RAF
        VLEGMEVV melanoma cyclophilin B (Cyp-B)
        FILPVLGAV melanoma Cadherin 3/P-cadherin
        LLGATCMFV melanoma cyclin D1
        LATEKSRWS melanoma B-RAF
        KLKHYGPGWV melanoma cyclophilin B (Cyp-B)
        FLWGPRAYA melanoma DAM-6, -10 (MAGE-B1, -B2)
        IMNDMPIYM melanoma EphA2
        VLAGVGFFI melanoma EphA2
        VLLLVLAGV melanoma EphA2
        TLADFDPRV melanoma EphA2
        FINDEIFVEL melanoma EZH2
        FMVEDETVL melanoma EZH2
        VLPDVFIRCV melanoma GnTV
        YLEPGPVTA melanoma gp100 / Pmel17
        RLASFYDWLP melanoma Livin (ML-IAP)
        SLGSPVLGL melanoma Livin (ML-IAP)
        QLCPICRAPV melanoma Livin (ML-IAP)
        RIDITLSSV melanoma M2BP
        KVLEYVIKV melanoma MAGE-A1
       VLPDVFIRC melanoma GnTV
    
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
     IMDQVPFSV melanoma gp100 / Pmel17
        MLGTHTMEV melanoma gp100 / Pmel17
        LLDVAPLSL melanoma hsp70
        GLYDGMEHL melanoma MAGE-A10
        LVHFLLLKY melanoma MAGE-A2
        KVLEFLAKL melanoma MAGE-C2
        KVAELVHFL melanoma MAGE-A3
        TILLGIFFL melanoma MC1R
        ELAGIGILTV melanoma Melan-A / MART-1
        ILTVILGVL melanoma Melan-A / MART-1
        TLNDECWPA melanoma Meloe-1
        CMHLLLEAV melanoma MG50
        VLSVNVPDV melanoma MG50
        QLSLLMWIT melanoma NY-ESO-1 / LAGE-2
        SLLMWITQCFL melanoma NY-ESO-1 / LAGE-2
        IMLCLIAAV melanoma P Polypeptide
        SAWISKPPGV melanoma SOX10
        SLYSFPEPEA melanoma PRAME
        VLDGLDVLL melanoma PRAME
        LLLDDLLVSI melanoma PRDX5
        SLLMWITQC melanoma NY-ESO-1 / LAGE-2
        VLHWDPETV melanoma RAB38 / NY-MEL-1
        LKLSGVVRL melanoma RAGE-1
        PLPPARNGGL melanoma RAGE-1
        YLMDTSGKV melanoma Replication protein A
        LLQAEAPRL melanoma SART-3
        RLAEYQAYI melanoma SART-3
        VYDFFVWLHY melanoma TRP-2
        SLLMWITQA melanoma NY-ESO-1 / LAGE-2
       RLVDDFLLV melanoma Telomerase
    
 bioRxiv preprint doi: https://doi.org/10.1101/433706; this version posted October 3, 2018. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.
     ILAKFLHWL melanoma Telomerase
        FLYDDNQRV melanoma Topoisomerase II
        ILLRDAGLV melanoma TRAG-3
        FVWLHYYSV melanoma TRP-2
        SLDDYNHLV melanoma TRP-2
        TLDSQVMSL melanoma TRP-2
        SVYDFFVWL melanoma TRP-2
        ATTNILEHY melanoma TRP2-6b
        FIASNGVKLV melanoma alpha-actinin-4
        KLDVGNAEV melanoma BAP31
        RLPPKPPLA melanoma Meloe-2
        LMAGCIQEA melanoma CDKN1A
        GLGLPKLYL melanoma CDKN1A
        FAWERVRGL melanoma CDKN1A
        NLVRDDGSAV melanoma CLP (coactosin-like protein)
        RLFAFVRFT melanoma CLP (coactosin-like protein)
        VVQNFAKEFV melanoma CLP (coactosin-like protein)
        YVDPVITSI melanoma c-MET
        WLQYFPNPV melanoma CYP1B1
        FLTPKKLQCV melanoma PSA+PAP
        VISNDVCAQV melanoma PSA+PAP
        FLTPKLQCV melanoma PSA+PAP
        KLQCVDLHV melanoma PSA+PAP
        ALDVYNGLL melanoma PSA+PAP
        FLFLLFFWL melanoma PSA+PAP
        ILLWQPIPV melanoma PSA+PAP
        TLMSAMTNL melanoma PSA+PAP
       YLPFRNCRP melanoma PSA+PAP
    </Text>
        </Document>
        <Document ID="96A675F7-14B4-4E8A-85DA-523C87E2420B">
            <Title>Input representation</Title>
        </Document>
        <Document ID="83C6512A-9411-4103-B7E5-BDEAC4230634">
            <Title>Predicting antigen specificity of single T cells based on TCR CDR3 regions</Title>
            <Text>     Article
  Predicting antigen specificity of single T cells based on TCR CDR3 regions
David S Fischer1,2 , Yihan Wu1 , Benjamin Schubert1,3 &amp; Fabian J Theis1,2,3,*
    Abstract
It has recently become possible to simultaneously assay T-cell specificity with respect to large sets of antigens and the T-cell receptor sequence in high-throughput single-cell experiments. Leveraging this new type of data, we propose and benchmark a collection of deep learning architectures to model T-cell specificity in single cells. In agreement with previous results, we found that models that treat antigens as categorical outcome variables outperform those that model the TCR and antigen sequence jointly. Moreover, we show that variability in single-cell immune repertoire screens can be mitigated by modeling cell-specific covariates. Lastly, we demonstrate that the number of bound pMHC complexes can be predicted in a continuous fashion providing a gateway to disentangle cell-to-dextramer binding strength and receptor-to- pMHC affinity. We provide these models in the Python package TcellMatch to allow imputation of antigen specificities in single-cell RNA-seq studies on T cells without the need for MHC staining.
Keywords antigen specificity; multimodal; single cell; supervised learning; T-cell receptors
Subject Categories Computational Biology; Immunology
DOI 10.15252/msb.20199416 | Received 16 December 2019 | Revised 13 July 2020 | Accepted 22 July 2020
Mol Syst Biol. (2020) 16: e9416
Introduction
Antigen recognition is one of the key factors of T cell-mediated immu- nity. T cells interact via a dimeric surface protein, the T-cell receptor (TCR), with an antigen presented on a major histocompatibility complex (MHC) located on the surface of antigen-presenting cells. This presenting cell can be experimentally modeled via an MHC multimer with an immobilized antigen (pMHC). The T cells of an individual organism cover a wide range of antigen specificities. This variability in specificity stems mostly from plasticity of three complementarity-deter- mining region (CDR) loops (CDR1-3) of both TCR ɑ- and b-chains. The hypervariable loops CDR3ɑ and CDR3b are most commonly aligned with the presented epitope (Singh et al, 2017) and are hypothesized to
be the main driver of T-cell specificity (Glanville et al, 2017). However, specificity-determining influences of the other CDR loops (Cole et al, 2009; Madura et al, 2013; Stadinski et al, 2014) and distal regions (Harris et al, 2016a,b) have also been demonstrated.
The ability to accurately predict T-cell activation upon antigen recognition based on antigen and TCR sequences would have trans- formative effects on many research fields from infectious disease, autoimmunity, and vaccine design to cancer immunology, but has been thwarted by a lack of training data and adequate models. In the absence of sufficiently large experimental data, most studies focused on molecular analysis of individual co-crystallized TCR–pMHC complexes and molecular dynamics simulations with limited success (Flower et al, 2010). Only recently, through concerted data collection efforts (Borrman et al, 2017; Shugay et al, 2018; Vita et al, 2019) and newly emerging high-throughput technologies that allow the sequenc- ing of the TCR while probing the T-cell specificity (Klinger et al, 2015; Bentzen et al, 2016), have large enough data sets become avail- able to begin modeling the TCR–pMHC interaction through machine- learning methods (Zvyagin et al, 2020). Current methods to predict the likelihood of binding of TCRs to specific antigens use linear posi- tion-specific scoring matrices (Glanville et al, 2017), Gaussian processes (preprint: Jokinen et al, 2019), or random forests (Gielis et al, 2018). A second set of methods attempts to directly model the TCR–pMHC inter- action with neural networks in order to generalize across unseen TCR– antigen pairs (preprint: Jurtz et al, 2018). We expand on these efforts but also consider the current limitation in the number of available anti- gens in training data sets. Secondly, we consider the inclusion of complex sets of cell-specific covariates into the prediction problem. The inclusion of cell-specific covariates has previously been shown to work in the example of transcriptome-derived clusters as covariates (preprint: Jokinen et al, 2019). Here, we leverage the data modalities in the new droplet-based single-cell experiments.
In this study, we exploit a newly developed single-cell technology that enables the simultaneous sequencing of the paired TCR ɑ- and b-chains and determining the T-cell specificity via bound peptide- loaded MHC (pMHC) complexes. This technology allows the routine collection of binding TCR and antigen complexes of the size of entire curated databases in a single study (Bagaev et al, 2019; 10x Geno- mics, 2019) and accordingly harnesses great potential to transform the field of T-cell receptor specificity prediction. We propose and trained multiple deep learning architectures that model the TCR–
 1 Institute of Computational Biology, Helmholtz Zentrum München, Neuherberg, Germany
2 TUM School of Life Sciences Weihenstephan, Technical University of Munich, Freising, Germany
3 Department of Mathematics, Technical University of Munich, Garching bei München, Germany
*Corresponding author. Tel: +49 89 3187 43260; E-mail: fabian.theis@helmholtz-muenchen.de
a 2020 The Authors. Published under the terms of the CC BY 4.0 license Molecular Systems Biology 16: e9416 | 2020 1 of 14

Molecular Systems Biology
David S Fischer et al
pMHC interaction. The models account for the variability found in single-cell data through cell-specific covariates. We show that models that include both ɑ- and b-chain have a predictive advantage over models that only include the b-chain, while models fit on only a single chain still perform well. We further find that T-cell specificity imputation in a single-cell sample from a known donor is possible, enabling assessment of the presence of disease-specific T cells, while generalization across unknown TCR–pMHC pairs is still not possible. Lastly, we anticipate a large number of single-cell studies involving T cells to exploit TCR specificity as an additional phenotypic readout. To facilitate the usage of our predictive algorithms, we built the Python package TcellMatch, which hosts a pre-trained model zoo for analysts to impute pMHC-derived antigen specificities and allows the transfer and re-training of models on new data sets.
Results
A joint deep learning model for alpha- and beta-chains, antigens, and covariates for single-cell TCR profiling experiments
We set out to predict the antigen specificity of single T cells based on TCR ɑ- and b-chain sequences and other cellular covariates, such as donor identity and cell surface protein counts. We used a publicly available single-cell data set (10x Genomics, 2019) based on a tech- nology in which cells are captured in droplets in a microfluidics system so that antigen specificity, the CDR3 TCR sequences, surface protein abundance, and mRNA abundance can be assayed for each captured cell (Fig 1A, Methods and Protocols). Antigen specificity was quantified via the count of unique molecular identifiers associ- ated with antigen-specific dextramer (pMHC complex) barcode sequences (10x Genomics, 2019). Additionally, we used databases (IEDB; Shugay et al, 2018; Vita et al, 2019) and VDJdb (Shugay et al, 2018) that harbor additional pairs of binding TCR and antigen
sequences from traditional low-throughput screenings and crystal structures to validate our results. The prediction of antigen speci- ficity was previously attempted on smaller data sets, but the new single-cell technology enables the collection of data sets that are orders of magnitude larger than what was previously available from curation efforts that integrated studies from the entire field of TCR specificity (Shugay et al, 2018; Vita et al, 2019). These large single- cell data sets may, however, be susceptible to greater noise than results derived from studies that are either conducted in bulk or validated separately. We chose deep learning models for the predic- tion task as these are well suited to cope with large noisy data sets. We included interpretable linear models and a previously proposed non-linear reference model (NetTCR; preprint: Jurtz et al, 2018) as baseline methods. The convolutional and linear models used here are in structure similar to models that relate antigen specificity to clusters of TCR sequences but are continuously differentiable and therefore easier to extend to new specificity groups.
The prediction of antigen specificity from TCR sequences and numeric cellular covariates is a mixed input data-type problem. The deep characterization of the single cells via modalities such as mRNA or surface protein abundance in the context of specificity assessment makes such mixed input data-type models much more relevant to single-cell data than they were previously to less well-characterized pairs of binding TCRs and antigens that were curated from literature. We approached this problem by combining a network tailored to numerical data with a network tailored to sequence-structured data to yield a single prediction (Fig 1B). Machine learning on sequence data is a field of ongoing research and different layer types have been shown to be effective for different tasks. Accordingly, we imple- mented all major sequence data-specific layer types to be able to perform a comprehensive comparison of deep learning architectures for the task of predicting TCR specificity. This comprehensive compar- ison is to the best of our knowledge the first of its kind. Specifically, we implemented recurrent layers (bidirectional GRUs; Schuster &amp;
▸
 Figure 1. Deep learning models predict binding of T-cell receptors (TCR) to peptide MHC complexes (pMHC) from defined antigen panels.
Distributions shown as boxplots are across threefold cross-validation. AUC ROC test: Area under the receiver operating characteristic curve on the test set for the binary
binding event prediction task. The top panel in (C), (F), (G) is a zoom into an informative region of the y-axis. counts: total mRNA counts, nc: negative-control pMHC counts,
surface: surface protein counts.
A Concept of multimodal single-cell immune profiling experiment with RNA-seq, surface protein quantification, bound pMHC quantification, and TCR reconstruction.
B Categorical TcellMatch model: A feed-forward neural network to predict a vector of antigen specificities of a T cell based on the CDR3 sequences of the TCR ɑ- and
b-chains. Gray boxes: layers of the neural network.
C Covariates improve sequence-based binding accuracy prediction. Shown are bidirectional GRU models fit on both ɑ- and b-chains (CONCAT). none: no cell-specific
covariates, donor: one-hot encoded donor identity, donor + counts: one-hot encoded donor identity and total mRNA counts per cell, counts, nc: negative-control pMHC count vector, nc + donor + counts: negative-control pMHC count vector, one-hot encoded donor identity and total mRNA counts per cell, counts,
nc + donor + counts + surface: negative-control pMHC count vector, one-hot encoded donor identity, total mRNA counts per cell and surface protein count vector (n = 4 cross-validations for models none and nc, “leave-one donor out”, and n = 3 cross-validations for all other models).
D Overlap of correctly and incorrectly classified test set observations from best-performing model to models with reduced covariate sets. Models without donor covariates were not included. full: nc + donor + counts + surface model from (C), red: model shown on x-axis tick (n = 3 cross-validations for all models).
E Antigen-wise prediction performance by covariates setting. In contrast to panel (C), the prediction performance is not aggregated across the entire test set but
evaluated separately the observations belonging to each antigen. Shown are bidirectional GRU models fit on both ɑ- and b-chains (CONCAT) (n = 4 cross-validations
for models without donor covariate, “leave-one donor out”, and n = 3 cross-validations for all other models).
F Antigen-binding prediction is improved by the inclusion of TCR CDR3 sequences. BIGRU: bidirectional GRU model, NOSEQ: model without TCR sequence embedding. Models
without donor covariates were not included (n = 4 cross-validations for models none and nc, “leave-one donor out”, and n = 3 cross-validations for all other models).
G Antigen-binding prediction based on TCR CDR3 sequences is improved by modeling ɑ- and b-chains. BIGRU: bidirectional GRU model, SA: self-attention model, CONV:
convolution model, LINEAR: linear model, CONCAT: models fit on the CDR3 sequences of both TCR ɑ- and b-chains, TRA, TRB: models fit on the CDR3 sequence of either the TCR ɑ- or the b-chain (n = 3 cross-validations for all other models).
 Data information: All boxplots: the center of each boxplot is the sample median; the whiskers extend from the upper (lower) hinge to the largest (smallest) data point no further than 1.5 times the interquartile range from the upper (lower) hinge. In (C, F, G), the underlying data points are shown as swarm plots color-coded in the same way as the boxplot.
2 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors

David S Fischer et al
Molecular Systems Biology
 A
BCD
Figure 1.
FG
E
Paliwal, 1997; Cho et al, 2014) and bidirectional LSTMs (Hochreiter &amp; Schmidhuber, 1997; Schuster &amp; Paliwal, 1997), convolutional layers (Szegedy et al, 2015), self-attention layers (Vaswani et al, 2017), and
densely connected networks, which include linear models that relate to previous work (Glanville et al, 2017). All of these sequence data embedding layer types require an initial representation of the elements
a 2020 The Authors
Molecular Systems Biology 16: e9416 | 2020 3 of 14

Molecular Systems Biology
David S Fischer et al
of the sequence: an initial encoding of the amino acids. We compared categorical, substitution frequency derived (BLOSUM), and learned embeddings and found that the initial amino acid embedding does not have a strong effect on the results (Appendix Fig S1). The novel learned embedding that we propose here is more parameter efficient as it can expose a lower-dimensional amino acid space to the sequence-embedding layers than the standard embedding layers do (Methods and Protocols). In the following, we only show model fits based on these learned 1×1 convolutional embeddings based on BLOSUM50 (Methods and Protocols).
We considered the binding event prediction task within a panel of antigens as a single- or multi-task prediction problem with anti- gen species as categorical output variables (“categorical antigen model”, Figs 1 and 2, Methods and Protocols). Secondly, we consid- ered binding event prediction on arbitrary antigens as a distinct scenario that requires the model to embed the input antigen sequence (“antigen-embedding model”, Fig 3, Methods and Proto- cols). The categorical antigen model predicts a probability distribu- tion across possible binding events, including a negative (no binding) event. The antigen-embedding model is based on the concept of positive and negative sets. In the single-cell data, a nega- tive set naturally arises from cells that did not bind to any or a given pMHC species. The positive set is naturally defined as the observed binding pairs. We generated the negative set for TCR–antigen- binding pairs from IEDB or VDJdb (preprint: Jurtz et al, 2018) shuf- fling TCR and antigen assignments in silico.
Assembling meaningful training and test sets across databases
We subset the data sets to allow a meaningful model comparison and predictivity evaluation: The single-cell data set contained more than 150,000 cells from four donors with successfully reconstructed TCR sequences and with measured binding specificity to 44 distinct pMHC complexes. The authors of this data set defined binding events by comparing the target pMHC counts to the counts of negative-control pMHCs. pMHCs were defined as negative-control pMHCs if they were not expected to specifically bind any TCR in the screen (10x Geno- mics, 2019). We assembled antigen specificity labels based on the same binding classification scheme. We removed putative cellular doublets from the data set (Methods and Protocols, Appendix Fig S2): A doublet of two cells of distinct specificities in a microfluidics setup may result in the TCR sequence of the first cell and the pMHC binding read-outs from the second cell being misreported as a third, non-exis- tent, specificity pair. To avoid such non-existent specificity pairs, we chose a conservative doublet exclusion threshold (Methods and Protocols). We only considered the eight antigens in the pMHC CD8+ T-cell data set that had at least 100 unique, non-doublet clonotype observations to remove effects from strong class imbalance (Appendix Fig S3A and B). The total data set size was 91,495 unique, non-doublet observations (cells) across the four donors.
We only assembled pairs of binding TCR CDR3 b-chain and anti- gen sequences from IEDB and VDJdb as these databases contain far fewer ɑ-chain than b-chain sequences and do not contain an equiva- lent of the cellular covariates found in the single-cell data. We only considered observations from the most commonly assayed HLA type HLA-A*02:01. We assembled a data set of 12,414 observations from 10,726 clonotypes and 71 antigens from IEBD and 3,964 observations from 2,812 clonotypes and 40 antigens from VDJdb, which contained
at most 10 TCR sequences per clonotype. The number of TCR clono- types per antigen was very heterogeneous, with the most frequently encountered antigen covering 4,812 clonotypes in IEDB, and 1,461 in VDJdb. We provided a detailed descriptive analysis of all data sets in Dataset EV3. TCR and specificity variation of the single-cell data are also described in detail elsewhere (10x Genomics, 2019).
To avoid an over-optimistic estimation of model performance, we clustered the T cells into clonotypes and separated the single-cell data into train, test, and validation sets with regard to their assigned clonotypes so that each clonotype only existed in one of the splits (Methods and Protocols). We down-sampled clonotypes to a maxi- mum of 10 observations.
Cell-specific covariates improve binding event prediction
Single-cell T-cell specificity screens feature multiple effects that confound the binding event and its observation. Here, we compared the performance of categorical antigen models with various sets of covariates to quantify the relevance of covariates for predictive models.
Firstly, one would expect the donor identity to affect the TCR sequence if donors vary in their HLA genotype. We compared models with and without a one-hot encoded donor identity covariate to estab- lish the impact of these donor-to-donor differences. We found that the performance of models without donor information varies strongly and is much worse than the performance of models with donor covariates. The mean area under the receiver operating characteristic curve (AUC ROC, Methods and Protocols) was 0.33 for bidirectional GRU models (the best-performing sequence-based models) without covariates and 0.81 for those with donor covariates (Fig 1C).
The identification of binding events based on single-cell RNA-seq libraries is liable to false negatives due to a low capture rate of RNAs. In the single-cell screen, negative-control pMHCs were included to provide a background distribution of non-specific binding events and were part of the definition of discrete binding events (Methods and Protocols). The discrete labels are therefore already corrected for false-positive binding events. We investigated whether normal- ization factors and negative-control pMHC counts are useful predic- tors of a false-negative binding event that cannot be rescued by background signal correction: A donor covariate-only model (“donor”) was not outperformed either by a model that also included a scaled total mRNA count covariate (“donor + counts”) or by one that additionally also contained negative-control count covariates (“nc + donor + counts”) (Materials and Methods, Fig 1C). We conclude that such false-negative observations are either rare or cannot be captured by the correction proposed here. We also identi- fied a predictive advantage of models that account for the cell state encoded by surface protein counts: bidirectional GRUs that accounted for donor, negative-control pMHC counts, and total counts improved from 0.83 AUC ROC to 0.86 if cell surface protein counts were added as a covariate (Fig 1C, Welch’s t-test, P &lt; 0.01). The surface protein counts can be used to embed cells based on their membrane surface structure in a latent space which can be used by the model to account for the abundance of TCRs and other binding- relevant proteins on the cell surface. The overall top-performing model accounted for donor, total counts, negative-control counts, and surface protein counts with an AUC ROC of 0.87 (Fig 1C).
We validated that growing the set of covariates modeled lead to models that had additional (rather than different) correct
4 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors

David S Fischer et al
Molecular Systems Biology
  AB
C
Figure 2. The binding strength of T cells to pMHC complexes can be modeled based on single-cell data.
A Sequence-encoding layer types outperform linear models on pMHC count prediction if donor and size factors are given as covariates. BIGRU: bidirectional GRU model,
SA: self-attention model, CONV: convolution model, LINEAR: linear model, CONCAT: models fit on the CDR3 sequences of both the TCR ɑ- and b-chains, TRA,
TRB: models fit on the CDR3 sequence of either the TCR ɑ- or the b-chain (n = 3 cross-validations for all other models).
B Performance of bidirectional GRU models that predict pMHC counts directly is best if covariates and both TCR chains are modeled. test R2 (log): test R2 on
log-transformed test data. none: no cell-specific covariates, donor: one-hot encoded donor identity, donor + counts: one-hot encoded donor identity and total mRNA counts per cell, counts, nc: negative-control pMHC count vector, nc + donor + counts: negative-control pMHC count vector, one-hot encoded donor identity and total mRNA counts per cell, counts, nc + donor + counts + surface: negative-control pMHC count vector, one-hot encoded donor identity, total mRNA counts per cell and surface protein count vector (n = 4 cross-validations for models without donor covariate, “leave-one donor out”, and n = 3 cross-validations for all other models).
C Multi-task models outperform separate single-task model on pMHC count prediction by antigen. multi: multi-task model, single: single-task model (n = 3 cross- validations for all other models).
 Data information: All boxplots: the center of each boxplot is the sample median; the whiskers extend from the upper (lower) hinge to the largest (smallest) data point no further than 1.5 times the interquartile range from the upper (lower) hinge. The underlying data points are shown as swarm plots color-coded in the same way as the boxplot.
 predictions. The best-performing model with the highest number of covariates predicted almost all observations correctly that were also predicted correctly by models with fewer covariates (Fig 1D). The test sets are not balanced across the different classes: We found similar trends across covariate settings on each individual class as we found globally (Fig 1E). We validated that sequence information
is indeed a relevant predictor in each of these covariate scenarios, indicating that the combination of sequence and non-sequence covariates is desirable (Fig 1F).
Lastly, we investigated whether models that were fit with cell- specific covariates generalize to observations that do not contain these covariates. For this purpose, we applied models presented in
a 2020 The Authors
Molecular Systems Biology 16: e9416 | 2020 5 of 14

Molecular Systems Biology
David S Fischer et al
this section to TCR sequences from matched and unmatched anti- gens from IEDB (Vita et al, 2019) and VDJdb (Shugay et al, 2018), setting the covariate input vector to zero. The best-performing linear predictors had true-positive rates above 0.55 while maintaining false-positive rates below 0.1 (Appendix Fig S4), suggesting that these models can generalize to settings in which not all covariates are observed.
Co-modeling alpha- and beta-chains improves binding event prediction
We compared the predictivity of models fit using one TCR CDR3 chain (“TRA only”, or “TRB only”) with models fit on both TRB and TRA chains (“TRA + TRB”, Materials and Methods) to evaluate the additional information inherent in the use of both chains. We found
 Figure 3.
AB
CD
E
6 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors

David S Fischer et al
Molecular Systems Biology
 Figure 3. Models tailored to generalize to unseen antigens are outperformed by categorical antigen models on seen antigens. Distributions shown as boxplots are across threefold cross-validation.
A
B C
D, E
The databases IEDB and VDJdb contain pairs of TCRs and antigens that were found to be specific to each other and are curated from many different studies.
A supervised model that predicts binding events can be trained on such data but also requires the assembly of a set of negative observations (Methods and Protocols).
Antigen-embedding TcellMatch model: A feed-forward neural network to predict a binding event based on TCR CDR3 sequences and antigen peptide sequence. Gray boxes: layers of the neural network.
Different sequence-encoding layer types perform similarly well on binding prediction based on TRB-CDR3 and antigen sequence. CONCAT: models in which TRB CDR3 sequence and antigen sequence are concatenated, SEPARATE: models in which TRB CDR3 sequence and antigen sequence are embedded by separate sequence-encoding layer stacks. BILSTM: bidirectional LSTM model, BIGRU: bidirectional GRU model, SA: self-attention model, CONV: convolution model, INCEPTION: inception-type model, NETTCR: NetTCR model (preprint: Jurtz et al, 2018), LINEAR: linear model (n = 3 cross-validations for all other models).
Antigen-wise categorical models outperform models that are built to generalize across antigens on high-frequency antigens in IEDB (D) and on overlapping antigens between IEBD and single-cell data (E). In both cases, the models were trained on IEDB and tested on held-out observations from IEBD (D) or on the single-cell data (E). embedding: models that are embedding the antigen sequence and can be run on any antigen (Fig 3b), categorical: antigen-wise categorical models that do not have the antigen sequence as a feature (Fig 1B) (n = 3 cross-validations for all other models).
 Data information: All boxplots: the center of each boxplot is the sample median; the whiskers extend from the upper (lower) hinge to the largest (smallest) data point no further than 1.5 times the interquartile range from the upper (lower) hinge. The underlying data points are shown as swarm plots color-coded in the same way as the boxplot.
◀
that TRA + TRB models were slightly better than TRA-only and TRB-only models across most layer types if basic single-cell covari- ates were included in the prediction. The top-performing TRA + TRB was 0.01 AUC ROC better than the corresponding single-chain model (Fig 1G). This suggests that the evolutionary constraint on the ɑ-chain is so strong that there is a strong correla- tion between the two chains, which is in line with recent results that are based on prediction performance on smaller single-cell data sets (preprint: Jokinen et al, 2019) and results based on TCR similarity (Lanzarotti et al, 2019). We found that recurrent and convolutional neural networks performed similarly to linear models, typically with a difference of up to 0.01 AUC ROC (Fig 1G). This suggests that anti- gen specificity of a ɑ- and b-chain pair can be well represented as a sequence motif problem in which the sequence motif has a fixed position on the CDR3 sequence.
Binding strength can be approximated based on pMHC counts
In single-cell studies, antigen-binding events are measured as the number of bound pMHCs of the target antigen compared with bound negative-control pMHCs (Fig 1A). We hypothesized that one can predict not only binarized binding events but also binding strength based on the pMHC counts. The pMHC complexes used here are multimers (“dextramers”) and there are typically many TCR complexes on the cell surface. Therefore, the number of bound pMHCs on a T cell is determined by a combination of the affinity of an individual TCR to a pMHC monomer and the number of possible interactions between the multimeric pMHC complex and TCR mono- mers on the cell surface, the compound binding strength.
We fit models that were similar in structure to the models dedi- cated to binarized binding event prediction on covariates and TCR CDR3 sequences (Fig 1B) to predict pMHC counts per cell (Fig 2A). We investigated whether total count and negative-control pMHC covariates explain additional variance in the data. In contrast to the discrete binding event prediction models, the labels (the pMHC counts) are no longer corrected for the negative-control background signal anymore in this scenario so that one would expect total counts and negative-control pMHC counts to influence the target pMHC counts. Indeed, the donor covariate-only model (“donor”) was outperformed by a model that also included a scaled total
mRNA count covariate (“donor + counts”, R2 of log count difference 0.07) and one that additionally also contained negative-control count covariates (“nc + donor + counts”, R2 of log count difference 0.05; Materials and Methods, Fig 2B). We conclude that T cell- specific covariates can be used to fit variation in the pMHC count signal. The best-performing model included donor, total count, negative-control pMHC counts, and surface protein covariates with an R2 of log counts of 0.63 (Fig 2B). The relevance of the surface state covariate beyond the background correction may be an indica- tion of a separation of affinity (pMHC to TCR interaction) and the strength of the pMHC complex to T-cell interaction. This overall interaction strength and may depend on additional surface proteins that influence the binding event. Components of variation in both effects can likely be modeled based on the surface protein composi- tion of the cell.
Weak binding events are not captured in the discretized binding data but may be represented in the pMHC counts. Such weak events may contain information about antigen–antigen similarities and therefore about output space correlations, which can be exploited by multi-task supervised learning. Indeed, we found that multi-task models that jointly model the prediction across antigens through shared hidden layers of the neural network architectures outper- formed single-task models on six out of eight antigens modeled (Fig 2C). An alternative interpretation of the improved performance of multi-task models is their ability to learn better de-noised low- dimensional representations of TCR sequences, through the integra- tion of more diverse training data.
Models with sequence-space embedding of antigens are outperformed by categorical models
The categorical approach to modeling antigens suffers from the disadvantage that predictions of unseen antigen sequences are diffi- cult or impossible. Models that are based on a learned embedding of the antigen amino acid sequence can overcome this limitation in principle and have been used (preprint: Jurtz et al, 2018) to predict binding events in databases such as IEBD (Vita et al, 2019) or VDJdb (Shugay et al, 2018) (Fig 3A and B). However, it is unclear whether the antigen diversity in the currently available data is suffi- cient to learn such a generalization across antigens as it does not yet
a 2020 The Authors
Molecular Systems Biology 16: e9416 | 2020 7 of 14

Molecular Systems Biology
David S Fischer et al
adequately cover the antigen space. To resolve this issue, we first built and compared a zoo of models that can embed antigen amino acid sequences to use the best-performing instances as an upper limit on predictive performance. Entries in the IEDB and VDJdb
mostly contain TCR b-chain sequences only. Accordingly, we built models that only use the TCR b-chain sequence to be able to conduct a meaningful extrapolation between the single-cell data, IEDB, and VDJdb. Previously, a specific single-layer motif-based
  A
B
C
D
 Figure 4. Imputed antigen specificity labels enrich single-cell RNA-seq workflows on T cells by an additional phenotype.
A–D UMAP with observed (A, C) and predicted (B, D) labels. (A, B) The cells in the UMAP are the cells from all donors (training and validation data, n = 189,512); the model was fit with donor and size factor covariates. (C, D) The cells in the UMAP are the cells from a validation donor (n = 46,526); the model was fit without covariates.
 8 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors

David S Fischer et al
Molecular Systems Biology
architecture was proposed to model antigen sequences (preprint: Jurtz et al, 2018). We generalized this architecture and found that all common sequence-embedding layer types can perform this prediction and that bidirectional LSTM-based networks perform best in terms of model uncertainty with AUC ROC of 0.82 (Fig 3C). Having built optimal antigen-embedding models, we assessed whether we find evidence for the ability of these models to general- ize in the antigen space on both the prediction task on antigens that are contained in the training set and the task held-out test antigens.
Firstly, we investigated whether antigen-embedding models have predictive advantages over similar categorical models on antigens that are in the training set. A lack of such predictive advantages would be indicative of an inability to learn generalizable embed- dings of antigen sequences. We found that antigen-wise categorical models have better predictive performance on the antigens they were trained on than sequence-embedding models, on both the IEDB (categorical model had a higher AUC ROC in 8 out of 11 anti- gens, Wilcoxon test, P &lt; 0.01) and the single-cell pMHC CD8+ T-cell data set (categorical model had a higher AUC ROC in 8 out of 8 anti- gens, Wilcoxon test, P &lt; 0.01; Fig 3D and E). We conclude that the previously proposed antigen sequence-embedding models are currently suboptimal for binding prediction on seen antigens. More- over, the analysis of seen antigens does not suggest that antigen- embedding models can learn representations of antigen sequences that allow for generalization in the antigen space.
Secondly, we tested the ability of sequence-embedding models to generalize to held-out antigens that are not contained in the training data. This task cannot be performed with models that treat antigens as categories. Firstly, we trained models on a subset of high- frequency antigens from IEDB and tested on low-frequency antigens from IEDB (Appendix Fig S5A, average of the top mean AUC ROC by antigen of 0.79). Secondly, we used a subset of observations of VDJdb with antigens not overlapping to IEDB as a test set (Appendix Fig S5B, average of the top mean AUC ROC by antigen of 0.86). Thirdly, we trained models on IEDB and tested on not over- lapping antigens from the single-cell pMHC CD8+ T-cell data (Appendix Fig S5C, average of the top mean AUC ROC by antigen of 0.59). While binding could be predicted for a few held-out antigens, the variation in prediction success across antigens and data sets was very large on the IEDB and VDJdb hold out scenarios. In the single- cell hold out (Appendix Fig S5C), in which we had sufficient data to assess prediction properly for each antigen, the predictivity was not very high (average AUC ROC 0.59). Thus, we cannot find evidence in the current TCR databases that extrapolation in the antigen space is possible based on current numbers of sampled antigens, in accor- dance with previous findings (preprint: Jurtz et al, 2018).
In summary, we do not find evidence that supports the usage of antigen-embedding models as they are outperformed by categorical models in the task of predicting antigens contained in the training data and because there is not enough antigen diversity in the avail- able training data to fit models that are able to generalize to unseen antigens.
Imputation of antigen specificity of T cells adds phenotypic information to single-cell studies
We showed that antigen specificity can be predicted based on TCR sequences from single-cell data. The inclusion of pMHC
binding detection in an experiment increases the sequencing and reagent costs compared with experiments involving CDR3 sequencing only; this will be especially pronounced in assays with many different antigens. However, antigen specificity is a layer of phenotypic information that adds to single-cell RNA-seq embeddings and can be used to relate activation states and cell types to specific disease-causing agents. The model classes shown here can be used to impute antigen specificity based on the CDR3 sequence only. Accordingly, pre-trained specificity-predicting models may serve as an alternative to including pMHCs in T-cell assays. All models discussed above can be used for the purpose of imputation. We found that the imputation of antigen specificity can give interpretable results in T-cell subpopulations identified based on the transcriptome (Fig 4): The observed labels are enriched in sub-regions of the transcriptome space (Fig 4A and C), which can be recovered in multiple cases based on the predicted labels (Fig 4B and D). This implies that cell states can be interpreted based on imputed specificity labels. In this scenar- io, one encounters the case of held-out donors. We showed above that prediction performance is strongly increased if donors are modeled (Fig 1C). Prediction to unseen donors requires the MHC alleles to be modeled directly; this requires larger patient cohorts than given in this study, though, and will be a focus of future research.
Discussion
Our results quantify the benefit of jointly modeling the TCR ɑ- and b-chains while accounting for single-cell variability through cell- and donor-specific covariates for the prediction of T-cell speci- ficity. Most importantly, we found that models that treat antigens as categorical outcome variables outperform those that model the TCR and antigen sequences jointly. Our results suggest that T-cell specificity can be predicted in an HLA genotype-specific fashion and thereby pave the way for research and development on all HLA types, beyond the commonly investigated type HLA-A*02:01. Here, we modeled donor rather than explicitly modeling MHC alle- les. In the future, one might directly use one-hot encoded MHC alleles as predictors when larger patient cohorts become available. The issue of MHC allele modeling is much simplified if pMHC panels are considered in isogenic mouse models only, which may be an important scenario for mouse-based single-cell immunology research. We showed that generalization to unseen antigens with antigen sequence-embedding models is currently challenging. However, these models will become more important as the diver- sity of assayed antigens increases. The models and analysis presented here can serve as a starting point for such studies in the future. Lastly, we showed that pMHC counts can be modeled as a measure of the strength of dextramer to T-cell binding and that multi-task models outperform single-task models in this setting, facilitating the integration of large pMHC panels in single experi- ments.
T-cell specificity complements standard immunological single- cell RNA-seq studies and can be used to uncover subpopulations that are expected to be activated during disease or used as an indicator of the presence of an antigen in a tissue. Consequently, we propose the computational imputation of T-cell specificity as
a 2020 The Authors
Molecular Systems Biology 16: e9416 | 2020 9 of 14

Molecular Systems Biology
David S Fischer et al
an important tool for immunologically focused single-cell RNA- seq experiments. Here, we chose a very conservative exclusion of putative doublet T cells that could be improved in the future based on the transcriptome-derived and the TCR sequence- derived doublet likelihoods of each observation. Imputation will reduce the number of pMHC species in experiments by allowing antigen prioritization or may entirely replace the pMHC reagents in this workflow. In addition to the economic value of this impu- tation, it will also offer unbiased specificity metrics that are not liable to errors in the pMHC panel choice. Such predictive models can also be directly applied to immunophenotyping by screening for TCRs that interact with known viral or cancer neoepitopes, enabling the characterization of a patient’s immuno- logical state and the stratification of subpopulations that are amenable to antigen-specific immunotherapies. Continuous T-cell binding strength models would permit the possibility of rational in silico TCR design, accelerating the development of TCR-based biologics.
Materials and Methods
Reagents and Tools table
Statistics
We present P-values for selected model performance comparisons. These P-values were computed on the comparison of two sets of performance metrics. We used Welch’s t-test if we compared two sets of performance metrics from two separate cross-validation sets, which is equivalent to the case of both sets sharing all model hyper- parameters other than cross-validation partition. We used the Wilcoxon test if we compared metrics across sets of models that vary in hyper-parameters, as one would no longer expect a unim- odal performance metric distribution in these cases.
Feed-forward network architectures
Here, we describe proposed architectures of the models that predict antigen specificity of a T-cell receptor (TCR) based on the CDR3 loop of both ɑ- and b-chains and on cell-specific covariates. Note that specificity-determining influences of CDR1 and CDR2 loops (Cole et al, 2009; Madura et al, 2013; Stadinski et al, 2014) and distal regions (Harris et al, 2016a,b) have also been demonstrated, but were not measured in the single-cell pMHC assay. All networks presented contain an initial amino acid embedding, a sequence data embedding block, and a final densely connected layer block.
Amino acid embedding
The choice of initial amino acid embedding may impact data and parameter efficiency of the model and therefore may impact the predictive power of models trained on data sets that are currently available. We used one-hot encoded amino acid embeddings, evolu- tionary substitution-inspired embeddings (BLOSUM), and learned embeddings. The learned embeddings were a 1 × 1 convolution on top of a BLOSUM encoding and were prepended to the sequence model layer stack. Here, channels are the initial amino acid embed- dings (we chose BLOSUM50) and filters are the learned amino acid embedding. This learned embedding can reduce the parameter size of the sequence model layer stack. All fits presented in the manu- script other than in Appendix Fig S1 are based on such a learned embedding with five filters. We anticipate that sequence-based embeddings will gain relevance in the context of extrapolation across antigens in the future. Here, parameter efficiency in the sequence models will play an important role and the 1 × 1 convolu- tion presented here is an intuitive first step in this direction.
Sequence data embedding
We screened multiple layer types in the sequence data embedding block: recurrent layers (bidirectional GRU and LSTM), self-attention, convolutional layers (simple convolutions and inception-like), and densely connected layers as a reference. Recurrent layer types and self-attention layers were previously useful for modeling language (Vaswani et al, 2017) and epitope (Wu et al, 2019) data. Convolu- tional layer types have been useful for modeling epitope (Han &amp; Kim, 2017; Vang &amp; Xie, 2017) and image (Szegedy et al, 2015) data. The sequence model layers retain positional information in subse- quent layers and can thereby build an increasingly abstract repre- sentation of the sequence. To achieve this on recurrent networks, we chose the output of a layer to be a position-wise network state which results in an output tensor of size (batch, positions × 2, output dimension) for a bidirectional network. This position-wise encoding occurs naturally in self-attention and convolutional networks. We did not use feature transforms with positional signals
  Reagent/ Resource
Reference or Source
 Identifier or Catalog Number
  Software
 python v3.7
scanpy v1.4
tensorflow v2.0.1
https://www.python.org/
https://pypi.org/project/scanpy/
https://pypi.org/project/tensorflow/
     Methods and Protocols
General note on data sets
In this study, we worked on data sets from public databases IEDB (Vita et al, 2019) and VDJdb (Shugay et al, 2018) and on a public data set from a single-cell pMHC-based T-cell specificity experi- ment (10x Genomics, 2019). IEDB and VDJdb contain pairs of binding T-cell receptors (TCRs) and antigens. In the single-cell experiment, cells were first treated with barcoded pMHCs and were then physically separated into droplets in a microfluidics setup. pMHCs captured in these droplet and T-cell receptor sequences associated with the captured cells are barcoded with a droplet-specific sequence so that both can be mapped to a single observation after sequencing (10x Genomics, 2019). Accordingly, one can obtain not only a list of bound TCRs and antigens but also pMHC counts for each TCR. These counts can be discretized into binding events and “spurious” binding or can be directly modeled as proposed in the main text. Importantly, one can easily establish the identity of multiple binding antigens to a single TCR sequence based on such pMHC counts. Two of the four donors (donors 1 and 2) were HLA-A*02:01 (10x Genomics, 2019), which was also the HLA type selected for in the IEDB and VDJdb samples. A detailed description of the HLA types and pMHC types used in this study is provided elsewhere (10x Geno- mics, 2019).
10 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors

David S Fischer et al
Molecular Systems Biology
(Vaswani et al, 2017) on the self-attention networks, so that the network has no knowledge of the original sequence-structure but can still retain inferred structure in subsequent layers. We presented models fit on the CDR3 loop of both ɑ- and b-chains of the TCR (Fig 1B) and models fit on the CDR3 loop of the b-chain and the antigen sequence (Fig 3B). In both cases, we needed to integrate two sequences. To this end, we either used separate sequence- embedding layer stacks for each sequence (all models presented in Fig 1 and models indicated as “separate” in Fig 3) or by appending the two padded sequences and using a single sequence-embedding layer stack (models indicated as “concatenated” in Fig 3). We reduced the positional encoding to a latent space of fixed dimen- sionality in the last sequence-embedding layer of recurrent networks by the emitted state of the model on the last element of the sequence in each direction. This last layer allows usage of the same final dense layers independent of input sequence length. Convolutional and self-attention networks were not built to be independent of sequence length. We did, however, pad the input sequences to mitigate this problem on the data handled in this paper. We used a residual connection across all sequence-embed- ding layers. Further layer-specific hyper-parameters can be extracted from the code supplied with this manuscript (Dataset EV1 and EV2).
Final densely connected layers
We fed the activation generated in the sequence-embedding block into a dense network that can integrate the sequence information with continuous or categorical donor- and cell-specific covariates. We modeled the binding event as a probability distribution over two states (bound and unbound) and compute the deviation of the model prediction from observed binding events via cross-entropy loss. Firstly, one can use such models to predict binding events on a single antigen represented as a single output node with a sigmoid activation function. Secondly, one can model a unique binding event among a panel of antigens with a vector of output nodes (one for each antigen and one node for non-binding) which are trans- formed with a softmax activation function.
Covariate processing
We set up a design matrix inspired by linear modeling to use as a covariate matrix. We modeled the donor as a categorical covariate, resulting in a one-hot encoding of the donor. We modeled total counts, negative-control pMHC counts, and surface protein counts as continuous covariates. We log(x + 1)-transformed negative- control pMHC counts and surface protein counts to increase the stability of training. We modeled total counts as the total count of mRNAs per cell divided by the mean total count.
Training, validation, and test splits
We used training data to compute parameter updates, validation data to control overfitting, and test data to compare models across hyper-parameters. Model training was terminated once a maxi- mum number of epochs were reached or if the validation loss was no longer decreasing. In the latter case, the model with the lowest validation in a sliding window of n epochs until the last epoch was chosen; n is given in the grid search scripts (Dataset EV3). The model metrics presented in this manuscript are metrics evalu- ated on the test data for models selected on cross-entropy
(categorical binding prediction) or mean-squared log error (dex- tramer count prediction) of the validation data. We provide train- ing curves for all models that contributed to panels in this manuscript in Dataset EV3.
Optimization
We used the ADAM optimizer throughout the manuscript for all models. We used learning rate schedules that reduce the learning rate at the time of training once plateaus in the validation metric are reached. The initial learning rate and all remaining hyper- parameters (batch size, number of epochs, patience, steps per epoch) were varied as indicated in the grid search hyper-para- meter list.
Model fitting objectives
We chose cross-entropy loss on sigmoid- or softmax-transformed output activation values to train models that predict binarized bind- ing events and mean-squared logarithmic error (msle) on exponenti- ated output activation values for models that predict continuous (count) binding affinities.
Performance metrics
We used AUC ROC, F1 scores, false-negative rates, and false- positive rates in the study to evaluate models that predict bind- ing probabilities. AUC ROC is useful if the observations cover the full range of classification thresholds and is useful because it provides a measure that summarizes all scalar classification thresholds. F1 scores can always be used to evaluate a classifier but rely on a strict threshold. We used AUC ROC where possible but complemented with F1 scores if the AUC ROC score may suf- fer from a disjointed support of test data set on the classification threshold. False-negative and false-positive rates are used in Appendix Fig S4 to emphasize how models trained on single-cell data generalize to data from IEBD and VDJdb in both the nega- tive and the positive classes separately. We used the R2 to evalu- ate the performance of models that predicted pMHC counts (positive integer space).
Single-cell immune repertoire (CD8+ T cell) data processing
Primary data processing
We downloaded the full data of all four donors from another study (10x Genomics, 2019). All data processing for each model fit is documented in the package code (Dataset EV1) and grid search scripts (Dataset EV2). The number of T-cell clonotypes per antigen varied drastically between the order of 100 and 104 (Appendix Fig S3A and B). Subsequently, we selected the eight most common anti- gens (ELAGIGILTV, GILGFVFTL, GLCTLVAML, KLGGALQAK, RLRAEAQVK, IVTDFSVIK, AVFDRKSDAK, RAKFKQLL) for categori- cal panel model fits to avoid issues with class imbalances. We used the binarized binding event prediction by the authors of the data set (10x Genomics, 2019; labeled “*_binder” in the files “*_binarized_- matrix.csv”) as a label for prediction. For the continuous case, in which we predicted pMHC counts, we chose the corresponding count data columns in the same file. Next, we performed multiple layers of observation filtering: (i) doublet removal, (ii) clonotype down-sampling, and (iii) class down-sampling. It was previously shown that doublets, namely, droplets containing two cells targeted with the same barcode, which cannot be distinguished in
a 2020 The Authors
Molecular Systems Biology 16: e9416 | 2020 11 of 14

Molecular Systems Biology
David S Fischer et al
downstream analysis steps, tend to be enriched in subsets of tran- scriptome-derived clusters (Wolock et al, 2019). We propose using the number of reconstructed TCR chain alleles to identify potential doublets and demonstrate that the so characterized doublets are indeed enriched in a particular cluster in each donor (Appendix Fig S2A–D). There are cells that have two active alleles for either TCR chain, but these cannot be easily separated from doublets that arise in the cell separation process. To avoid bias of the presented results by potential cellular doublets, we chose to exclude all cells showing more than one allele for either the ɑ- or the b-chain. We further investigated the overall contribution of potentially ambient mole- cules that give rise to all observed T cells and found that high- frequency chains do not dominate the overall signal (Appendix Fig S2E and F). This analysis presents an upper bound to the impact of ambient molecules on this experiment as evolutionary effects prob- ably also contribute to over-representation of particular chain sequences. Subsequently, we removed all cellular barcodes that contain more than one ɑ- or b-chain as mature CD8+ T cells are expected to only have a single functional ɑ- and b-chain allele. Next, we down-sampled each clonotype to a maximum of 10 observations to avoid biasing the training or test data to large clones. Here, we used clonotypes as defined by the authors of the data set in the files “*_clonotypes.csv” (10x Genomics, 2019). Lastly, we down-sampled the larger class to a maximum of twice the size of the smaller class when predicting a binary binding event for a single antigen. We did not perform this last step on multiclass and count prediction scenarios. We padded each CDR3 sequence to a length of 40 amino acids and concatenated these padded chain observations to a sequence of length 80 for models that were trained on both chains. We performed leave-one-donor- out cross-validation on models that did not take the donor iden- tity as a covariate. We sampled 25% of the full data clonotypes and assigned all of the corresponding cells to the test set for all models that did use the donor covariate. The latter case yielded 68,716 clonotypes and 91,495 cells across all four donors. All cross-validations shown across different models are based on threefold cross-validation with seeded test–train splits resulting in the same split across all hyper-parameters. We present an analy- sis of the clonotype diversity encountered in this data set in Appendix Fig S6.
Binarization of single-cell pMHC counts into bound and unbound states
We used the binarization described in the original publication (10x Genomics, 2019) for the raw counts to receive binary outcome labels: A total pMHC UMI count larger than 10 and at least five times as high as the highest observed UMI count across all negative- control pMHCs was required for a binding event. If more than one pMHC passed these criteria, the pMHC with the largest UMI count was chosen as the single binder.
Test set assembly for models fit on IEDB data
This section describes how the test described in Fig 3E and Appendix Fig S5C was prepared. The cells were filtered as described above. We then extracted one binding TCR-antigen pair per cell from this list. We used the remaining TCR-antigen pairs as validated negative examples and down-sampled these to the number of posi- tive observations to maintain class balance. All cross-validations shown across different models are based on threefold cross-
validation with seeded test–train splits resulting in the same split across all hyper-parameters.
IEDB data processing
Primary processing
We downloaded the data from the IEDB website (Vita et al, 2019) with the following filters: linear epitope, MHC restriction to HLA- A*02:01 and organism as human and only human. This yielded a list of matched TCR (mostly b-chain CDR3s) with bound antigens. We assigned TCR sequences to a single clonotype if they were perfectly matched and down-sampled all clonotypes to a single observation. We only extracted the b-chain and CDR3 sequences to a length of 40 amino acids. We padded the antigen sequences to a length of 25 amino acids. We sampled 10% of all observations as a test set. We generated negative samples for both training and test sets separately by generating unobserved pairs of TCR and antigens. Here, we assumed that all TCRs bind a unique antigen out of the set of all antigens present in the database so that any other pairing would not result in a binding event. This procedure yielded 9,697 observations for both the positive and the negative sets before the train–test split from 71 antigens.
Test set assembly for models fit on IEDB data
This section describes how the test depicted in Appendix Fig S5A was prepared. To explore the ability of antigen-embedding Tcell- Match models to generalize to unseen antigens, we fit such a model on the subset of high-frequency antigens of IEDB with at least five unique TCR sequences and tested the models on the remaining anti- gens. All cross-validations shown across different models are based on threefold cross-validation with seeded test–train splits resulting in the same split across all hyper-parameters.
VDJdb data processing
Primary processing
We provided an exploratory analysis of this data set in Appendix Fig S3 “exploration_vdjdb_data.*”. We downloaded the data from the VDJdb (Shugay et al, 2018) website with the following filters: Species: human, Gene (chain): TRB, MHC First chain allele(s): HLA-A*02:01. This yielded 3,964 records from 40 antigens. We assigned TCR sequences to a single clonotype if they were perfectly matched and down-sampled all clonotypes to a single observation. We only extracted the b-chain and CDR3 sequences to a length of 40 amino acids. We padded the antigen sequences to a length of 25 amino acids.
Test set assembly from VDJdb for models fit on IEDB data
This section describes how the test depicted in Fig 3D and Appendix Fig S5B was prepared. We sub-selected observations with matching or non-matching antigens with respect to the training set depending on the application (described in the figure caption or main text). All cross-validations shown across different models are based on threefold cross-validation with seeded test–train splits resulting in the same split across all hyper-parameters.
Data availability
The data sets and computer code produced in this study are avail- able in the following databases:
12 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors

David S Fischer et al
Molecular Systems Biology
• Modeling python package (TcellMatch) and analysis scripts: GitHub (https://github.com/theislab/tcellmatch). Model fits are available in Dataset EV4.
Expanded View for this article is available online.
Acknowledgments
We would like to thank Dr. Mike Stubbington for fruitful discussions on the topic of predicting T-cell specificity. D.S.F. acknowledges support from a German Research Foundation (DFG) fellowship through the Graduate School of Quanti- tative Biosciences Munich (QBM) [GSC 1006 to D.S.F.] and by the Joachim Herz Stiftung. B.S. acknowledges financial support from the Postdoctoral Fellowship Program of the Helmholtz Zentrum München. F.J.T. acknowledges financial support from the Graduate School QBM, the German Research Foundation (DFG) within the Collaborative Research Centre 1243, Subproject A17, by the Helmholtz Association (Incubator grant sparse2big, grant #ZT-I-0007), by the BMBF grant #01IS18036A, and grant #01IS18053A and by the Chan Zuckerberg Initiative DAF (advised fund of Silicon Valley Community Foundation, 182835). We thank the Center for Information Services and High Performance Computing (ZIH) at TU Dresden for generous allocations of computer time.
Author contributions
DSF and YW implemented the modes and performed the analysis. DSF, BS, and FJT wrote the manuscript.
Conflict of interest
F.J.T. reports receiving consulting fees from Roche Diagnostics GmbH and Cellarity Inc., and ownership interest in Cellarity Inc.
References
Bagaev DV, Vroomans RMA, Samir J, Stervbo U, Rius C, Dolton G, Greenshields-Watson A, Attaf M, Egorov ES, Zvyagin IV et al (2019) VDJdb in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium. Nucleic Acids Res 48: D1057 – D1062
Bentzen AK, Marquard AM, Lyngaa R, Saini SK, Ramskov S, Donia M, Such L, Furness AJS, McGranahan N, Rosenthal R et al (2016) Large-scale detection of antigen-specific T cells using peptide-MHC-I multimers labeled with DNA barcodes. Nat Biotechnol 34: 1037 – 1045
Borrman T, Cimons J, Cosiano M, Purcaro M, Pierce BG, Baker BM, Weng Z (2017) ATLAS: a database linking binding affinities with structures for wild-type and mutant TCR-pMHC complexes. Proteins 85: 908 – 916
Cho K, van Merrienboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y (2014) Learning Phrase Representations using RNN Encoder– Decoder for Statistical Machine Translation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp 1724 – 1734. https://doi.org/10.3115/v1/D14-1179
Cole DK, Yuan F, Rizkallah PJ, Miles JJ, Gostick E, Price DA, Gao GF, Jakobsen BK, Sewell AK (2009) Germ line-governed recognition of a cancer epitope by an immunodominant human T-cell receptor. J Biol Chem 284:
27281 – 27289
Flower DR, Phadwal K, Macdonald IK, Coveney PV, Davies MN, Wan S (2010) T-cell epitope prediction and immune complex simulation using molecular dynamics: state of the art and persisting challenges. Immunome Res 6(Suppl 2): S4
10x Genomics (2019) A New Way of Exploring Immunity - Linking Highly Multiplexed Antigen Recognition to Immune Repertoire and Phenotype - 10x Genomics. 10x Genomics. https://www.10xgenomics.com/resources/ application-notes/
Gielis S, Moris P, De Neuter N, Bittremieux W, Ogunjimi B, Laukens K, Meysman P (2018) TCRex: a webtool for the prediction of T-cell receptor sequence epitope specificity. Front Immunol https://doi.org/10.3389/fimmu. 2019.02820
Glanville J, Huang H, Nau A, Hatton O, Wagar LE, Rubelt F, Ji X, Han A, Krams SM, Pettus C et al (2017) Identifying specificity groups in the T cell receptor repertoire. Nature 547: 94 – 98
Han Y, Kim D (2017) Deep convolutional neural networks for pan-specific peptide-MHC class I binding prediction. BMC Bioinformatics 18: 585
Harris DT, Singh NK, Cai Q, Smith SN, Vander Kooi C, Procko E, Kranz DM, Baker BM (2016a) An engineered switch in T cell receptor specificity leads to an unusual but functional binding geometry. Structure 24: 1142 – 1154
Harris DT, Wang N, Riley TP, Anderson SD, Singh NK, Procko E, Baker BM, Kranz DM (2016b) Deep mutational scans as a guide to engineering high affinity T cell receptor interactions with peptide- bound major histocompatibility complex. J Biol Chem 291:
24566 – 24578
Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput
9: 1735 – 1780
Jokinen E, Huuhtanen J, Mustjoki S, Heinonen M, Lähdesmäki H (2019)
Determining epitope specificity of T cell receptors with TCRGP. bioRxiv
https://doi.org/10.1101/542332v2 [PREPRINT]
Jurtz VI, Jessen LE, Bentzen AK, Jespersen MC, Mahajan S, Vita R, Jensen KK,
Marcatili P, Hadrup SR, Peters B et al (2018) NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks. bioRxiv https://doi.org/10.1101/433706v1 [PREPRINT]
Klinger M, Pepin F, Wilkins J, Asbury T, Wittkop T, Zheng J, Moorhead M, Faham M (2015) Multiplex identification of antigen-specific T cell receptors using a combination of immune assays and immune receptor sequencing. PLoS One 10: e0141561
Lanzarotti E, Marcatili P, Nielsen M (2019) T-cell receptor cognate target prediction based on paired a and b chain sequence and structural CDR loop similarities. Front Immunol 10: 2080
Madura F, Rizkallah PJ, Miles KM, Holland CJ, Bulek AM, Fuller A, Schauenburg AJA, Miles JJ, Liddy N, Sami M et al (2013) T-cell receptor specificity maintained by altered thermodynamics. J Biol Chem 288: 18766 – 18775
Schuster M, Paliwal KK (1997) Bidirectional recurrent neural networks. IEEE Trans Signal Process 45: 2673 – 2681
Shugay M, Bagaev DV, Zvyagin IV, Vroomans RM, Crawford JC, Dolton G, Komech EA, Sycheva AL, Koneva AE, Egorov ES et al (2018) VDJdb: a curated database of T-cell receptor sequences with known antigen specificity. Nucleic Acids Res 46: D419 – D427
Singh NK, Riley TP, Baker SCB, Borrman T, Weng Z, Baker BM (2017) Emerging concepts in TCR specificity: rationalizing and (Maybe) predicting outcomes. J Immunol 199: 2203 – 2213
Stadinski BD, Trenh P, Duke B (2014) Effect of CDR3 sequences and distal V gene residues in regulating TCR–MHC contacts and ligand specificity. J Immunol: 192: 6071 – 6082
Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015) Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pp 1–9
a 2020 The Authors
Molecular Systems Biology 16: e9416 | 2020 13 of 14

Molecular Systems Biology
David S Fischer et al
Vang YS, Xie X (2017) HLA class I binding prediction via convolutional neural networks. Bioinformatics 33: 2658 – 2665
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser ŁU, Polosukhin I (2017) Attention is All you Need. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. pp 1 – 11
Vita R, Mahajan S, Overton JA, Dhanda SK, Martini S, Cantrell JR, Wheeler DK, Sette A, Peters B (2019) The immune epitope database (IEDB): 2018 update. Nucleic Acids Res 47: D339 – D343
Wolock SL, Lopez R, Klein AM (2019) Scrublet: computational identification of cell doublets in single-cell transcriptomic data. Cell Syst 8:
281 – 291.e9
Wu J, Wang W, Zhang J, Zhou B, Zhao W, Su Z, Gu X, Wu J, Zhou Z, Chen S (2019) DeepHLApan: A Deep Learning Approach for High-Confidence Neoantigen Prediction. Front Immunol 10: 2559
Zvyagin IV, Tsvetkov VO, Chudakov DM, Shugay M (2020) An overview of immunoinformatics approaches and databases linking T cell receptor repertoires to their antigen specificity. Immunogenetics 72: 77 – 84
License: This is an open access article under the terms of the Creative Commons Attribution 4.0 License, which permits use, distribution and reproduc- tion in any medium, provided the original work is properly cited.
 14 of 14 Molecular Systems Biology 16: e9416 | 2020
a 2020 The Authors
</Text>
        </Document>
        <Document ID="1AEBABF5-F65C-436C-BA17-638A66B51D0C">
            <Title>Source</Title>
            <Text>- The dataset for pretraining the model consisted of BA measurement data and MS-identified EL ligand data, excluding tumor-associated neopeptides used in fine-tuning the model.
- The binding affinity measurement dataset for pretraining the model was compiled from a snapshot of the Immune Epitope Database (IEDB)[{Vita:2015bh}] MHC ligands downloaded on Jan. 28, 2019(http://www.iedb.org/doc/mhc_ligand_full.zip) and the BA dataset augmented with the BD2013 dataset[{Kim:2014jg}](http://tools.iedb.org/static/main/binding_data_2013.zip).
- The MS-identified EL dataset  for pretraining the model consisted of the MS-identified ligands from IEDB, SysteMHC Atlas [{Shao:2017hb}] and 186,464 eluted peptides from 95 HLA-A, —B, -C and -G alleles[{Sarkizova:2019fu}].  </Text>
            <Notes>The affinity measurement dataset used for training and model selection was assembled from a snapshot of the Immune Epitope Database (IEDB) MHC ligands downloaded on Dec. 1, 2017 augmented with the BD2013 dataset (Kim et al., 2014).

Here, we expand our initial dataset of &gt;24,000 peptides from 16 cell lines and identify and characterize 186,464 eluted peptides from 95 HLA-A, -B, -C and -G alleles. We included HLA-G peptidomes because this HLA is impli- cated in maternal–fetal tolerance and is also upregulated in many cancers10,11. These data allow us to compare peptide length pref- erences and the spectrum of distinct and shared submotifs across HLA class I alleles, revealing the diversity and complexity of endog- enous HLA ligands. Using this information, we trained allele- and-length-specific and pan-allele-pan-length predictors, which identify 1.5-fold more peptides than conventional prediction tools when evaluating ligands directly detected by LC–MS/MS from 11 patient-derived tumor cell lines. The datasets of HLA binding pep- tides from mono-allelic cells and patient-derived tumors, as well as the prediction models (HLAthena) and interactive web tools, are all made publicly available. 

 </Notes>
        </Document>
        <Document ID="5F1DFC67-3910-462D-8AD4-E971113A7409">
            <Title>양방향 자기주목을 통한 pMHC 결합패턴 학습</Title>
            <Text>양방향 자기주목 기반의 언어모델인 BERT를 도용함으로써, 펩타이드-MHC 결합과 펩타이드 제시에서의 아미노산 간의 상호작용 패턴- 장단거리 상호작용, 집합적 상호작용, 일반적 상호작용 등-을 펩타이드 길이와 상관없이 효과적으로 반영하는 contextual representation(language model)을 사전 훈련 시킬 수 있다.
</Text>
        </Document>
        <Document ID="88903A84-890C-4361-B7C3-9EE59018EC3E">
            <Title>BassaniSternberg2015</Title>
            <Text>Mass spectrometry of HLA-I peptidomes
 - {BassaniSternberg:2015js}: Bassani-Sternberg, M. &amp; Pletscher-Frankild, S. Mass Spectrometry of Human Leukocyte Antigen Class I Peptidomes Reveals Strong Effects of Protein Abundance and Turnover on Antigen Presentation. Molecular &amp; Cellular … 14, 658–673 (2015).
 - We here present a rich and high confidence HLA-I peptidome, established by applying state-of-the-art mass-spectrometric techniques on a collection of seven cell lines. We investigate how abundance affects the propensity of proteins to be presented as measurable HLA peptides and whether or not there are specific protein classes that are overrepresented even independent of abundance. Likewise, we explore how to use in silico immunogenicity tools on the set of identified HLA peptides from cancer-associated proteins, with a view to select vaccine candidates.
#

- Data: https://www.mcponline.org/content/suppl/2015/01/09/M114.042812.DC1</Text>
        </Document>
        <Document ID="276EF430-6B2B-4746-86BD-CDCB7B06F16F">
            <Title>An Overview of Multi-Task Learning in Deep Neural Networks∗ – TensorMSA</Title>
            <Text>
TENSORMSA
TOGGLE NAVIGATION
AN OVERVIEW OF MULTI-TASK LEARNING IN DEEP NEURAL NETWORKS∗
BY TMDDNO1@NAVER.COM | AUGUST 28, 2018 | NO COMMENTS | PAPER STUDY
An Overview of Multi-Task Learning in Deep Neural Networks (Paper, Blog)
1. 서론

보통 우리가 머신러닝으로 무언가를 할때 통상적으로 하나의 모델을 훈련하거나 복수의 모델을 훈련하여 Ensemble 하는 형태로 우리가 추구하고자 하는 목표를 잘 해석할 수 있는 모델을 만들고자 노력한다. 그 이후에는 Fine 튜닝 등의 방법으로 성능을 끌어올리고자 노력하다가 더 이상 모델의 성능의 개선되지 않으면 모델 개발을 완료하는 형태로 진행한다. 이러한 방법으로도 어지간한 경우에는 원하는 성능을 달성할 수 있다. 하지만 이렇게만 모델 개발을 종료하게 되면 어쩌면 우리의 모델의 도움이 되었을 수도 있는 연관된 데이터들을 활용할 수가 없다. 이러한 문제를 해결하고자 Multi Tasking 이라는 방법이 연구되었으며, 이 논문에서는 몇 가지 대표적인 Multi Tasking 방법과 응용 방법을 소개하고자 한다.

2. 대표적인 두 가지 방법

(1) Hard Parameter Sharing 

Hard Parameter Sharing 은 가장 흔히 사용되는 방법으로 Hidden Layer 를 공유하고 Task 별로 일부 개별적인 Layer 를 가지고 가는 형태로 활용된다. 이러한 방법의 활용은 당연히 Over Fitting 을 방지하는데 효과가 있다. (N 개의 Task 에 전부 Over Fit 되기는 어려움으로.. )



(2) Soft Parameter Sharing 

Soft Parameter Sharing 는 조금 다르게 각각의 Task 별로 별도의 Layer 를 가지고 있다. 다만, 각각의 Layer 가 비슷해 질 수 있도록 L2 Distance 를 사용한다. (아마도 LOSS 함수 구성시 Classification Loss 와 각각의 Layer 간의 거리를 최소화 하는 Loss 를 조합하는 형태일 것이라고 예상함)



3. Multi Task Learning 이 동작하는 이유 

(1) Implicit Data Augmentation  

어짜피 딥러닝은 High Dimension Data 를 Deep Learning 을 통해 Low Dimension 에서 Representation 하는 것에 목적이 있는데 , 특정한 데이터 셋에 종속적으로 모델을 훈련하는 것보다 다양한 데이터를 활용하여 더욱더 범용적인 Representation 을 만들어 낼 수 있다면, Over fitting 을 회피할 수 있다.

(2) Attention Focusing   

Task 가 만약 매우 지저분하거나 데이터가 제한적인 경우 모델이 관련이 있는 것과 관련이 없는 것을 구분하는 것이 쉽지 않을 것이다. 이러한 경우에 MTL 기법은 모델에 관련있는 것과 관련 없는 것을 구분하기 위한 추가적인 정보를 제공하여 줄 것이다.

(3) Eavesdropping  

어떤 Feature G 가 있다고 하자, Task A 에서는 이러한 Feature 를 학습하게 어려운데 Task B 에서는 학습하기가 용이하다고 하자, 이러한 문제를 해결하기에 가장 쉬운 방법은 Task B 를 통해서 Feature G 를 학습하기에 중요한 포인트를 전달 받아서 Task A 를 훈련하는 것일 것이다.

(4) Representation Bias   

하나의 Task 뿐만 아니라 다른 Task 에서도 선호되는 Representation 을 만들 수 있도록 Bias 를 줄 수 있다. 이를 통해서 모델의 Generalization 을 달성하는데 도움을 줄 수 있다.

(5) Regularization    

MTL 은 Regularizer 의 역할도 수행을 하는데, 귀납적인 Bias 를 제공하여 준다. 결론적으로 Over fitting 의 위험등을 감소 시킬 수 있다.

4. 최근 MTL 에 관한 연구  

(1) “Learning Multiple Tasks with Deep Relationship Networks” NIPS 2017 (링크)

Pretrained 된 CNN(Alex Net) 을 활용하여 Fine Tune 개념으로 CNN Layer 를 활용하며, Multi Tasking 까지 적용하는 형태로 뒤의 FCC 레이어는 각각의 Task 별로 존재하는 형태로 자세한 사항은 직접 논문을 참조하기를 바란다.



(2) Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in
Person Attribute Classification (링크)

Bottom up 방식으로  Thin 아키택쳐로 시작해서 복잡한 아키택쳐로 확대해 가는 형태로 훈련을 진행하면서 Dynamic 하게 Branch 를 추가해 가는 형태를 제안하고 있는다.

 

(3) Cross-stitch Networks for Multi-task Learning (링크)

일반적인 Soft Parameter Sharing 과 비슷한 형태로 구성되어 Shared CNN Layer 에 대해서 각 Task 별 별도의 Hidden Layer 를 구성하고 각 Layer 간의 거리를 최소화하는 방향으로 훈련하는 개념으로 구성되어 있다. 다만, Cross-Stitch 라는 개념이 추가되어 있는데, Task A 와 Task B 가 있다고 했을 때,  각 Task 에서 다음 Layer 의 Input 을 계산할 때, 아래와 같이 서로간에 Linear 한 관계로 값을 Combine 하는 구조가 추가되어 있다.





(4) A Joint Many-Task Model: Growing a NN for Multiple NLP Tasks (링크)

NLP 에는 POS, Chunking, Dependency Parsing, entailment 등 다양한 Task 들이 존재하는데,  보통은 각각의 목적별로 별도의 아키택쳐를 구성하지만, 여기에서는 모든 Task 를 하나의 아키택쳐로 구성하고, 훈련하고자 하는 대상별로 다른 LOSS Function 을 구성하여 하나의 아키택쳐에서 같이 훈련시키고 있다.

 



 

 

(5) Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics (링크)

Share 하는 Layer 을 훈련하는 형태가 아닌 복수의 목적을 갖는 Loss Function 을 설계하고 이를 Sum 하여 한번에 훈련하는 형태를 제시하고 있다.



(3) Learning what to share between loosely related tasks (링크)

지금까지 나온 Hard Parameter Sharing, Cross-Stitch , NLP의 Task Hierarchy 등 다양한 기법들을 복합적으로 적용한, MTL 아키택쳐이다.



5. Examples with Tensorflow   

(1) Linear Transformation 

간단한 Linear Regression 을 Tensorflow 로 구현한 모습이다. MTL 을 설명하기 전에 가장 간단한 구조를 한번 설명하고 있다.



&lt;code class="language-python" data-lang="python"&gt;# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np
# ======================
# Define the Graph
# ======================
# Create Placeholders For X And Y (for feeding in data)
X = tf.placeholder("float",[10, 10],name="X") # Our input is 10x10
Y = tf.placeholder("float", [10, 1],name="Y") # Our output is 10x1
# Create a Trainable Variable, "W", our weights for the linear transformation
initial_W = np.zeros((10,1))
W = tf.Variable(initial_W, name="W", dtype="float32")
# Define Your Loss Function
Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name="Loss")
with tf.Session() as sess: # set up the session
    sess.run(tf.initialize_all_variables())
    Model_Loss = sess.run(
                Loss, # the first argument is the name of the Tensorflow variabl you want to return
                { # the second argument is the data for the placeholders
                  X: np.random.rand(10,10),
                  Y: np.random.rand(10).reshape(-1,1)
                })
    print(Model_Loss)&lt;/code class="language-python" data-lang="python"&gt;
(2) Simple Hard Parameter Sharing 

Hard Parameter Sharing 의 간단한 예가 되겠다. 공유되는 Shared Layer 를 가지고 있는 상태에서 각각의 Task 가 별도의 Layer 를 가지고 있으며, 두개의 Task 는 각각 다른 X,Y Set 으로 Loss 구해서 BackPropagation 을 진행하고 있다.



#  GRAPH CODE
# ============
# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np
# ======================
# Define the Graph
# ======================
# Define the Placeholders
X = tf.placeholder("float", [10, 10], name="X")
Y1 = tf.placeholder("float", [10, 20], name="Y1")
Y2 = tf.placeholder("float", [10, 20], name="Y2")
# Define the weights for the layers
initial_shared_layer_weights = np.random.rand(10,20)
initial_Y1_layer_weights = np.random.rand(20,20)
initial_Y2_layer_weights = np.random.rand(20,20)
shared_layer_weights = tf.Variable(initial_shared_layer_weights, name="share_W", dtype="float32")
Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name="share_Y1", dtype="float32")
Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name="share_Y2", dtype="float32")
# Construct the Layers with RELU Activations
shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))
Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))
Y2_layer = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))
# Calculate Loss
Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)
Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)
# optimisers
Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)
Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss)
(3) Joint Loss 

각각 따로 따로 모델을 훈련하는 것이 아닌 Joint Loss 를 활용하여 훈련하는 예제이다



#  GRAPH CODE
# ============
# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np
# ======================
# Define the Graph
# ======================
# Define the Placeholders
X = tf.placeholder("float", [10, 10], name="X")
Y1 = tf.placeholder("float", [10, 20], name="Y1")
Y2 = tf.placeholder("float", [10, 20], name="Y2")
# Define the weights for the layers
initial_shared_layer_weights = np.random.rand(10,20)
initial_Y1_layer_weights = np.random.rand(20,20)
initial_Y2_layer_weights = np.random.rand(20,20)
shared_layer_weights = tf.Variable(initial_shared_layer_weights, name="share_W", dtype="float32")
Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name="share_Y1", dtype="float32")
Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name="share_Y2", dtype="float32")
# Construct the Layers with RELU Activations
shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))
Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))
Y2_layer = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))
# Calculate Loss
Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)
Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)
Joint_Loss = Y1_Loss + Y2_Loss
# optimisers
Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)
Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)
Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss)
# Joint Training
# Calculation (Session) Code
# ==========================
# open the session
with tf.Session() as session:
    session.run(tf.initialize_all_variables())
    _, Joint_Loss = session.run([Optimiser, Joint_Loss],
                    {
                      X: np.random.rand(10,10)*10,
                      Y1: np.random.rand(10,20)*10,
                      Y2: np.random.rand(10,20)*10
                      })
    print(Joint_Loss)
6. Real Examples    

(1) VoC Example  

고객의 STT 데이터를 가지고 현업에서 실제 사용할 모델을 만들 때 응용했던 결과를 보여주고 있다. 현업에서의 문제는 Inbound Call 이 들어올 때, 고객이 선택한 카테고리 정보 외에 우리가 분류하고 싶은 형태의 정답지(Labeled Data)가 없거나 매우 적다는 것이 문제다. 이러한 문제를 해결하기 위해서 Transfer Learning, Multi Tasking, Learning by Association 3가지 기법을 적용한 결과이다. 전체적으로 데이터가 적어서 발생하는 Over Fitting 을 회피하기 위한 기법들이 위주로 적용되었으며, 아래와 같이 Walker Loss, Classification Loss 와 더불어 우리가 목적하였던 고객의 감정 분석 모델의 Loss 와 Accuracy 도 향상됨을 볼 수 있다. 실제 테스트 결과 완전 별도로 작업한 Test Set 1,000건 기준으로 79.5 F1 Score 를 달성 하였다.



Post navigation
← SCORING
WAVENET: A GENERATIVE MODEL FOR RAW AUDIO  →
LEAVE A REPLY

Your email address will not be published. Required fields are marked *

Comment


Name *


Email *


Website


 Notify me of follow-up comments by email.

 Notify me of new posts by email.




BLOG STATS

180,269 hits
ShopIsle powered by WordPress

</Text>
        </Document>
        <Document ID="BD4A845F-4272-484B-AEF7-9EBF19F309CB">
            <Title>Finetuning the model</Title>
            <Text>We performed fine-tuning the pre-trained model in two rounds, switching the freezing layers. In the first fine-tuning round, the model was trained while freezing the embedding layer and top two encoding layers, where the weights of the layers were not updated. The freezing layers was extended to top six encoding layers in the second fine-tuning round. In each fine-tuning round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs[{Prechelt:2012 }].  We used the Adam optimizer[{Kingma D:2014}] with learning rate 0.0001 and 128 batch size in all epochs. PyTorch deep learning library(https://pytorch.org) were used for implementing our model.</Text>
        </Document>
        <Document ID="E2DAB33D-D631-4965-AF2C-57DD005088AF">
            <Title>Binding Affinity Prediction</Title>
            <Text>In training the binding affinity prediction model with mean square error loss, the pMHC-I binding affinity values were transformed using 1-log(a)/log(50,000), where a is the measured binding affinity to scale between 0 and 1[Nielsen, 2003]. </Text>
            <Notes>1.	Nielsen, M. et al. Reliable prediction of T-cell epitopes using neural networks with novel sequence representations. Protein Sci. 12, 1007–1017 (2003).

the output values used in the training and testing of the neural networks on a scale between 0 and 1. The transformation is defined as 1-log(a)/log(50,000), where a is the measured binding affinity. In this transformation high binding peptides, with a mea- sured affinity stronger than 50 nM, are assigned an output value above 0.638, intermediate binding peptides, with an affinity stron- ger than 500 nM, an output value above 0.426, and peptides, with an affinity weaker than 500 nM, an output value below 0.426. Peptides that have an affinity weaker than 50,000 nM are assigned an output value of 0.0.</Notes>
        </Document>
        <Document ID="AE43CA13-604F-4FE8-BCA5-A78AD76A79D6">
            <Title>말하고자 하는 연구내용</Title>
            <Text>본 논문에서는  MHC-bound 후보 (neo-)peptide의 예측을 위한 BERT를 도용한 전이학습(semi-supervised?) 방법을 제안한다. 
MHC-펩타이드 binding affinity 데이터를 사용한 선행학습을 통해 MHC-peptide의 결합에 대한 contextual language model을 construct한다. The pre-trained model을 특정 MS-identified natural ligand 데이터셋을 사용한 fine-tuning을 통해 최종 (Neo-) 펩타이드 예측 모델을 구축하고 cross-validation을 통해 성능을 검증한다. 또한, final predictive 모델을 external dataset을 사용하여 독립 성능 검증을 수행하고 다른 예측 방법들 보다 범용성과 정확도가 우수하다는 것을 증명한다.
 


</Text>
            <Notes>￼


1)  펩타이드-주조직적합성복합체(MHC: Major Histocompatibility Complex) 결합데이터셋(IEDB 결합데이터, Mass Spectrometry 펩타이드 데이터 등)으로 부터 펩타이드 아미노산 서열과 MHC 아미노산 서열을 20개 아미노산이 단어로 표현되는 문장으로 변환하여 펩타이드-MHC 결합 사전(Peptide Corpus)를 구축한다.
2) 펩타이드-MHC 결합데이터에 대응하는 각 펩타이드 문장(Peptide Sentence) 데이터를 사용하여 양방향 언어모델(BERT: Bidirectional Encoder Representations from Transformer)을 구축하고 선행 학습을 통해 펩타이드-MHC 결합 언어 모델(Peptide Language Model)을 구축한다. 이 때 언어모델의 범용성(generality)을 위해 입력  펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 삭제 또는 치환하여 선행학습을 수행한다. 펩타이드와 MHC 서열에서의 아미노산 삭제 또는 치환은 임의로 특정 비율의 서열 단편에 대해 수행하되 펩타이드 서열의 경우는 알려진 위치특이적 점수행렬(PSSM: Position-Specific Scoring Matrix)에 기반한 편향성을 주고, MHC 서열의 경우는 MHC 결합사이트의 진화적 보존성에 기반한 편향성을 부여한다. 
3) 특정 인종 또는 지역에 집중된 HLA 유전형에 대한 신항원 펩타이드-MHC 결합데이터셋을 사용한 선행학습된 펩타이드-MHC 결합 언어모델의 미세튜닝을 통하여 최종  신항원 펩타이드-MHC 결합 언어모델을 구축한다


Neopeptide language model에게 입력 펩타이드-MHC 서열들을 선행 학습시킬 때 prior knowledge를 가미하기 위해  펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 치환한 서열들을 추가로 학습시킨다. 


펩타이드-MHC 결합에서의 서열을 20개 아미노산이 단어로 표현되는 문장으로 인코딩하여 </Notes>
        </Document>
        <Document ID="5DB05CFD-5D6F-4897-A706-E74DD4D542F0">
            <Title>T-cell 면역유도에서 TCR의 역할 및 중요성</Title>
            <Text>T-cell은 dimeric surface protein인 T-cell receptor(TCR)을 통하여 세포 표면으로 제시된 MHC-bound epitope을 인지하고 활성화되고 증폭된다.

The peptide-MHC (pMHC) complex is then presented to T cells which can recognize the complex via T cell receptor (TCR) proteins, consequently leading to T cell activation and proliferation by clonal expansion [1]. During clonal expansion, a fraction of T cells gain a long-living memory phenotype and therefore a clonal population of T cells with identical TCR rearrangements remain for years against the recognized antigen [2], thus forming a potentially decodable immunological signature. Learning these signatures could have implications in broad range of clinical applications including infectious diseases, autoimmunity and tumor immunology. 
Antigen recognition is one of the key factors of T cell-mediated immunity. T cells interact via a dimeric surface protein, the T-cell receptor  (TCR), with an antigen presented on a major histocompatibility  complex (MHC) located on the surface of antigen-presenting cells. This  presenting cell can be experimentally modeled via an MHC multimer  with an immobilized antigen (pMHC).
</Text>
        </Document>
        <Document ID="8435D019-B9B9-4615-8BDC-F7783789006D">
            <Title>ML/DL using MS peptidome data</Title>
            <Text>NetMHCpan4.0
MHCflurry 1.2.0
DeepLigand
</Text>
        </Document>
        <Document ID="F95868B2-CAE7-4810-86D9-D5068D18847B">
            <Title>Model Architecture</Title>
            <Text>Predictive model의 구조는 BERT base model을 참조하여 구성되었다[{Devlin:2018uk}]. 
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M). 

</Text>
            <Notes>We will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017)[{Vaswani:2017ul}] as well as excellent guides such as “The Annotated Transformer.”[https://nlp.seas.harvard.edu/2018/04/03/attention.html]
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). 
BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4 

</Notes>
        </Document>
        <Document ID="4429E692-D772-4F9B-BBCA-D39AC2CA5791">
            <Title>Independent validation results</Title>
            <Text>Fine-tuned 된 최종 모델은 11 patient-derived tumor cell lines으로부터 detected ligands를 포함한external dataset을 사용하여 independently evaluated 되었다. 
이 데이터셋을 사용한 NetMHCpan, MHCflurry와의 예측 성능 비교를 통하여 fine-tuned 최종 예측 모델의 독립 검증을 수행하였다. 
&lt;성능비교 Table 1&gt;
Table1에서 전체적인 성능은 our model이 좋았다. 특히, HLA-A*31:01 alllele에 대한 예측 성능은 다른 방법들보다 outperformed하였다.</Text>
        </Document>
        <Document ID="55C83DD3-D3F6-4816-8A1B-ECC0ED7E9CE1">
            <Title>Supervised pretraining in NLP, BERT</Title>
            <Synopsis>Self Supervised Representation Learning in NLP</Synopsis>
            <Text>최근의 NLP에서의 발전들은 자기주도 학습이 unlabeled sequence 데이터로부터 유용한 정보를 뽑아내는데 powerful한 도구라는 것을 증명하였다[{Peters:2018vk}, {Devlin:2018uk}, {Radford:2019vn}]. One successful approach, BERT[{Devlin et al., 2018}]
}]는 masked language model과 next token prediction 등의 self-supervised bidrectional 선행학습 언어 모델을 구축하고 이를 11개의 downstream tasks에서 SOTA를 달성하였다.
이러한 self-supervised transfer learning 전략, - 즉 unlabeled 대용량의 데이터셋를 사용하여 self-supervised pretraining된 모델을 downstream task에서 적은 수의 데이터셋으로 finetuning하여 최종 모델을 구축하는 전략 - 은 학습데이터셋이 부족한 task에서 신뢰할 만 예측 모델을 구축하는데 효과적이다.
</Text>
        </Document>
        <Document ID="CCCBB00B-7B66-4774-84C7-428957054D8F">
            <Text>{EmmiJokinen:2021bn}</Text>
        </Document>
        <Document ID="7574218D-766E-40BB-A6A5-45FA31BA9A98">
            <Title>IEDB binding affinity</Title>
            <Text> -  {Vita:2015bh} : Vita, R. et al. The immune epitope database (IEDB) 3.0. Nucleic Acids Research 43, D405–12 (2015).

 - 정성: positive-high, positive, positive-intermediate, positive-low, negative, 정량: nM
 - Data: http://www.iedb.org/doc/mhc_ligand_full.zip 
</Text>
        </Document>
        <Document ID="D9258C60-3AAA-4A58-A50C-BFDAFB4890E1">
            <Title>Prediction of epitope specificity</Title>
            <Text>이러한 TCR diversity에도 불구하고 최근의 연구들은 특정한 공통 target epitope은 TCRs는 종종 공통적인 서열 특징들을 공유한다는 것을 제시하였고 결과적으로 epitope specificity을 예측할 수 있는 모델을 구축할 수 있는 가능성을 제시하였다.
 신뢰성있는 예측모델을 구축하기 위해서는 특정 에피토프와 결합하는 TCR 서열데이터 확보가 
Nevertheless, profiling of epitope-specific TCRs remains exhaustive as they  require sample-consuming experiments with distinct pMHC-multimers for each epitope of  interest. Therefore, there is a great need for models that examine which epitopes a TCR can  recognize or to which TCRs an epitope can bind to [15]
TCRs binding the same MHC-peptide  may share similarities. Thus, while for public clones the task of  deciphering the relation between a peptide and the TCR binding  is based on tallying the candidate public TCR, for most highly  cross-reactive TCRs, a probabilistic approach is required.
 Given the fact that  different individuals have different TCR repertoires, in theory,  epitope immunogenicity should differ between individuals.  However, there are several examples of immunodominant  epitopes that are targeted by the adaptive immunity of different  individuals (11–15). Indeed, immunodominant epitopes have  already been clinically utilized, for example, in the interferon-  gamma release assay, a clinically available peripheral blood  assay to determine if the subject has previously been sensitized  by Mycobacterium tuberculosis (Mtb) (16, 17). To explain  this phenomenon, it is plausible to hypothesize that those  immunodominant epitopes share some intrinsic patterns which  render them more prone to be recognized by the T cell immunity  of multiple individuals. Because TCR-epitope interaction is  governed by the physicochemical principles like other protein-  protein interactions, more immunodominant epitopes are  expected to have a higher chance of stronger interaction  when scanned by a large set of TCRs. In this scenario,  we could utilize TCR sequences as “baits” to probe highly  immunodominant epitopes.
 Despite this potential diversity, TCRs from T cells that recognize the same pMHC epitope often share conserved sequence features, suggesting that it may be possible to predictively model epitope specificity. 
Growing evidence   suggests that T cell receptors specific to a common target share common properties (17, 18).   Increasing amounts of data are now available linking TCR sequences to their cognate targets.   The predictive power of the model proposed here is in line with these observations. 
</Text>
        </Document>
        <Document ID="EDDF805F-DEB3-40A9-95D6-C6269D9CCF73">
            <Title>Conclusion</Title>
        </Document>
        <Document ID="11E2714D-70B1-4ED2-8E9C-68CA5123FC7B">
            <Title>Datasets</Title>
            <Text>{Devlin:2018uk}은 BooksCorpus (800M words)와 English Wikipedia (2,500M words) 같은  대용량의 Corpus의 텍스트를 사용하여 unsupervised한 manner로 pretraining을 수행하여 corpus에서의 contextual 한 word representations(embedding vector로 구현되는)를 통한 언어모델을 구축한 후 이를 11개의 NLP task에서의 specific한 task dataset을 사용하여 fine-tuning을 수행하여 SOTA 성능을 보이는 최종 예측 모델을 구축하였다. 

MHC와 결합하는 모든 펩타이드가 Tumor-specific 신항원 펩타이드는 아니다. [{Biotechnol:fe}]. MHC 분자와 결합하는 펩타이드 중에 오직 30% 미만이 tumor cell에서 process 되고 이중 일부만이 T-cell을 유도하고 clinically-relevant immunogenicity를 갖는 neopeptide 일 수 있다[{Vitiello:2017dm}, {Ott:2017ft}].
따라서, BERT에서의 pretraining에서 fine-tuning에 걸친 전이학습 전략을 employing하면, 먼저 최대한 많은 수의 펩타이드-MHC 결합데이터를 사용하여 펩타이드-MHC 결합에 대한 언어모델을 pre-training한 후, pretrained 언어 모델을 HLA allele 별로 tumor-specific한 신항원 펩타이드 결합데이터셋을 사용하여 fine-tuning을 수행하여 최종 학습 모델을 구축한다.
 
</Text>
            <Notes>NetMHCPan4.0
One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this binding to MHC is the most selective step in peptide Ag presentation. However, other factors, including Ag processing (12) and the sta- bility of the peptide–MHC complex (13), could influence the likelihood of a given peptide to be presented as an MHC ligand. Similarly, the length distribution of peptides available for binding to MHC molecules is impacted by other steps in the processing and presentation pathway, such as TAP transport and ERAP trimming, which are not reflected in binding data in itself (6). 

MHCflurry

We used the MHC class I dataset curated by O’Donnell et al. (2018). This dataset consists of 525 672 binding affinity and mass spectrometry measurements collected from IEDB, Abelin et al. (2017) and Kim et al. (2014). This dataset also contains 2 541 370 non-ligand sequences (decoys) sampled from the protein-coding transcripts that also contained the mass spectrometry-identified pep- tides (hits) based on protein sequences in the UCSC hg19 proteome and transcript quantification from RNA sequencing of the relevant cell line (B721.221). As described in Abelin et al., for an allele with n hits, 100n decoys of length 8–15 were sampled, weighting tran- scripts by the number of hits. 

</Notes>
        </Document>
        <Document ID="4E75867E-0DCE-4D8A-817E-C298ABE9B6DE">
            <Title>SysteMHC</Title>
            <Text>SysteMHC Atlas
 - {Shao:2017hb}: Shao, W. et al. The SysteMHC Atlas project. Nucleic Acids Research 46, D1237–D1247 (2017).

 - Immunopeptidomics datasets used for building the first version of the SysteMHC Atlas. Data from 23 projects that collectively generated 1184 raw MS files constitute the initial contents of the SysteMHC Atlas.
In May 2017, ∼29.5 million MS/MS spectra were searched using a uniform and well-tested computational pipeline and yielded 250,768 and 1,458,698 distinct peptides with iProphet probability P ≥ 0.9 and P &gt; 0.0, respectively. After applying strict confidence filters for the identification of class I and class II peptides, 119,073 high-confidence HLA class I peptides (peptide FDR 1%, 8–12 amino acids) were identified and annotated to specific HLA-A, -B or -C alleles using an automated annotation strategy as described. For class II molecules, 73,465 high-confidence peptides were identified (peptide FDR 1%, 10–25 amino acids, belonging to groups of two or more peptides with an overlap of at least four amino acids). Of note, the assignment of peptides to specific HLA class II alleles will be considered in the future as soon as robust bioinformatics tools for class II peptide annotation become openly available (26). The high-confidence class I and class II peptides were mapped onto 13,132 and 7,704 of the human UniProtKB/Swiss-Prot proteins, respectively.
  -Data:  https://systemhcatlas.org/Builds_for_download/180409_master_final.tgz
</Text>
        </Document>
        <Document ID="3A58E4B4-D4C2-4AB4-9BF4-3C7D9376D4C8">
            <Title>Exploring datasets</Title>
            <Text>Final dataset은 구성되었다. 총 0000, 000 alleles, % binders</Text>
            <Notes>IEDB entries with non-class I, non-specific, mutant, or unparseable allele names were dropped, as were those with peptides identified by MS or containing post-translational modifications or noncanonical amino acids. This yielded an IEDB dataset of 143,898 quantitative and 43,978 qualitative affinity measurements. Of 179,692 measurements in the BD2013 dataset (Kim et al., 2014), 57,506 were not also present in the IEDB dataset. After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the combined dataset consists of 230,735 measurements across 130 alleles. </Notes>
        </Document>
        <Document ID="79DE2CBE-A5F2-4DCD-AF2B-2DE5B7FECBD5">
            <Title>기존의 예측방법들</Title>
        </Document>
        <Document ID="CCC18EA0-028E-48BD-B119-E36928977BD7">
            <Title>Positive datasets</Title>
            <Text>For the first fine-tuning round, the positive dataset containing epitope-specific TCR CDR3beta sequences was compiled from three data sources on May 2021: Dash et al[{Dash:2017go}] containing epitope-specific paired TCRα and TCRβ chains for  three epitopes from humans and for seven epitopes from mice, two manually curated databases that contains pathology-associated TCR sequences, such as VDJdb[{Bagaev:2019hf}](https://vdjdb.cdr3.net) and McPAS-TCR[{Tickotsky:2017bo}](http://friedmanlab.weizmann.ac.il/McPAS-TCR/).  Every entry in VDJdb has been given a confidence score between 0  and 3 (0: critical information missing, 1: medium confidence, 2: high confidence, 3: very high  confidence). We selected all VDJdb entries with a confidence score at least 1.  For the second fine-tuning round, COVID-19 epitope-specific CDR3beta sequence data was collected from Immune Epitope Database[{Vita:2015}](https://iedb.org) on June 2021.
After selecting all epitopes that have at least 20 CDR3beta sequences and removing duplicates with the same {epitope, CDR3beta} from each fine-tuning dataset, two datasets contained 12,619 positive data points covering 80 epitopes and 49,273 positive data points covering 145 epitopes, respectively.




</Text>
        </Document>
        <Document ID="EB12DB0F-66F9-4C99-913B-F320D2CF17EF">
            <Title>Enormous TCR diversity</Title>
            <Text>TCRs are generated by genomic rearrangement of the germline TCR loci from a large collection of variable(V), diversity(D) and joining(J) gene segments. During T cell development, TCRs are formed by chains through the V(D)J recombination in each locus independently. It is estimated that this rearrangement can result in the range of 1018 different TCRs, which provides enormous diversity of epitope-specific T cell repertoires.

T cells undergo non-homologous recombination during T cell development, which involves rearrangement of the germline TCR loci from a large collection of variable (V), diversity (D) and joining (J) gene segments as well as template-independent insertions and deletions at the V-D and D-J junctions [3, 4]. 
TCRs are formed by a pair of α- and β-chains (90-95% of  T cells) or γ and δ-chains (5-10%) and V(D)J recombination happens in each locus independently. It is estimated that this rearrangement can result in the range of 1018 different TCR  genes [5, 6] which provides enormous diversity for epitope-specific T cell repertoires.
The complementarity determining regions (CDRs) of a TCR determine whether the TCR recognizes and binds to an antigen or not [7]. Of these regions, CDR3 is the most variable and primarily interacts with the peptide, while CDR1 and CDR2 primarily interact with the peptide binding groove of the MHC protein presenting the peptide but they can also be directly in con- tact with the peptide [8, 9]. Dash et al. [10] noted that also a loop between CDR2 and CDR3 (IMGT positions 81-86 [11]), which they called CDR2.5, has sometimes been observed to make contact with pMHC in solved structures. 
It is well known that the CDR3β of a TCR is important in recognizing peptides presented to the T cell, but it still remains unclear which specific physicochemical or structural features of the CDR3β or of other parts of the TCR determine the antigen recognition specificity of the T cell. High-throughput sequencing of V- and J-segment enriched DNA has enabled large-scale characterization of TCR sequences, initially only for the CDR3β with bulk methods [6, 12] but recently for the whole paired TCRαβ at single-cell resolution using plate or droplet-based methods [13, 14]. Nevertheless, profiling of epitope-specific TCRs remains exhaustive as they require sample-consuming experiments with distinct pMHC-multimers for each epitope of interest. Therefore, there is a great need for models that examine which epitopes a TCR can recognize or to which TCRs an epitope can bind to [15]. Curated databases of experimentally verified TCR-peptide interactions have recently been launched, such as VDJdb, IEDB, and McPAS [16–18]. Such data sources enable more comprehensive, data-driven analysis of TCR- peptide interactions, and allow the use of statistical machine learning techniques for the afore- mentioned tasks. Yet only a few computational methods for predicting recognition between TCRs and epitopes [10, 19–22] and for clustering similar TCRs [9, 23, 24] have been published. In addition to supervised and unsupervised methods for predicting TCR-epitope interactions, computational methods and web services such as [25] have also been proposed to predict the structure of TCRs based on their amino acid sequences. 

The affinity of a TCR for a given peptide epitope and the specificity of the binding are governed by the heterodimeric αβ T-cell receptors (2). While both chains have been reported to be important to affect binding, we show here that for many TCR-peptide pairs the TCR’s binding to target MHC-peptide can be determined with high accuracy using the β-chain only. Including the alpha chain in the analysis is essential for better accuracy. However, as many experimental settings provide, only beta chains, a binding prediction tool based on these chains is of importance. 
Within the TCRβ chain, the complementarity-determining region 1 (CDR1) and CDR2 loops of the TCR contact the MHC alpha-helices while the hypervariable complementary determining regions (CDR3) interact mainly with the peptide (1, 2). In both TCRα and TCRβ chains, CDR3 loops have the highest sequence diversity and are the principal determinants of receptor binding specificity. 

T cells are defined by a heterodimeric surface receptor, the T cell receptor (TCR), that mediates recognition of pathogen- associated epitopes through interactions with peptide and major histocompatibility complexes (pMHCs). TCRs are generated by genomic rearrangement of the germline TCR locus, a process termed V(D)J recombination, that has the potential to generate marked diversity of TCRs (estimated to range from 1015 (ref. 1) to as high as 1061 (ref. 2) possible receptors).



</Text>
        </Document>
        <Document ID="CD00FFE0-BB2D-4BD4-8B53-C899C3A0A22E">
            <Title>Ott2017</Title>
            <Text>An immunogenic personal neoantigen vaccine for patients with melanoma 
 
- {Ott:2017ft}
#

#

- Data: N/A
</Text>
        </Document>
        <Document ID="D954E4DC-2133-4243-B1DB-B92FC8CEE257">
            <Title>새로운 폴더</Title>
        </Document>
        <Document ID="C1021751-BA69-4227-885F-C6DEFD0E8D29">
            <Title>Epitope specificity 예측의 필요성</Title>
            <Text>이러한 TCR diversity에도 불구하고 최근의 연구들은 특정한 공통 target epitope을 인지하는 TCRs는 종종 공통적인 서열 특징들을 공유한다는 것을 제시하였고 결과적으로 epitope specificity를 데이터 기반으로 예측할 수 있는 가능성을 제시하였다[{Dash:2017go},{Glanville:2017js}].
</Text>
        </Document>
        <Document ID="D370B833-377D-4E02-908A-748C54666386">
            <Title>Performance evaluation of the fine-tuned models</Title>
        </Document>
        <Document ID="8E8C7847-F3D5-4A8C-8E41-44ADD00F4A80">
            <Title>Supervised pretrained protein embedding, TABE</Title>
        </Document>
        <Document ID="B9FA15E2-3288-4BA5-B3E1-A85D3789FFD7">
            <Title>Pre-training the model</Title>
            <Text>Loss curves in Masked LM and Binding Affinity Prediction training epochs
</Text>
        </Document>
        <Document ID="E3957CE4-B02A-4243-BD36-CB9CAFAFDC7A">
            <Title>Data preprocessing</Title>
            <Text>&lt;IEDB&gt;
The IEDB entries were filtered to MHC allele class = I, Epitope Object Type = Linear peptide and Allele Name consistent with human HLA class I nomenclature with four-digit typing (that is, regex: “^HLA-[ABC]\\*[0–9]{2}:[0–9]{2}$”), resulting in a dataset with 0000 quantitative or qualitative measurements. Redundant ligand data with the same ligand sequence and MHC molecule were removed. A peptide was considered a binder if it had a quantitative affinity of &lt;500 nM or qualitative label of ‘Positive’, ‘Positive-High’, ‘Positive-Intermediate’ or ‘Positive-Low’.

After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, , the curated IEDB dataset contained 0000 measurements for 0000 HLA-I alleles.   
We excluded the subsets grouped by allele and peptide length, which have fewer than 25 entries.
The length of the ligands ranged from 4 to 37 aa. All lengths that were associated with &gt;= 0.5% of total ligands were selected for further analysis; this included lengths 8–15 aa and included 99% of the assay entries.
This yielded an IEDB dataset of 143,898(?) quantitative and 43,978(?) qualitative affinity measurements. 정성적 데이터는 [{ODonnell:2018fv}]에서 한 방법으로 정량데이터로 변환되었다; For qualitative affinity data, we assigned the following inequalities and measurement values: positive-high, &lt; 100 nM; positive, &lt; 500 nM, positive-intermediate, &lt; 1,000 nM; positive-low, &lt; 5,000 nM; negative, &gt; 5,000 nM. 

&lt;Kim2014&gt;
The Kim et al dataset contained quantitative half maximal inhibitory concentration(IC50) measurements for 186,281 peptides, where a peptide was denoted as a binder if it had measurement value &lt; 500 nM.

&lt;MS ligands&gt;
The additional MS ligand dataset contained 0000 ligands(binder peptides) formed by combining 0000 ligands from SysteMHC[{Shao:2017hb}] and 0000 ligands from [{Sarkizova:2019fu}] that was extended from [{Abelin:2017cn}], where  the list of MS ligands from SysteMHC Atlas were filtered to remove entries with low confidence (prob &lt; 0.99). 
Lower cases of a peptide in [{Sarkizova:2019fu}] indicates variable modification

&lt;Final&gt;
The final combined dataset used for pretraining our model contained 0000 quantitative or qualitative measurements that were labeled as binder(1) or non-binder(0), after selecting peptides of length 8-15, dropping alleles with fewer than 25 measurements, and selecting only one entry of the major label in duplicated cases removing duplicated records with the same {allele, peptide} pair. 

This yielded an IEDB dataset of 0000 binding affinity measurements and 0000 qualitative MS ligands labeled as Positive-High, Positive-Intermediate, Positive, Positive-Low and Negative.
The 186,281 quantitative half-max? (IC50) measurements in {Kim:2014jg} were labeled as Positive if a &lt; 500 nM, otherwise Negative. 
The MS ligands from SysteMHC Atlas were filtered to remove entries with low confidence (prob &lt; 0.99) and then labeled as Positive.
- The MS-identified EL dataset  for pretraining the model consisted of the MS-identified ligands from IEDB, SysteMHC Atlas [{Shao:2017hb}] and the eluted ligands from 95 HLA-I -allelic cell lines[{Sarkizova:2019fu}].  
Pretraining을 위해 사용된 최종 dataset은 중복된 ligands 제거하면서, 두 데이터셋을 combine한 최종 데이터셋을
The final dataset with  ? measurements that combines two datasets, removing redundant entries with the same peptide sequence and MHC allele, was used for pretraining the model.

</Text>
            <Notes>MHCflurry
IEDB entries with non-class I, non-specific, mutant, or unparseable allele names were dropped, as were those with peptides identified by MS or containing post-translational modifications or noncanonical amino acids. This yielded an IEDB dataset of 143,898 quantitative and 43,978 qualitative affinity measurements. Of 179,692 measurements in the BD2013 dataset (Kim et al., 2014), 57,506 were not also present in the IEDB dataset. After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the combined dataset consists of 230,735 measurements across 130 alleles. 

NetMHCPan4.0
Data on all class I MHC ligand elution assays available in the IEDB database (http://www.iedb.org) were collected, including the ligand sequence, details of the source protein, position of the ligand in the source protein, and the restricting allele of the ligand. There were 160,527 distinct assays in total, and the length of the ligands ranged from 4 to 37 aa. All lengths that were associated with $0.5% of total ligands were selected for further analysis; this included lengths 8–15 aa and included 99% of the assay entries. 
The restricting MHC molecule of the ligands was analyzed, and entries with alleles listed unambiguously were selected. For example, some entries for which the HLA alleles are listed as just the gene name, as well as alleles from chicken, horse, cow, and mouse for which we did not have binding prediction algorithms, were excluded. Representative alleles were assigned for entries where only supertypes were listed (e.g., HLA-A*26:01 for HLA-A26). Thus, there were 127 class I molecules from human and mouse in the selected data set. Redundant entries with the same ligand sequence and MHC molecule were removed, and MHC molecules with $50 ligand entries were selected. This included 55 class I molecules, and the number of available ligands per molecule varied widely from 50 to 9500.

The IEDB entries were filtered to MHC allele class = I, Epitope Object Type = Linear peptide and Allele Name consistent with human HLA class I nomenclature with four-digit typing (that is, regex: “^HLA-[ABC]\\*[0–9]{2}:[0–9]{2}$”). Peptides with quantitative measurements in units other than nM were removed and so were the following three assay types due to detected inconsistency between predicted (NetMHC 3.0) and actual affinity: ‘purified MHC/direct/radioactivity/dissociation constant KD’, ‘purified MHC/direct/fluorescence/half maximal effective concentration (EC50)’ and ‘cellular MHC/direct/fluorescence/ half maximal effective concentration (EC50)’. A peptide was considered a binder if it had a quantitative affinity of &lt;500 nM or qualitative label of ‘Positive’, ‘Positive-High’, ‘Positive-Intermediate’ or ‘Positive-Low’. In cases where multiple records are available for the same {peptide, allele} pair, we either took the mean affinity or removed the peptide when the difference between the maximum and minimum log-transformed affinities (1 − log(nM)/log(50,000)) was &gt;0.2. Similarly, peptides with multiple qualitative, were removed if the same number of positive and negative labels were found or kept otherwise Major 라벨의 엔트리 하나만 남겨두고 나머지는 제거하였다.
Our previously published data for 16 HLA-A and -B alleles were removed from the analysis of IEDB counts (PubMedID = 28228285). 
After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the curated IEDB dataset consisted of 0000 measurements for 0000 HLA-I alleles.  
</Notes>
        </Document>
        <Document ID="C4A65FAB-1FED-4DDE-9A5F-9ADB49BD7C94">
            <Title>T-cell 면역대응이 유도되는 과정과 TCR의 핵심적인 역할</Title>
        </Document>
        <Document ID="45F6EB93-C4E9-4497-B7C9-F29817494DFF">
            <Title>BD2013</Title>
            <Text> - {Kim:2014jg}: Kim, Y. et al. Dataset size and composition impact the reliability of performance benchmarks for peptide-MHC binding predictions. 15, 241 (2014).

 - Peptide-MHC quantitative binding affinity with inequality(&gt;, &lt;, =)
 - Data: http://tools.iedb.org/main/datasets/, http://tools.iedb.org/static/main/benchmark_mhci_reliability.tar.gz


</Text>
        </Document>
        <Document ID="9C3C3F8E-5925-4B65-A744-0C2551B9E41C">
            <Title>ML/DL 방법의 한계점</Title>
        </Document>
        <Document ID="E66782ED-6E85-453B-A16B-53D3BB790792">
            <Title>DeepLigand</Title>
            <Text>#

Fig. 1. Schematics of DeepLigand. DeepLigand consists of a binding affinity prediction module and a peptide embedding module. The affinity prediction module takes as input a pair of MHC and peptide sequence and predicts the mean and variance their binding affinity using a deep residual network. The peptide embedding module is a deep language model (ELMo) that embeds each peptide into a vector representation. The outputs from the two modules are concatenated and provided as input to one fully connected (FC) layer with sigmoid activation to predict whether the input peptide is a natural ligand of the input MHC. As a pre-training step, the embedding module is trained on the natural ligands in the training set for a given CV split. Then the affinity prediction module and the combining layer is jointly trained on all training examples taking as input the MHC and peptide sequences as well as the peptide embeddings produced by the pre-trained embedding module 

DeepLigand는 peptide 서열(‘sentence’)에서의 각 아미노산(‘word’)의 contextual한 vector representation(‘language model’)을 구하기 위해 ELMo 기반의 peptide embedding module을 통하여 MS-eluted natural ligands 데이터셋을 사전 학습하였다.

ELMo는 LSTM 기반 bi-directional language model이다. 문장에서의 단어간의 long-term dependancies를 self-attention 기반의 Transformer[{Vaswani:2017ul}]가 LSTMs 보다 효과적으로 다룰 수 있다. 

ELMo를 기반으로 한 DeepLigand는 peptide의 vector representation을 사전 학습할 때 펩타이드 아미노산과 MHC contact residue 간의 상호작용을 고려하지 않았다(아마도 LSTM의 long-term relationship(AA interactions)을 다루는 것에서의 한계 때문일 수 있다)

</Text>
        </Document>
        <Document ID="4B3ACD70-F7D5-4C83-834B-227BAD730C73">
            <Title>BERT 논문정리</Title>
            <Text>tmax.ai
ABOUT ARCHIVES CATEGORIES TAGS SEARCH
BERT 논문정리
● 23 Feb 2019

Written by Minho Park
BERT: Pre-trainig of Deep Bidirectional Transformers for Language Understanding
최근에 NLP 연구분야에서 핫한 모델인 BERT 논문을 읽고 정리하는 포스트입니다.
구성은 논문을 쭉 읽어나가며 정리한 포스트기 때문에 논문과 같은 순서로 정리하였습니다.
Abstract

BERT : Bidirectional Encoder Representations form Transformer
논문의 제목에서 볼 수 있듯이, 본 논문은 “Attention is all you need(Vaswani et al., 2017)”(arxiv)에서 소개한 Transformer 구조를 활용한 Language Representation에 관한 논문입니다.
Transformer에 대한 자세한 구조를 알고 싶은 분은 위 논문을 읽어보시거나, 다음 블로그 MChromiak’s blog를 참고하시면 좋을 듯 합니다.
BERT는 기본적으로, wiki나 book data와 같은 대용랑 unlabeled data로 모델을 미리 학습 시킨 후, 특정 task를 가지고 있는 labeled data로 transfer learning을 하는 모델입니다.
BERT이전에 이러한 접근 방법을 가지는 모델이 몇가지 있었습니다. 대용량 unlabeld corpus를 통해 language model을 학습하고, 이를 토대로 뒤쪽에 특정 task를 처리하는 network를 붙이는 방식(ELMo, OpenAI GPT…)
하지만 BERT 논문에서는 이전의 모델의 접근 방식은 shallow bidirectional 또는 unidirectional하므로 language representation이 부족하다고 표현하였습니다.
게다가 BERT는 특정 task를 처리하기 위해 새로운 network를 붙일 필요 없이, BERT 모델 자체의 fine-tuning을 통해 해당 task의 state-of-the-art를 달성했다고합니다.
1. Introduction

Introduction에서는 BERT와 비슷한 접근 방식을 가지고 있는 기존 model에 대한 개략적인 소개를 합니다.
Language model pre-training은 여러 NLP task의 성능을 향상시키는데에 탁월한 효과가 있다고 알려져 있습니다. (Dai and Le, 2015; Peters et al., 2018, 2018; Radford et al., 2018; …)
이러한 NLP task는 token-level task인 Named Entity Recognition(NER)에서부터 SQuAD question answering task와 같은 task까지 광범위한 부분을 커버합니다
이런 pre-trained language representation을 적용하는 방식은 크게 두가지 방식이 있습니다. 하나는 feature-based 또 다른 하나는 fine-tuning 방식입니다.
feature-based approach : 특정 task를 수행하는 network에 pre-trained language representation을 추가적인 feature로 제공. 즉, 두 개의 network를 붙여서 사용한다고 보면 됩니다. 대표적인 모델 : ELMo(Peters et al., 2018)
fine-tuning approach : task-specific한 parameter를 최대한 줄이고, pre-trained된 parameter들을 downstream task 학습을 통해 조금만 바꿔주는(fine-tuning) 방식. 대표적인 모델 : Generative Pre-trained Transformer(OpenAI GPT) (Radford et al., 2018)
앞에 소개한 ELMo, OpenAI GPT는 pre-training시에 동일한 objective funtion으로 학습을 수행합니다, 하지만 BERT는 새로운 방식으로 pre-trained Language Representation을 학습했고 이것은 매우 효과적이었습니다.
BERT의 pre-training 방법론


그림1. BERT, GPT, ELMo (출처 : BERT 논문)

BERT pre-training의 새로운 방법론은 크게 2가지로 나눌 수 있습니다. 하나는 Masked Language Model(MLM), 또 다른 하나는 next sentence prediction이다.
기존 방법론 : 앞에 소개한 ELMo, OpenAI GPT는 일반적인 language model을 사용하였습니다. 일반적인 language model이란, 앞의 n 개의 단어를 가지고 뒤의 단어를 예측하는 모델을 세우는 것입니다(n-gram). 하지만 이는 필연적으로 unidirectional할 수 밖에 없고, 이러한 단점을 극복하기 위해 ELMo에서는 Bi-LSTM으로 양방향성을 가지려고 노력하지만, 굉장히 shallow한 양방향성 (단방향 concat 단방향)만을 가질 수 밖에 없었습니다(그림1).
Masked Language Model(MLM) : MLM은 input에서 무작위하게 몇개의 token을 mask 시킵니다. 그리고 이를 Transformer 구조에 넣어서 주변 단어의 context만을 보고 mask된 단어를 예측하는 모델입니다. OpenAI GPT도 Transformer 구조를 사용하지만, 앞의 단어들만 보고 뒷 단어를 예측하는 Transformer decoder구조를 사용합니다(그림1). 이와 달리 BERT에서는 input 전체와 mask된 token을 한번에 Transformer encoder에 넣고 원래 token 값을 예측하므로(그림1) deep bidirectional 하다고 할 수 있습니다. BERT의 MLM에 대해서는 뒷장의 Pre-training Tasks에서 더 자세히 설명하겠습니다.
next sentence prediction : 이것은 간단하게, 두 문장을 pre-training시에 같이 넣어줘서 두 문장이 이어지는 문장인지 아닌지 맞추는 것입니다. pre-training시에는 50:50 비율로 실제로 이어지는 두 문장과 랜덤하게 추출된 두 문장을 넣어줘서 BERT가 맞추게 시킵니다. 이러한 task는 실제 Natural Language Inference와 같은 task를 수행할 때 도움이 됩니다.
2. Related Work

ELMo, OpenAI GPT와 같은 모델이 존재하고, 앞에서 충분히 소개하였기 때문에 생략하도록 하겠습니다. 자세한 내용에서는 BERT 논문을 참고 바랍니다.
3. BERT

BERT의 아키텍처는 Attention is all you need에서 소개된 Transformer를 사용하지만, pre-training과 fine-tuning시의 아키텍처를 조금 다르게하여 Transfer Learning을 용이하게 만드는 것이 핵심입니다.
3.1 Model Architecture
BERT는 transformer 중에서도 encoder 부분만을 사용합니다. 이에 대한 자세한 내용은 Vaswani et al (2017) 또는 tensor2tensor를 참고 바랍니다.
BERT는 모델의 크기에 따라 base 모델과 large 모델을 제공합니다.
BERT_base : L=12, H=768, A=12, Total Parameters = 110M
BERT_large : L=24, H=1024, A=16, Total Parameters = 340M
L : transformer block의 layer 수, H : hidden size, A : self-attention heads 수, feed-forward/filter size = 4H
여기서 BERT_base 모델의 경우, OpenAI GPT모델과 hyper parameter가 동일합니다. 여기서 BERT의 저자가 의도한 바는, 모델의 하이퍼 파라미터가 동일하더라도, pre-training concept를 바꾸어 주는 것만으로 훨씬 높은 성능을 낼 수 있다는 것을 보여주고자 하는 것 같습니다.
OpenAI GPT모델의 경우 그림1에서 볼 수 있듯, next token 만을 맞추어내는 기본적인 language model 방식을 사용하였고, 그를 위해 transformer decoder를 사용했습니다. 하지만 BERT는 MLM과 NSP를 위해 self-attention을 수행하는 transformer encoder구조를 사용했음을 알 수 있습니다.
실제로 대부분의 NLP task SOTA는 BERT_large모델로 이루어 냈습니다.
3.2 Input Representation


그림2. bert input representation (출처: BERT 논문)

BERT의 input은 그림 2와 같이 3가지 embedding 값의 합으로 이루어져 있습니다.
WordPiece embedding을 사용합니다. WordPiece(Wu et al., 2016)에 대한 자세한 내용은 논문 링크를 참고 하시거나, lovit님의 블로그글을 참고 바랍니다. BERT english의 경우 30000개의 token을 사용하였습니다.
그림 2에서 볼 수 있듯이, Position embedding을 사용합니다. 이는 Transformer에서 사용한 방식과 같으며, jalammer의 블로그 글을 참고하시면 position embedding 뿐만 아니라 transformer의 전체적인 구조를 이해 하실 수 있습니다.
모든 sentence의 첫번째 token은 언제나 [CLS](special classification token) 입니다. 이 [CLS] token은 transformer 전체층을 다 거치고 나면 token sequence의 결합된 의미를 가지게 되는데, 여기에 간단한 classifier를 붙이면 단일 문장, 또는 연속된 문장의 classification을 쉽게 할 수 있게 됩니다. 만약 classification task가 아니라면 이 token은 무시하면 됩니다.
Sentence pair는 합쳐져서 single sequence로 입력되게 됩니다. 각각의 Sentence는 실제로는 수 개의 sentence로 이루어져 있을 수 있습니다(eg. QA task의 경우 [Question, Paragraph]에서 Paragraph가 여러개의 문장). 그래서 두 개의 문장을 구분하기 위해, 첫째로는 [SEP] token 사용, 둘째로는 Segment embedding을 사용하여 앞의 문장에는 sentence A embedding, 뒤의 문장에는 sentence B embedding을 더해줍니다.(모두 고정된 값)
만약 문장이 하나만 들어간다면 sentence A embedding만을 사용합니다.
3.3 Pre-training Tasks
기존의 ELMO나 GPT는 left to right or right to left Language Model을 사용하여 pre-training을 하지만, BERT는 이와 다르게 2가지의 새로운 unsupervised prediction task로 pre-training을 수행합니다.
3.3.1 Task #1: Masked LM
Introduction의 pre-training 방법론에서 설명한 내용과 동일한 내용입니다.
이번 장에서는 MLM이 구체적으로 어떤 식으로 수행되는 지에 대해서 설명하겠습니다.


그림3. BERT Masked Language Model (출처: rani horev’s blog : BERT explained)

그림 3에서 볼 수 있듯, 일단 단어 중의 일부를 [MASK] token 으로 바꾸어 줍니다. 바꾸어 주는 비율은 15% 입니다.
그리고 plain text를 tokenization하는 방법은 input representation에서 설명한 바와 같이 WordPiece(Wu et al., 2016)를 사용합니다.
이를 통하여 LM의 left-to-right (혹은 r2l)을 통하여 문장 전체를 predict하는 방법론과는 달리, [MASK] token 만을 predict하는 pre-training task를 수행합니다.
이 [MASK] token은 pre-training에만 사용되고, fine-tuning시에는 사용되지 않습니다. 해당 token을 맞추어 내는 task를 수행하면서, BERT는 문맥을 파악하는 능력을 길러내게 됩니다.
15%의 [MASK] token을 만들어 낼 때, 몇가지 추가적인 처리를 더 해주게 됩니다. 그것은 다음과 같습니다.
80%의 경우 : token을 [MASK]로 바꿉니다. eg., my dog is hairy -&gt; my dog is [MASK]
10%의 경우 : token을 random word로 바꾸어 줍니다. eg., my dog is hariy -&gt; my dog is apple
10%의 경우 : token을 원래의 단어로 그대로 놔둡니다. 이는 실제 관측된 단어에 대한 표상을 bias해주기 위해 실시합니다.
pre-trained 되는 Transformer encoder의 입장에서는 어떤 단어를 predict하라고 하는건지, 혹은 random word로 바뀌었는지 알 수 없습니다. Transformer encoder는 그냥 모든 token에 대해서 distributional contextual representation을 유지하도록 강제합니다.
또한 random word로 바꾸는 것 때문에 모델의 language understanding능력에 해를 끼친다고 생각 할 수 있지만, 바뀌는 부분이 1.5%(15%의 10%)에 불과하므로, 해를 끼치지 않습니다.
또한 MLM은 보통의 LM 보다 converge하는데에 많은 training step이 필요하지만, emperical하게는 LM보다 훨씬 빠르게 좋은 성능을 냅니다. 이는 Section 5.3에서 자세히 설명하겠습니다.
3.3.2 Task #2: Next Sentence prediction
이 task 또한 Introduction의 pre-training 방법론에서 설명한 내용입니다.
이 pre-training task 수행하는 이유는, 여러 중요한 NLP task중에 QA나 Natural Language Inference(NLI)와 같이 두 문장 사이의 관계를 이해하는 것이 중요한 것들이기 때문입니다.. 이들은 language modeling에서 capture되지 않습니다.
그래서 BERT에서는 corpus에서 두 문장을 이어 붙여 이것이 원래의 corpus에서 바로 이어 붙여져 있던 문장인지를 맞추는 binarized next sentence prediction task를 수행합니다.
50% : sentence A, B가 실제 next sentence
50% : sentence A, B가 corpus에서 random으로 뽑힌(관계가 없는) 두 문장
예를 들어
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] LABEL = IsNext

Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext

pre-training이 완료되면, 이 task는 97~98%의 accuracy를 달성했습니다. 이러한 간단한 task를 부여해도, QA와 NLI에 굉장히 의미있는 성능 향상을 이루어 냈습니다. 이는 section5. Ablation Studies에서 자세히 설명이 되어있습니다.
3.4 Pre-training Procedure
pre-training의 기본적인 절차는 LM에서 수행하는 것과 같습니다.
BERT_english의 경우 BookCorpus (Zhu et al., 2015) (800M words)와 English Wikipedia (2,500M words)를 사용하였습니다. Wikipedia 데이터에서는 text passage만 추출하여 사용했다고 합니다. 이유는, long contiguous sequence만을 학습시키고 싶어서입니다.
input pre-processing
먼저, NSP를 위해 sentence를 뽑아서 embedding A, B를 먹여줍니다. 물론 50%는 진짜 next sentence, 나머지는 random sentence를 사용합니다.
이 모든 토큰이 합쳐진 길이는 512개 이하여야 합니다. (OOM 때문)
이후 Masking 작업을 해줍니다.
pre-training 시의 Hyper Parameters
batch size : 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps -&gt; 3.3 billion word corpus의 40 epochs
Adam optimizer, learning rate : 1e-4, β1=0.9
β
1
=
0.9
, β2=0.999
β
2
=
0.999
, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, linear decay of the learning rate
Dropout prob: 0.1 for all layers
using gelu activation Hendrycks and Gimpel, 2016
BERT_base - 4 TPUs, BERT_large - 16 TPUs를 사용하여 4일동안 학습
3.5 Fine-tuning Procedure
sequence-level classification tasks에 대해서는 BERT fine-tuning과정이 매우 straightforward합니다.
input sequence에 대해서 일정한 차원수의 representation 결과를 얻고 싶기 때문에, [CLS] token의 Transformer output값을 사용합니다.
[CLS] token의 벡터는 H차원을 가집니다. C∈ℝH
C
∈
R
H
여기서 classify하고 싶은 갯수(K)에 따라 classification layer를 붙여 줍니다. classification layer : W∈ℝK×H
W
∈
R
K
×
H
label probabilities는 standard softmax로 계산 됩니다. P=softmax(CWT)
P
=
s
o
f
t
m
a
x
(
C
W
T
)
W matrix와 BERT의 모든 파라미터가 같이 fine-tuning 됩니다.
span-level, token-level prediction tasks의 경우에는, 위의 과정에서 약간 변형시켜 fine-tuning이 진행됩니다. 자세한 내용은 Section 4에서 설명하도록 하겠습니다.
fine-tuning시의 Hyper-Parameter
몇가지를 제외하고는 pre-training때의 hyper parameter와 대부분 동일합니다.
다른점은 batch size, learning rate, trainig epochs 수 입니다.
optimal hyperparameter는 task마다 달라지지만, 다음에 제시하는 것을 사용하면 대부분 잘 학습됩니다.
Batch size: 16, 32 Learning rage (Adam): 5e-5, 3e-5, 2e-5 Number of epochs : 3, 4

fine-tuning시의 dataset의 크기가 클수록 hyperparameter에 영향을 덜 받고 잘 training 됨을 관측할 수 있었다고 합니다.
Fine-tuning은 굉장히 빠르게 학습되며(pre-training에 비해) 이로 인해 최적의 hyperparameter 탐색을 exhaustive search로 찾아내도 무방합니다.
3.6 Comparison of BERT and OpenAI GPT
OpenAI GPT와 BERT는 transformer구조를 사용한다는 점에 있어서 공통점을 갖지만, BERT가 훨씬 좋은 성능을 갖습니다. 이 차이는 앞에 설명한 MLM task, NSP task를 수행하는 것과 별개로 또 더 다른점을 갖기에 생깁니다.
GPT의 경우 BookCorpus(800M words)만을 pre-training에 사용; BERT는 거기에 + Wikipedia(2500M words) 사용
GPT는 [SEP]과 [CLS] token을 fine-tuning시에만 추가하여 학습; BERT는 pre-training시에도 학습 (NSP task가 존재하기 때문에 가능)
GPT : 32,000 words/batch for 1M steps ; BERT : 128,000 words/batch ofr 1M steps
GPT는 모든 fine-tuning을 수행할 때 learning rate : 5e-5를 사용; BERT는 task-specific하게 조절하여 사용
위에 나열된 차이점에 대한 효과는 ablation experiments가 수행된 Section 5.1에 설명되어 있습니다.
4. Experiments

이번 섹션에서는, 11개의 NLP tasks에 대한 BERT fine-tuning에 대해서 설명하겠습니다.


그림 4. BERT experiments result (출처: BERT 논문)

그림 4를 참고하면, 각 fine-tuning 유형마다 어떻게 학습하는지를 알아 볼 수 있습니다.
(a), (b)는 sequence-level task, (c)와 (d)는 token-level task입니다. sequence-level task의 경우는 3.5 Fine-tuning Procedure에서 설명을 드렸습니다.
(c)의 QA task경우는, Question에 정답이 되는 Paragraph의 substring을 뽑아내는 것이므로, [SEP] token 이후의 token들에서 Start/End Span을 찾아내는 task를 수행합니다.
(d)의 경우는 Named Entity Recognition(NER)이나 형태소 분석과 같이 single sentence에서 각 토큰이 어떤 class를 갖는지 모두 classifier 적용하여 정답을 찾아냅니다.
4.1 GLUE Datasets
해당 섹션은 The General Language Understanding Evaluation(GLUE) benchmark (Wang et al., 2018)에 대한 설명이 이어져있습니다. 본 포스트에서는 생략하고 넘어가도록 하겠습니다.
4.1.1 GLUE Results


그림5. GLUE results (출처: BERT 논문)

쉽게 말해, 모든 task에 대해 SOTA를 달성합니다.
특히 인상적인 것은 데이터 크기가 작아도 fine-tuning 때는 좋은 성능을 낼 수 있다는 것입니다.
그리고 BERT_large가 BERT_base에 비해 훨씬 좋은 성능을 냅니다.(dataset이 작은 task의 경우에도!) BERT의 모델 크기에 따른 성능은 Section 5.2에서 살펴보겠습니다.
4.2 SQuAD v1.1
기존 SQuAD dataset의 경우에는 GLUE dataset을 BERT에 fine-tuning할 때와 방식이 좀 다릅니다. GLUE dataset의 경우에는 task가 sequence classification이지만, SQuAD는 질문과 지문이 주어지고, 그 중 substring인 정답을 맞추는 task입니다.
이러한 차이를 BERT는 아주 간단한 방법론으로 극복합니다.
일단 질문을 A embedding, 지문을 B embedding 으로 처리하고, 지문에서 정답이 되는 substring의 처음과 끝을 찾는 task로 문제를 치환 합니다.
start vector S∈ℝH
S
∈
R
H
와 end vector E∈ℝH
E
∈
R
H
를 fine-tuning중에 학습히여, 지문의 각 token들과 dot product하여 substring을 찾아냅니다. (그림 4 (c) 참조)
이런 접근법으로 BERT_large의 경우 기존의 모든 시스템을 wide margin을 두고 최고성능을 달성 합니다.
4.3 Named Entity Recognition
token tagging task를 평가해보기 위해서 CoNLL 2003 Named Entity Task를 fine-tuning 해보았습니다.
해당 데이터셋은 200k개의 words로 이루어져 있고, 각각의 단어들은 Person, Organization, Location, Miscellaneous, Other로 태깅되어 있습니다.
각 토큰마다 classifier를 붙여서 위의 제시된 class에 대해 판별을 합니다.
각각의 prediction은 주위의 prediction에 영향을 받지 않습니다. (CRF나 autoregressive를 사용하지 않음!)
이 task또한 SOTA를 달성합니다.
4.4 SWAG
The Situations With Adversarial Generations(SWAG) 데이터셋은 113k sentence-pair로 이루어져 있으며, grounded common-sense inference를 측정하기 위해 사용합니다.
앞 문장이 주어지고, 보기로 주어지는 4 문장중에 가장 잘 이어지는 문장을 찾는 task 입니다.
A girl is going across a set of monkey bars. She (i) jumps up across the monkey bars. (ii) struggles onto the bars to grab her head. (iii) gets to the end and stands on a wooden plank. (iv) jumps up and does a back flip.

이 데이터셋을 BERT에 적용하는 것은 GLUE dataset 적용법과 비슷합니다. 4개의 가능한 input sequences를 구성합니다. 이는 given sentence(sentence A)와 possible continuation(sentence B)을 concat한 것들입니다.
해당 task specific한 벡터 V∈ℝH
V
∈
R
H
를 학습시키고, input sequences의 값들을 합산한 Ci∈ℝH
C
i
∈
R
H
를 dot product하여 softmax 합니다.
이 또한 사람을 능가하는 SOTA를 달성합니다.
5. Ablation Studies

해당 챕터에서는 이전 챕터에서 중요한 요소라고 설명했던 부분을을 하나씩 제거하며 요소들의 중요함을 파악해 보고있습니다.
개인적으로 논문에서 가장 중요한 챕터라고 생각합니다.
5.1 Effect of Pre-training Tasks
5.1에서는 이전 3.3 Pre-training Tasks에서 소개한 2가지 task를 하나씩 제거하면서 각각의 task의 효과를 알아봅니다.
BERT_base와 동일한 hyperparameter로 실험을 진행하지만 ablation한 두가지 다른 모델로 실험을 진행합니다.
No NSP: MLM은 사용하지만, 다음 문장 예측 (NSP)를 없앤 모델
LTR &amp; No NSP : MLM 대신 Left-to-Right (LTR) 을 사용하고, NSP도 없앤 모델, 이는 OpenAI GPT모델과 완전히 동일하지만, 더 많은 트레이닝 데이터를 사용하였습니다.

그림6. pre-training task ablation result (출처: BERT 논문)

표에서 볼 수 있듯, pre-training task를 하나라도 제거하면 성능이 굉장히 떨어지는 것을 볼 수 있습니다.
No NSP의 경우에는 NLI계열의 task에서 성능이 많이 하락하게 되는데, 이는 NSP task가 문장간의 논리적인 구조 파악에 중요한 역할을 하고 있음을 알 수 있습니다.
MLM대신 LTR을 쓰게 되면 성능하락은 더욱더 심해지게 됩니다. BiLSTM을 더 붙여도, MLM을 쓸 때보다 성능이 하락하는 것으로 보아, MLM task가 더 Deep Bidirectional한 것임을 알 수 있습니다.
5.2 Effect of Model Size

간단하게 말해서, 측정한 데이터셋에서는 모두 모델이 커질수록, 정확도가 상승함을 볼 수 있습니다.

그림7. Ablation over BERT model size (출처: BERT 논문)

수 년동안 알려져 왔듯, 번역 task나 language modeling과 같은 large-scale task는 모델 사이즈가 클 수록 성능은 계속 상승합니다.
특히 BERT의 경우에는, downstream task를 수행하는 dataset의 크기가 작아도, pre-training덕분에, model의 크기가 클 수록 정확도는 상승함을 볼 수 있습니다.
5.3 Effect of Number of Training Steps


그림8. Ablation over number of training steps (출처: BERT 논문)

그림 8에서는 MNLI Dev accuracy를 pre-training step에 따라 정리하였습니다. 세부사항은 문답형식으로 진행하겠습니다.
Question 1. fine-tuning 단계에서 높은 정확도를 얻으려면, pre-training 단계에서 많은 training step이 필요합니까?
A : 그렇습니다. 0.5M step에 비해 1M step때 accuracy가 거의 1.0% 상승함을 볼 수 있습니다.
Question 2. 3.3.1 Task #1: Masked LM의 마지막 부분에서 시사한 바와 같이, MLM으로 학습하면 15%의 단어만 맞추는 것으로 학습을 진행하기 때문에, LTR보다 수렴속도가 훨씬 느리지않습니까?
A : 수렴속도가 조금 느린 것은 사실이지만, LTR보다 훨씬 먼저 out-perform성능이 나오게 됩니다.
5.4 Feature-based Approach with BERT

지금까지 BERT는 pre-training을 진행 한 후, downstream task를 학습 할 때, 간단한 classifier를 부착해서 모든 layer를 다시 학습시키는 fine-tuning 방법을 사용하는 것만을 설명드렷습니다.
하지만 BERT를 ELMO와 같이 feature based approach로도 사용을 할 수 있습니다.
Feature-based Approach는 몇가지 이점이 존재합니다.
Transformer encoder는 모든 NLP task를 represent하지는 못하므로, 특정 NLP task를 수행할 수 있는 Network를 부착하여 쓸 수 있습니다.
Computational benefit을 얻을 수 있습니다.
해당 section에서는 BERT를 ELMO와 같이 마지막 레이어에 Bi-LSTM을 부착시켜, 해당 레이어만 학습 시키는 방법론을 사용해보았습니다.

그림9. Ablation using BERT with a feature-based approach (출처: BERT 논문)

그림 9에서 볼 수 있듯이, Concat Last Four Hiddem값을 사용하면, Finetune All과 단지 0.3 F1 score차이밖에 나지 않습니다.
이를 통해, BERT는 Feature-based Approach에서도 효과적이라고 할 수 있겠습니다.
6. Conclusion
읽으면서 계속해서 느꼈지만, 굉장히 놀라운 논문이라고 생각합니다. 특히 MLM과 NSP task를 생각해 낸것이 굉장히 놀라웠습니다. 이러한 간단한 intuition을 가지고 NLP의 전분야를 아우르는 SOTA를 달성해 낸 것에 대해서 놀랍고, 또 부럽다는 생각을 했습니다.
마치며…
논문을 읽은 것이 10월 말 경이었던 것 같은데, 조금씩 글로 정리하다 보니 벌써 2월 말이 되었습니다. 그 사이에 BERT를 토대로 다시 SOTA를 달성한 새로운 논문(Multi-Task Deep Neural Networks for Natural Language Understanding)을 MS에서 발표하였습니다. 해당 논문도 빠른 시일내에 review해 보도록 하겠습니다. 감사합니다.
NLP Language Representation transformer 고급
© 2019 Tmaxdata. All right reserved.
 </Text>
        </Document>
        <Document ID="EDBBFFF7-D55D-4894-BACA-EBA8D8E76836">
            <Title>Conclusions</Title>
        </Document>
        <Document ID="DD412833-B736-4C1B-A0DB-380ED3237EFD">
            <Title>sda</Title>
        </Document>
        <Document ID="E310FC17-2D4D-44CE-A230-875564BC8D32">
            <Title>Fine-tuning pre-trained model</Title>
            <Text>Training for fine-tuning the pre-trained model was carried out in standard 5-fold cross-validation. </Text>
        </Document>
        <Document ID="BAE7C2A7-FA27-47DA-B731-55CD47958273">
            <Title>Evaluation datasets</Title>
            <Text>For evaluating our model, the model was tested using two independent datasets: the dataset contained COVID-19 S-protein269-277(YLQPRTFLL)  with the 352 epitope-specific TCRβs from the recent study of Shomuradova et al. [34](from hereon referred to as Shomuradova dataset) and the dataset contained 415 COVID-19 S-protein269-277-specific TCRs from the ImmuneRACE study launched by Adaptive Biotechnologies and Microsoft (https://immunerace.adaptivebiotech.com, June 10, 2020 dataset, from hereon referred to as ImmuneCODE dataset)(Table S2).</Text>
        </Document>
        <Document ID="A566450C-00EA-428C-870B-DF5554D8C33C">
            <Title>Fine-tuning the pre-trained model</Title>
            <Text>Two fine-tuned models were generated from the pre-trained model. One was fine-tuned using  HLA-A*02:01-restricted epitope-specific CDR3b sequence data which which were used for training NetTCR deep learning models[{Jurtz:2018}]. </Text>
        </Document>
        <Document ID="F7BDF1C9-45ED-4DF3-A627-5C2F1DFC783A">
            <Title>NGS, Database, machine learning methods</Title>
            <Text>아주 최근의 공동의 데이터 수집의 노력[{Borrman:2017kl}, {Tickotsky:2017bo}{Mahajan:2018bj},{Bagaev:2019hf}]과 새롭게 떠오르는 high throughput TCR 시퀀싱 기술[{Klinger:2015ew},{Bentzen:2016hy}]의 발달로 인하여 기계학습 방법을 통한 epitope-specific TCR recognition의 모델링을 시작하기에 충분한 데이터셋이 확보되고 있는 상황이다[{Zvyagin:2019ds}]. 현재까지 PSSM[{Glanville:2017js}], 가우시안 프로세스[{Jokinen:2021bn}], random forest[{Gielis:2019dl}], CNN[{Jurtz:2018jt}], joint deep learning model[{Jokinen:2021bn}], 그리고 deep generative model[{Isacchini:2021jo}], NLP-based deep learning model{Springer:2020kp} 등을 사용한 기계학습(또는 딥러닝) 방법들이 제안되어 왔다.</Text>
        </Document>
        <Document ID="DE8931AE-4127-4DC0-88EA-461B5027C118">
            <Title>full_ms_model(EDGE)</Title>
        </Document>
        <Document ID="E565E835-E394-476B-BB3F-5795B5149310">
            <Title>First fine-tuning</Title>
        </Document>
        <Document ID="0886F03E-89E1-4BC8-929A-E22A5358E7CF">
            <Title>Materials and Methods</Title>
        </Document>
        <Document ID="4E7F7C0E-EFED-47E0-B573-F2239B3C4964">
            <Title>Model architecture</Title>
            <Text>Figure 2 shows our model architecture. Input amino acid sequences concatenated by epitope and CDR3beta sequences were first encoded into tokens using a tokenizer, where each token is a integer code for a single amino acid. Each token is then embedded into a 768 dimensional vector in the pre-trained TAPE model based on BERTbase which has 12 encoding layers with 12 self-attention heads in each layer.  The TAPE model was pre-trained using unlabeled 31 million protein sequences with two tasks: next-token prediction and bidirectional masked-token prediction, including further supervised features on contact prediction and remote homology detection. The output of the pre-trained TAPE model is the hidden states of the first token. The final classifier consisted 2-layer feed forward network is used to predict either binder or not.</Text>
        </Document>
        <Document ID="25E226DF-1E4D-42B8-8FE9-3AFFCB744D7D">
            <Title>BERT?</Title>
            <Text>Transformer의 encoder를 사용하여 self-attention 기반의 BERT(Bidirectional Encoder Representations from Transformers)[{Devlin:2018uk}]는 대용량 corpus의 semi-supervised 사전학습을 통해 contextual word representation(language model)을 구축하고 이를 specific한 task의 비교적 적은 양의 데이터에 대한 supervised fine-tuning을 통해 최종 예측 모델을 구축하는 방법으로 11개의 자연어 처리 task(SQuAD1.1 등)에서 SOTA를 달성하면서 최근에 크게 주목받고 있다. </Text>
        </Document>
        <Document ID="82400414-9ECB-454D-8F62-CF43D1F96576">
            <Title>Not all binding peptides are presented on the cell</Title>
            <Text>MHC와 결합하는 모든 펩타이드가 Tumor-specific 신항원 펩타이드는 아니다. [{Biotechnol:fe}]. MHC 분자와 결합하는 펩타이드 중에 오직 30% 미만이 tumor cell에서 process 되고 이중 일부만이 T-cell을 유도하고 clinically-relevant immunogenicity를 갖는 neopeptide 일 수 있다[{Vitiello:2017dm}, {Ott:2017ft}].

One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this binding to MHC is the most selective step in peptide Ag presentation. However, other factors, including Ag processing [{Tenzer:2005bt}] and the stability of the peptide–MHC complex [{Harndahl:2012jj}], could influence the likelihood of a given peptide to be presented as an MHC ligand. 
Similarly, the length distribution of peptides available for binding to MHC molecules is impacted by other steps in the processing and presentation pathway, such as TAP transport and ERAP trimming, which are not reflected in binding data in itself[{Trolle:2016jw}]. 
</Text>
            <Notes>Some of the most well-documented and applied methods for predicting peptide binding to MHC class I include NetMHC (4, 7) and NetMHCpan (1, 8). Over the last years, these tools have garnered increasing interest because of the recent focus on neoantigen identification within the field of personalized immunotherapy (9, 10). However, as underlined in several studies, including the recent Nature Biotechnology Editorial (11), “neoantigen discovery and validation remains a daunting problem,” primarily as a result of the relatively high false positive rate of predicted epitopes. 
One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this binding to MHC is the most selective step in peptide Ag presentation. However, other factors, including Ag processing (12) and the sta- bility of the peptide–MHC complex (13), could influence the likelihood of a given peptide to be presented as an MHC ligand. 
Similarly, the length distribution of peptides available for binding to MHC molecules is impacted by other steps in the processing and presentation pathway, such as TAP transport and ERAP trimming, which are not reflected in binding data in itself (6). </Notes>
        </Document>
        <Document ID="14C67731-9074-4B0E-9FAE-499B79D20F44">
            <Title>Encoding pMHC binding data into a sentence</Title>
            <Text>pMHC-I 결합데이터는 peptide 서열과 MHC-I 분자의 contact residue 서열을 concat하여 하나의 문장으로 encoding 될 수 있다. 여기서 concat 된 아미노산 서열에서 아미노산들은 word들로 treated 되어 질 수 있다(Figure-&gt;).
우리는 HLA-bound 사전훈련 데이터셋부터 20개의(or 22(Sec, Pyl)) 단어로 구성된 vocab을 갖는 하나의  sentence corpus를 구축할 수 있다.
</Text>
            <Notes>
By formulating protein data as standard sequence data like sentences in a text corpus, standard NLP algorithms can be readily applied. More concretely, individual peptides are treated as individual sentences and amino acids are treated as words. In this article, the skip-gram model is used with a context window of size 5, 5 negative samples, and 15-dimensional vector space embedding. Various other dimensional size were explored, however, 15-dimen- sions gave the best results on 10-fold cross-validation of HLA- A*02:01 subtype. The entire post-processed dataset by Luo et al. (2016) was used to learn this new distributed representation. The 15-dimensional vector space distributed representation, HLA-Vec, is summarized in Table 1 
</Notes>
        </Document>
        <Document ID="FFE94667-FA04-40A3-B535-5E2BAD1571F5">
            <Title>Protein Emdedding</Title>
        </Document>
        <Document ID="D745CF75-4F0B-478C-A4A7-730300A5E22C">
            <Title>BassaniSternberg2016</Title>
            <Text>- {BassaniSternberg:2016kt}: Bassani-Sternberg, M. et al. Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry. Nat Commun 7, 185–16 (2016).
- Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry
- Data: https://www.nature.com/articles/ncomms13404#Sec32</Text>
        </Document>
        <Document ID="EAC34DAC-B9D9-4622-A2C8-A702C718BF69">
            <Title>Loss curve in Binding Affinity Prediction</Title>
            <Text>#</Text>
        </Document>
        <Document ID="8B672601-17EB-4F96-9EF3-D558685F6AB5">
            <Title>Loss curve in Masked LM</Title>
            <Text>#</Text>
        </Document>
        <Document ID="3C7848E8-8DE4-4FDC-88D7-B655EAC58A0A">
            <Title>Pre-traing model</Title>
            <Text>pMHC-I 결합 언어모델은 were pre-trained using two tasks, such as masked language model, adding prior knowledge and binding affinity prediction, in semi-supervised manner as did in BERT model, 
Masking task에서의 loss는 cross entropy loss이고 binding affinity prediction task는 mean square loss가 사용되었고 total loss는 두 loss의 합이 사용되었다.</Text>
            <Notes>Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1. 
Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. 
In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. 
Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. 
</Notes>
        </Document>
        <Document ID="394FE7AE-392D-48B6-A54C-AA1FBE421B44">
            <Title>전이학습</Title>
        </Document>
        <Document ID="DA18EADC-F821-4854-98F1-6CBAA9EA69E9">
            <Title>Overall training process</Title>
            <Text>Figure 1 shows the schematic representation of overall training process of our model. The initial model was cloned from the pretrained TAPE model based on BERT, adding a classification layer at the end. First, the initial TAPE model was pre-trained using epitope-TCR CDR3b sequence data while freezing the embedding layer and top two encoding layers. 
Next, two fine-tuned models were generated from the pre-training model using IEDB epitope-specific TCR CDR3b sequence data and COVID-19 epitope-specific TCR CDR3b sequence data, respectively, while freezing the embedding layer and top six encoding layers.
</Text>
        </Document>
        <Document ID="FC045630-47F5-4117-BD12-2A682CEE275A">
            <Title>NIHMS905283-supplement-1</Title>
        </Document>
        <Document ID="4DF57EC0-4186-4549-9E41-A4CAAB17DE4A">
            <Title>sydney_comb_smm</Title>
            <Notes>1.	Sidney, J. et al. Quantitative peptide binding motifs for 19 human and mouse MHC class I molecules derived using positional scanning combinatorial peptide libraries. Immunome Res 4, 2–14 (2008).</Notes>
        </Document>
        <Document ID="1DAADC60-C206-4147-9C2A-282CE4D1BE98">
            <Title>Negative examples</Title>
            <Text>To increase the specificity of our model, it was necessary to add more epitope-specific  TCR CDR3beta sequence data to each fine-tuning dataset  as negative examples which were expected to interact between TCRs and epitopes. The background CDR3beta sequences were obtained from Howie at al., who have collected blood from two healthy donors. A negative example generated by combining the epitopes from the positive dataset and randomly selected TCR CDR3beta sequences derived from two healthy donors[{Howie:2015}].
Table 1 summarizes the epitope-specific CDR3beta sequences for each fine-tuning dataset(see Table S1 for the fine-tuning datasets in detail). 



To increase our predictive model, negative data examples which we do not expect to interact between TCRs and peptides were added. The negative examples were made by combining the peptides retrieved from the IEDB, selecting only peptides derived human proteins and TCR CDR3 sequences from 20 healthy donors.

Background TCRs. To estimate background frequencies of the different V and J   genes, generate background TCRs for use in assessing the significance of CDR3   motifs, and as negative samples for discrimination tests, we relied on the following   high-throughput repertoire profiling experiments: for the mouse α chain, short   read archive (SRA) projects SRP010815 (ref. 10) and SRP059581 (ref. 11); for   the mouse β chain, SRA projects SRP059581 (ref. 11), SRP015131 (ref. 12), and   SRP004475; for the human α and β chains, the study of Howie et al.13. For background frequency comparisons we took the minimum normalized JS
To increase the specificity of our model it was necessary to add more TCR sequences and peptides as negative examples to the training. The negative TCR sequences were obtained by repertoire sequencing of healthy individuals and paired with the peptides in the IEDB and further self peptides identified by ligand elution assays to be presented by HLA-A*02:01. Combining TCR sequences of healthy individuals with human self peptides should result in true negative examples in the vast majority of cases, but when combining those TCRs with the peptides present in the IEDB data, which are to a large extent well studied influenza and herpes virus epitopes (42) , they might result in some false negatives. 
For creating the negative examples for the  TPP-I task, we first chose a peptide randomly from the peptides  in the training set. Then, we chose five random TCRs from the  training set that are not reported to bind this peptide, to create  five internal wrong pairs 
For the training and testing of the models, we also required some background TCRs that we do not expect to recognize the epitopes in our data sets. For this purpose, we randomly sampled the required amount of TCRs from sets of background TCRs constructed by Dash et al. [10]. They report that the human α- and β-chains have been obtained from Howie et al. [52], who have collected blood from two healthy adults. The mouse α-chains they have gath- ered from short read archive (SRA) projects SRP010815 [53], and SRP059581 [54] and mouse β-chains from SRA projects SRP059581 [54], SRP015131 [55], and SRP004475. To create paired α- and β-chains they randomly paired the unpaired α- and β-chains from the reper- toires for the corresponding organism. 


</Text>
        </Document>
        <Document ID="4ECF2D4A-9F04-4E46-ACFA-A4C334545EBA">
            <Title>Prediction of Specific TCR-Peptide Binding From Large Dictionaries of TCR-Peptide Pairs</Title>
            <Text>    ORIGINAL RESEARCH
published: 25 August 2020 doi: 10.3389/fimmu.2020.01803
                                            Edited by:
Nadia Caccamo, University of Palermo, Italy
Reviewed by:
Vanessa Venturi, University of New South Wales, Australia Benny Chain, University College London, United Kingdom
*Correspondence:
Yoram Louzoun louzouy@math.biu.ac.il
Specialty section:
This article was submitted to T Cell Biology, a section of the journal Frontiers in Immunology
Received: 12 February 2020 Accepted: 06 July 2020 Published: 25 August 2020
Citation:
Springer I, Besser H, Tickotsky-Moskovitz N, Dvorkin S and Louzoun Y (2020) Prediction of Specific TCR-Peptide Binding From Large Dictionaries of TCR-Peptide Pairs. Front. Immunol. 11:1803. doi: 10.3389/fimmu.2020.01803
Prediction of Specific TCR-Peptide Binding From Large Dictionaries of TCR-Peptide Pairs
Ido Springer1, Hanan Besser1, Nili Tickotsky-Moskovitz2, Shirit Dvorkin1 and Yoram Louzoun 1*
1 Department of Mathematics, Bar Ilan University, Ramat Gan, Israel, 2 The Goodman Faculty of Life Sciences, Bar Ilan University, Ramat Gan, Israel
Current sequencing methods allow for detailed samples of T cell receptors (TCR) repertoires. To determine from a repertoire whether its host had been exposed to a target, computational tools that predict TCR-epitope binding are required. Currents tools are based on conserved motifs and are applied to peptides with many known binding TCRs. We employ new Natural Language Processing (NLP) based methods to predict whether any TCR and peptide bind. We combined large-scale TCR-peptide dictionaries with deep learning methods to produce ERGO (pEptide tcR matchinG predictiOn), a highly specific and generic TCR-peptide binding predictor. A set of standard tests are defined for the performance of peptide-TCR binding, including the detection of TCRs binding to a given peptide/antigen, choosing among a set of candidate peptides for a given TCR and determining whether any pair of TCR-peptide bind. ERGO reaches similar results to state of the art methods in these tests even when not trained specifically for each test. The software implementation and data sets are available at https://github.com/ louzounlab/ERGO. ERGO is also available through a webserver at: http://tcr.cs.biu.ac.il/.
Keywords: TCR repertoire analysis, epitope specificity, evaluation methods, machine learning, deep learning, long short-term memory (LSTM), autoencoder (AE)
INTRODUCTION
T lymphocytes (T cells) are pivotal in the cellular immune response (1, 2). The immense diversity of the T-cell receptor (TCR) enables specific antigen recognition (3, 4). Successful recognition of antigenic peptides bound to Major Histocompatibility Complexes (pMHCs) requires specific binding of the TCR to these complexes (5–7), which in turn modulates the cell’s fitness, clonal expansion, and acquisition of effector properties (7). The affinity of a TCR for a given peptide epitope and the specificity of the binding are governed by the heterodimeric αβ T-cell receptors (2). While both chains have been reported to be important to affect binding, we show here that for many TCR-peptide pairs the TCR’s binding to target MHC-peptide can be determined with high accuracy using the β-chain only. Including the alpha chain in the analysis is essential for better accuracy. However, as many experimental settings provide, only beta chains, a binding prediction tool based on these chains is of importance.
 Frontiers in Immunology | www.frontiersin.org 1 August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
 Within the TCRβ chain, the complementarity-determining region 1 (CDR1) and CDR2 loops of the TCR contact the MHC alpha-helices while the hypervariable complementary determining regions (CDR3) interact mainly with the peptide (1, 2). In both TCRα and TCRβ chains, CDR3 loops have the highest sequence diversity and are the principal determinants of receptor binding specificity.
Following specific binding of T cell receptors to viral and bacterial-derived peptides bound to MHC (5), or from neo- antigens (8–10), the appropriate T cells expand, resulting in the increased frequency of T cells carrying such receptors. Recently, high-throughput DNA sequencing has enabled large- scale characterization of TCR sequences, producing detailed T cell repertoire (Rep-Seq) (11). Expanded clones are more likely to be repeatedly sampled in Rep-Seq than non-expanded clones and can serve as biomarkers for previous or current exposures to their cognate target, so tools that precisely identify TCRs binding different targets are essential in utilizing T cell repertoires as systemic biomarkers (often referred to as “reading the repertoire”).
A direct method for using TCR Rep-Seq as biomarkers has been proposed by Emerson et al. (12) and similar approaches (13) who detected patients that have Cytomegalovirus (CMV) based on their full repertoire and the choice of TCRs that differ between CMV positive and negative patients (12, 14). This approach is based on the presence of highly specific and repetitively observed public TCR in the response of different hosts to the same peptide [often denoted public clones, although the definition of such clones varies among authors (15)]. Such an approach requires extensive repertoire sequencing for every condition tested.
In contrast, many TCR responses are characterized by a high level of cross-reactivity with single TCRs binding a large number of MHC-bound peptides, and single peptides binding a large number of TCRs (16, 17). TCRs binding the same MHC-peptide may share similarities. Thus, while for public clones the task of deciphering the relation between a peptide and the TCR binding is based on tallying the candidate public TCR, for most highly cross-reactive TCRs, a probabilistic approach is required.
Important steps have been made in this direction by Glanville et al. (4) and Dash et al. (18), who detected the clear signature of short amino acid motifs in the CDR3 region of TCRβ and TCRα in response to specific peptides presented by specific MHC molecules. This work was then extended by recent efforts that combined these motifs with machine learning to predict peptide-specific TCRs vs. naïve TCRs, using Gaussian Processes (19) or Random Forest (20), or predicting TCR-epitope binding with Convolutional Neural Networks (21, 22). These methods significantly outperform random classification in the distinction of TCR binding a specific peptide and random TCRs.
All the approaches above are sequence-based and do not model the structure of the interaction. Moreover, it is assumed that the TCR-peptide binding is a binary prediction, instead of explicitly computing the off-rate or on-rate. This is indeed a simplification, based on an arbitrary cutoff of the affinity. Also, all such predictions ignore cross-reactivity; no attempt is made to predict whether a given peptide is the only target of a TCR (or vice versa).
Still, this question is of importance in two experimental setups. The first case is the attempt to predict from deep sequencing whether a given host has been infected by a pathogen [e.g., CMV (12, 14)]. A similar question that is often raised is which receptors to test for a given target. The binary solution to this question helps to sort TCRs to test. For both scenarios, predicting whether a TCR binds a peptide is of importance. We here follow a similar approach and propose a clear framework for the validation of current approaches, and a novel method to predict the binding of any peptide to any TCR (instead of predicting binding to predefined peptide).
The next required step for using the repertoire to develop specific biomarkers would be to distinguish between TCR binding different peptides. An essential step in the development of high precision predictors is the standardization of the comparison methods.
In contrast with most machine learning tasks, where one attempts to predict the output for a given input (e.g., predicting the content of an image), TCR binding is a pairing problem, where one is given a pair of inputs X and Y (a peptide and a TCR), and the goal is to predict whether they would bind. As such, there are many ways to divide the train and the test, and as a result many possible tests. One could for example assume that either X or Y are fixed and already seen in the training phase, and the other is varied. An alternative division could be that all X and Y are already known during the training and the test is on whether a given pair of known X and Y would bind. Finally, one could imagine a more complex scenario where one would ask on X and Y both absent from the training set whether they bind.
This formally translates into five different tests, each with different outcomes, as the standard method to estimate such predictions (Figure 1A):
•
•
•
•
•
Single Peptide Binding—SPB. Testing whether an unknown TCR binds a predefined target, using (as training information) TCRs known to bind to this target (18–20). In other words, the target is fixed, and TCRs are divided into disjoint training and test sets. The outcome of such a prediction would be the Area Under Curve (AUC) for the binding of an unseen TCR to this target.
Multi-Peptide Selection—MPS. Given a set of predefined peptides, predict which of those will be bound by a new TCR. In such a case, one trains on a set of different target peptides, and TCRs are again divided into disjoint training and test sets. The outcome of that would be the accuracy of the choice as a function of the number of candidate peptides.
TCR-Peptide Pairing I—TPP-I. Given a large set of peptides and TCRs, test whether a randomly chosen TCR binds a randomly chosen peptide. In this task, all TCR and peptides both belong to training and test sets. However, TCR-peptide pairs are divided into disjoint training and test sets. TCR-Peptide Pairing II—TPP-II is similar to TPP-I, except that now, TCRs contained in the pairs belonging to the training set cannot belong to the test set.
TCR-Peptide Pairing III—TPP-III is again a similar test on pairs, but here neither TCR nor peptide can be in both training and test set.
 Frontiers in Immunology | www.frontiersin.org 2
August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
     FIGURE 1 | (A) Illustration of the tests we suggest for evaluating the model performance as explained—SPB, single peptide binding; MPS, multi-peptide selection; and TPP, TCR-peptide pairing. (B) LSTM based model architecture. (C) Autoencoder based model architecture. (D) ROC curve of autoencoder based model SPB performance on 3 human peptides from Dash et al. (18) dataset. (E–G) Comparison of amino acids of CDR3 beta sequences of TCRs binding Dash et al. (18) peptides vs. TCRs that do not bind these peptides, in McPAS (23) database (logos were created with Two-Sample-Logos); the height of symbols within the stack in each logo indicates the relative frequency of each amino acid at that position. Only amino acids whose distribution differs significantly between the two sets are shown, and only 13 length TCRs were compared.
 We propose these different tests as standard measures for the quality of TCR-peptide binding predictions. The TCR-peptide pairing (TPP) task is often addressed in Natural Language Processing (NLP) using recurrent neural networks (RNN) (24). Long short-term memory (LSTM) networks are common types of RNN (25). We employed LSTMs that produce an encoding of the varying TCR and peptide into constant length real- valued encodings and created ERGO (pEptide tcR matchinG predictiOn). In all the following results, ERGO is only trained for the TPP-I task and tested on all other tasks.
RESULTS
ERGO Outline
Target peptides and TCRs have different generation mechanisms [TCRs through VDJ recombination and junctional diversity (11), and peptides through antigen generation, trafficking, processing
and MHC binding (26)]. As such they have different sequence probability distributions. To capture these differences, ERGO uses different parallel encoders. At the broad level, we encode the CDR3 of each TCR and each peptide into numerical vectors. The encoded CDR3 and peptide are concatenated and used as an input to a feed-forward neural network (FFN), which is trained to output 1 if the TCR and peptide bind and 0 otherwise (Figures 1B,C). At this stage, the MHC and V genes were not included since they did not contribute significantly to prediction accuracy in the current formalism. We plan to further enlarge the formalism to include both.
For the peptides, we first use an initial random embedding and translated each amino acid (AA) into a 10-dimensional embedding vector. Changing the encoding dimension did not significantly change the obtained accuracy. To merge the encoding vectors of each position into a single vector representing the peptide, each vector was used as input to an
 Frontiers in Immunology | www.frontiersin.org 3
August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
 LSTM. We used the last output of the LSTM as the encoding of the whole sequence. The embedding matrix values, the weights of the LSTM, and the weights of the FFN were trained simultaneously. For the TCR encoding, we either used a similar approach or an autoencoder (AE) (see Methods and Figure 1C).
These models were trained on two large datasets of published TCR binding specific peptides (23, 27). McPAS-TCR (23) is a manually curated database of TCR sequences associated with various pathologies and antigens based on published literature, with more than 20,000 TCRβ sequences matching over 300 unique epitope peptides. These TCRs are associated with various pathologic conditions (including pathogen infections, cancer, and autoimmunity) and their respective antigens in humans and mice. VDJdb (27) is an open, comprehensive database of over 40,000 TCR sequences and over 200 cognate epitopes and the restricting MHC allotype acquired by manual processing of published studies. For each TCR-peptide pair, a record confidence score was computed. ERGO was trained on both CD4 and CD8 T cell receptors. However, the large majority of peptides and TCR in both McPAS and VDJdb are CD8 T/MHC-I TCR/peptide combinations (Supplementary Figure S1C). When testing ERGO on TPP-I-III, we used an equal distribution of CD4 and CD8 T cell receptors in the training and test sets.
ERGO Can Predict TCR Binding to Specific
Epitopes or Antigens (SPB Task)
ERGO was trained to solve the TPP-I problem (pairing TCR and peptide) on the two datasets above, and then tested on all five mentioned tests. To test the performance of ERGO on the SPB task (detecting whether a previously unseen TCR binds a known peptide), we analyzed the five most frequent peptides in each dataset and tested the possibility of detecting whether a randomly selected TCR binds the peptide. The AUC for the binary classifications ranged between 0.695 and 0.980 (Table 1). The results are not sensitive to the number of TCRs reported for the peptides, and all peptides with more than 50 reported TCRs had similar values (Supplementary Table S4).
Note that ERGO is never trained on any specific target. Instead, it learns a model for the entire set of peptides through the LSTM. As such, its performance on different peptides varies and
TABLE 1 | Comparison between the different versions of the ERGO classifier [AE (Autoencoder) vs. LSTM and McPAS (23) vs. VDJdb (27)] for the SPB task.
is a function of the fit of the trained model to this specific peptide. This is both a strength and a weakness of ERGO. It is a strength in that it applies to a wide range of peptides, but a weakness since for a specific peptide with a large number of known binding TCRs, it can perform worse than existing classifiers.
To compare ERGO to current approaches, we tested its performance on current tools that predict TCR-peptide binding. We first compared it to the work of Jokinen et al. (19) who compared TCRs found by Dash et al. (18) to bind three human epitopes and seven mice epitopes with TCRs from VDJdb database (27), which bind additional 22 epitopes. These peptide- TCR pairs were compared with naïve TCRs not expected to recognize the epitopes. Jokinen et al. evaluated the TCRGP model using leave-one-subject-out cross-validation (LOSO). The TCRGP model was trained with all subjects but one at a time and tested on the last. In the VDJdb data, the authors use 5-fold cross- validation instead of LOSO. Other evaluations were reported by using leave-one-out cross-validation of all unique TCRs (as defined by CDR3 sequence and V-gene). We compared ERGO when only the CDR3β sequence is utilized with the published TCRGP results for three specific human peptides from Dash et al. (18) dataset. ERGO outperforms TCRGP models on 2/3 peptides, although ERGO was not trained to solve the SPB task for these specific peptides, but rather the more generic TPP task. ERGO was also compared to another epitope-specific based model by Gielis et al. (20), that used random forest algorithm to train their TCRex model. Gielis et al. distinguished between epitope-specific TCRs from McPAS (23) and VDJdb (27) databases (after some filtering methods) and background TCRs that were taken from an external dataset. Unlike ERGO, the TCRex data usage also includes the V and J genes. Nevertheless, ERGO performance on the SPB task is competitive with TCRex results on various peptides, even though ERGO is trained without V and J genes on the TPP task (Table 2 and Figure 1D, Supplementary Table S5 and methods for details of the training and test procedure for these and all other tests).
We used two-sample-logos (28) to compare the CDR3 sequences of cognate TCRs for the three human peptides from Dash et al. (18) dataset with TCRs that do not bind these peptides in the McPAS database (23) (Figures 1E–G). Only 13 AA long TCRs were compared to avoid any alignment bias. While one can see that different peptides have different signatures, it is interesting to see that the signature is not equally positioned among peptides. For the GLCTLVAML peptide, a signature is divided equally along the TCR, with a strong bias for the initial “CSA” at the beginning of the CDR3 sequence and not “CAS.” The NLVPMVATV signature is distributed following the standard “CAS” to the end of the CDR3, while the GILGFVFTL binding peptides are characterized by a dominant RS at position 6-7. Note that a part of this difference can be the result of different V and J gene usage, which is not explicit in ERGO, but may be captured by the algorithm.
The single peptide binding task can be extended to the single antigen protein task, where we predict whether a TCR would bind any peptide from a protein. Instead of testing whether an unseen TCR can bind a specific peptide, we tested whether it can bind any peptide from a target protein. The performance on this
  Peptide
LPRRSGAAGA GILGFVFTL NLVPMVATV GLCTLVAML SSYRRPVGI
McPAS
AE LSTM
0.772 0.760 0.843 0.832 0.835 0.821 0.803 0.816 0.969 0.980
Peptide
KLGGALQAK GILGFVFTL NLVPMVATV AVFDRKSDAK RAKFKQLL
VDJdb
AE LSTM
0.695 0.731 0.820 0.817 0.665 0.686 0.676 0.695 0.828 0.825
    The five most frequent peptides in each database are shown. Other less frequent peptides SPB results are in the Supplementary Table S4. The values are the AUC over the test set of a previously unseen TCR for this peptide.
 Frontiers in Immunology | www.frontiersin.org 4
August 2020 | Volume 11 | Article 1803

Springer et al. ERGO—TCR-Peptide Binding Prediction
 TABLE 2 | Comparison between the different versions of the ERGO classifier [AE vs. LSTM and McPAS (23) vs. VDJdb (27)] and existing classifiers [TCRGP by Jokinen et al. (19), TCRex by Gielis et al. (20)] for the SPB task.
 Peptide
GLCTLVAML NLVPMVATV GILGFVFTL
McPAS
ERGO
VDJdb LSTM
McPAS+ VDJdb
ERGO best
0.816
0.835 0.843
TCRGP (β,3), LOSO
0.782 0.587 0.818
TCRGP (β,3), unique LOO
0.852
0.651 0.822
TCRex
0.82±0.02 0.72±0.01 0.81±0.01
    AE
0.803 0.835 0.843
LSTM
0.816 0.821 0.832
AE
0.764 0.665 0.820
0.770 0.686 0.817
AE
0.708 0.624 0.725
LSTM
0.686 0.632 0.712
  Bolded values are the best results. The peptides here are the human peptides in Dash et al. (18) dataset proposed by Jokinen et al. (19). The other VDJdb peptides tested by the same authors are in Supplementary Table S5. The values are the AUC over the test set of previously unseen TCR for this peptide.
 TABLE 3 | Comparison between the different versions of the ERGO classifier [AE vs. LSTM and McPAS (23) vs. VDJdb (27)] for the binding to a specific antigen.
on the three TPP tasks. The easiest task (TPP-I) is predicting unknown TCR-peptide parings (AE AUC value 0.86). A more complex task is the prediction of pairs containing a known peptide with an unknown TCR (TPP-II—AE AUC value 0.81). The hardest pairing task is to predict the binding of a previously unseen peptide and a previously TCR (TPP-III). This task has never been tested and reaches an AUC of 0.669 (Table 4 and Figure 2B). We now plan to enlarge ERGO to include the alpha chain and V and MHC to see if this score can be improved.
To test if this performance can be improved by enlarging the training set to better learn the generic properties, we trained ERGO on McPAS (23) and VDJdb (27) simultaneously. On the TPP-III task, the more complex LSTM encoder reached a higher AUC of 0.674, yet on the TPP-I and TPP-II tasks, the results are better when ERGO is trained separately on McPAS and VDJdb (Table 4). McPAS and VDJdb databases contain different TCR and peptides with different distributions (Supplementary Figure S1), therefore ERGO performance on the combined dataset is often lower, suggesting that further increasing the training set with a similarly distributed set would improve the accuracy.
To further test the effect of the training set size, we subsampled the training set and tested the TPP-I AUC score for different sample sizes. The AUC increased with sample size and did not seem to saturate at the current sample size (Figure 2C). Some peptides have many reported TCRs binding them, while some have a single reported binding TCR (Figure 2E). We tested whether a larger number of reported binding TCR improves accuracy (Figure 2F). Again, a higher number of bound TCRs induces higher prediction AUC, suggesting that larger datasets would further improve ERGO’s performance.
Prediction of TCR-Neoantigen Binding
In the future, ERGO may contribute to the development of TCR-based diagnostic tools. However, it can already be used for the detection of TCRs that bind specific tumor antigens. Given a neoantigen extracted from full genome sequencing of tumors (29, 30) and a target TCR, one could estimate the binding probability of the TCR to such a neoantigen. To test for that, we applied ERGO to neoantigen binding prediction; we used a positive dataset of cancer neoantigen peptides and their matching TCRs, published by Zhang et al. (31), and expanded it with TCR- matching neoantigens in the McPAS-TCR and VDJdb databases.
 Protein McPAS
AE LSTM
NP177 0.772 0.767 IE1
M1 0.843 0.832 M pp65 0.814 0.803 pp65 BMLF1 0.808 0.819 EBNA4 PB1 0.958 0.970 Gag
There are no previous results on this task.
Protein VDJdb
  AE LSTM
0.703 0.738 0.825 0.820 0.702 0.716 0.711 0.717 0.890 0.897
  task varies drastically between target peptides, with AUC ranging from 0.71 to 0.97 (Table 3). This difference is not directly related to the number of target TCRs in the training set, but may rather represent the contribution of other factors not incorporated here, such as the alpha chain or the MHC.
Determining the Target of a TCR
(MPS Task)
To use a TCR as a biomarker, one should be able to predict which specific peptide it binds. To test for that, we computed the accuracy (as measured by the sum of the diagonal in the confusion matrix) of predicting the proper target, with a different number of possible targets (Figure 2A). Again, ERGO was not trained for this task, but for the TPP-I task. The targets were the peptides with the highest number of binding TCR in the databases (Supplementary Table S6). The AE produces better accuracies than the LSTM and the prediction for the AE and VDJdb yields better accuracies than McPAS. An important result is that the accuracy is still at 0.5 even for 10 peptides, suggesting that high accuracy can be obtained even when choosing from a large number of peptides.
Distinguishing TCRs Binding Different
Targets (TPP Task)
A more important task from a diagnostic point of view would be to distinguish between TCRs binding different peptides for any set of either known or previously unseen TCRs and peptides. To test the specificity of the prediction, we evaluated ERGO’s AUC
 Frontiers in Immunology | www.frontiersin.org 5
August 2020 | Volume 11 | Article 1803

Springer et al. ERGO—TCR-Peptide Binding Prediction
     FIGURE 2 | (A) AE and LSTM models MPS accuracy per number of peptide classes in McPAS-TCR (23) and VDJdb (27) datasets. (B) ROC curve of TPP-I, II, and III AE model performance on McPAS dataset. (C) AUC for TPP-I as a function of the sub-sample size. (D) AUC of TPP-I per missing amino-acids index. (E) Number of TCRs per peptide distribution in McPAS-TCR and VDJdb datasets, logarithmic scale. (F) AUC of TPP-I per number of TCRs per peptide bins (bins are the union of all TCRs that match peptides with total number of TCRs in a specific range).
 TABLE 4 | AUC of TPP task with either known peptide and TCR (but unknown pairing TPP-I), known peptide unseen TCR (TPP-II), and unseen peptide and TCR (TPP-III). Evaluation AUC McPAS VDJdb McPAS+ VDJdb Tumor
     AE LSTM AE LSTM AE LSTM AE
LSTM
 TPP-I 0.860 0.859 0.840 0.842 0.776 0.761 TPP-II 0.810 0.798 0.792 0.764 0.770 0.745 TPP-III 0.601 0.562 0.669 0.522 0.636 0.674
0.805 0.813 0.805 0.813 0.570 0.646
 The results are the test AUC using either AE or LSTM on McPAS (23) and VDJdb (27) separately or on the joined dataset. The final column is the prediction of tumor antigens TCR binding. Again, the AE consistently outperforms the LSTM, except for the TPP-III task, where increasing the training set size and the complexity of the encoders improves the performance. Bolded values are the best results.
We tested again TPP-I, TPP-II, and TPP-III (Table 4), and got a high AUC for TPP-I and II (above 0.8), and 0.65 for the most complex TPP-III task. A caveat of this analysis may be that it was performed on a comparison of a dataset of TCRs binding neo- antigens and T cells from repertoires of healthy donors. Thus, this is not a direct measurement of the possibility of detecting neo-antigen specific TCRs within a donor.
Comparison of TPP With Literature
While TPP-III was never previously tested, TPP-II was recently tested by Jurtz et al. (21), who used a convolutional neural network (CNN) based model, NetTCR, for predicting binding- probabilities of TCR-HLA-A∗02:01 restricted peptide pairs. An IEDB dataset was used to train the model. The MIRA assay provided by Klinger et al. (32) was used for evaluating the model by testing the model performance on shared IEDB and MIRA peptides and new TCRs. Jurtz et al. used two models
in their experiments. One was trained with positive IEDB examples and only negative examples made from the IEDB dataset itself (no additional sources) while another model had also additional naïve negatives (33). We used the united IEDB and MIRA dataset provided by Jurtz et al. and created also negative examples from that dataset. We trained ERGO models with 80% of the united data (positive and negative examples) and evaluated the model performance on the rest of the data (20%). Again, ERGO outperformed the current results, 0.88 vs. 0.73 (Supplementary Table S3). Note that some differences exist between the training and test set used here and in the Jurtz paper, as detailed in the methods section.
CDR3 Sequence Characteristics
To test which position along the CDR3 has the strongest effect on the binding prediction, we trained ERGO ignoring one TCR amino-acid position at a time, by nullifying the position in the
 Frontiers in Immunology | www.frontiersin.org 6
August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
 autoencoder based model or by skipping that position input in the LSTM based model (Figure 2D). Omitting each one of the central amino-acids of the TCR’s CDR3 beta (positions 7–15) impairs the model’s performance, especially in the LSTM-based model. The autoencoder-based model is more stable than the LSTM based model, perhaps due to exposure to a variety of TCRs in the TCR autoencoder pre-training.
and corresponding TCRs for adoptive cell transfer. Finally, an important future implication would be to predict TCR- MHC binding, such prediction can be crucial for improving mismatched bone marrow transplants (45).
MATERIALS AND METHODS
Data
Three TCR-peptide datasets were used in the binding prediction task. McPAS-TCR dataset was downloaded from http:// friedmanlab.weizmann.ac.il/McPAS- TCR/ and VDJdb dataset was downloaded from https://vdjdb.cdr3.net/, both in November 2019. We used a dataset of cancer neoantigen peptides and their matching TCRs, published by Zhang et al. (31). A set of cancerous peptides was made for extracting TCRs matching to these peptides also in McPAS-TCR and VDJdb databases. We extended the original cancer dataset to include all TCRs- cancerous peptide pairs in all datasets. The data were processed into TCR-peptide pair files, using only TCRβ chains and valid TCR and peptide sequences.
The TCR autoencoder was trained on a data which was derived from a prospective clinical study (NCT00809276) by Kanakry et al. (46) The dataset is freely available at the Adaptive database (www.adaptivebiotech.com) that provides open access to a variety of datasets of TCRs next-generation sequencing.
Datasets Studied
In each model, training data was loaded as batches of positive and negative examples. For the positive examples, we took the existing TCR-peptide pairs in the database and split it into a train set and a test set. For creating the negative examples for the TPP-I task, we first chose a peptide randomly from the peptides in the training set. Then, we chose five random TCRs from the training set that are not reported to bind this peptide, to create five internal wrong pairs. A similar process was done to create a test set containing positive and negative examples. Thus, the number of negative examples is five times larger than the number of positive examples in both train and test sets.
Models
We used two models for predicting TCR-peptide binding. The models use deep-learning architectures to encode the TCR and the peptide. Then the encodings are fed into a multilayer perceptron (MLP) to predict the binding probability. Two encoding methods are applied—LSTM acceptor encoding and Autoencoder-based encoding. The peptide is always encoded using the LSTM acceptor method, so the two models differ in the TCR encoding method.
LSTM Acceptor
First, the amino acids were embedded using an embedding matrix. We set each amino acid an embedding vector, randomly initialized. Next, the TCR or the peptide was fed into a Long Short Term Memory (LSTM) network as a sequence of vectors. The LSTM network outputs a vector for every prefix of the sequence; we used the last output as the encoding of the whole sequence. We used two different embedding matrices and
DISCUSSION
We propose a set of standard tests to evaluate the accuracy of TCR-peptide binding and show that training a model using a combination of deep learning methods and curated datasets on the complex task of pairing random peptides and TCR can lead to high accuracy on all other tests. The main element affecting prediction accuracy is the training size. Enlarging the database improves the prediction accuracy for unseen peptides. Also, when subsampling the existing datasets, the accuracy increases with sample size and does not seem to saturate at the current sample size (Figure 2C).
Several other elements can affect the results, such as the V and J gene used and the alpha chain. In general, TCR-sequencing has often been limited to the TCR β chain due to its greater combinatorial and junctional diversity (10) and to the fact that a single TCRβ chain can be paired with multiple TCRα chains (34). Pogorelyy et al. (35) have shown concordance between TCRα and TCRβ chain frequencies specific for a given epitope and suggested this justifies the exclusive use of TCRβ sequences in analyzing the antigen-specific landscape of heterodimeric TCRs. Only recently, with single-cell techniques that enable pairing of α and β chains sequences, more data on alpha-beta TCRs is accumulating (36). Once large-scale curated alpha-beta TCR- peptide datasets are available, their integration into the current method is straight forward.
ERGO is based on LSTM networks to encode sequential data. Previous models by Jurtz et al. (21) used convolutional neural networks (CNN) for a similar task. While CNNs are good at extracting position-invariant features, RNN (in particular LSTM) can catch a global representation of a sequence, in various NLP tasks (37). Similarly, we did not use attention-based models (38) since the TCR can bind the peptide MHC at different angles and specific TCR positions are not well-correlated with specific peptide positions (39).
ERGO randomly initializes our amino-acid embeddings and trains the embeddings with the model parameters. Using word- embedding algorithms such as Word2Vec (40) or GloVe (41) can give a good starting point to the embeddings. Special options for amino-acids pre-trained embeddings include the use of BLOSUM matrix (42) or Kidera-factors-based manipulations (43). As pre-trained embedding usually provides better model results, we plan to further test such encodings.
The prediction method presented here can serve as a first step in identifying neoantigen-reactive T cells for adoptive cell transfer (ACT) of tumor-infiltrating lymphocytes (TILs) targeting neoantigens (44). The ERGO algorithm can accelerate the preliminary selection of valid target epitopes
 Frontiers in Immunology | www.frontiersin.org 7
August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
 LSTM parameters for the TCRs and the peptides encodings. The embedding dimension of the amino acids was 10. We use two- layered stacked LSTM, with 500 units at each layer. A dropout rate of 0.1 was set between the layers.
TCR Autoencoder
The TCR autoencoder was trained before training the Autoencoder-based attachment prediction model. To train the TCR autoencoder, first we added a “stop-codon” at the end of every TCR CDR3 sequence. Each amino acid was represented as a one-hot vector of 21 numbers (20 possible amino acids and an additional stop codon) where all values were set to zeros except one index of the corresponding amino acid which was set to 1. Each of the CDR3 vector representations one-hot vectors (i.e., 20 positions for each amino acid with zeros, except for the position appropriate for this amino acid) were joined and, terminated with a “stop codon” one-hot vector. Zero padding was then added to the CDR3 vectors, completing the vectors to the maximum lengths chosen according to the data lengths distribution. Each zero codon was represented as a fully zeroed one-hot vector.
The concatenated TCR vectors were fed into the Autoencoder network, which was based on a combination of linear layers, creating similar “encoder” and “decoder” networks. In the encoder, the TCRs were first put into a layer of 300 units, then into a layer of 100 units, and then into the encoding layer of 100 units. This layer output was used to encode the TCR in the trained autoencoder model. We used Exponential Linear Unit (ELU) activation between the linear layers and dropout rate of 0.1. The decoding layers were similar to the encoding layers in the reverse order—first, the encoded TCR vectors were fed into a layer with 100 units, then into a layer with 300 units, and then into a layer with the original TCR concatenated one-hot vector length units. We used softmax (a function translating the last layer into a probability function) on the last decoder layer output on every sub-vector matching to an input amino acid one-hot vector position.
We used Mean Squared Error (MSE) loss (when the decoder output should be like the concatenated one-hot input). The autoencoder was trained using Adam optimizer with a learning rate of 1e-4, we used batching with batch size 50. The autoencoder was trained for 300 epochs.
In order to read the TCR from the decoding vector, we first split the long vector into “one-hot” like vectors. We back- translated the one-hot vectors into amino-acids by taking the amino acid matching to the maximal value index in the vector (which should be 1). We dropped all amino acids from the stop codon and forward to get a sequence of amino acid which should be the TCR. The autoencoder was trained with 80% of the data and was evaluated with the rest of it. The autoencoder was only trained on the TCRs and no information on the peptides was ever used to train the autoencoder.
MLP Classifier (Also Mentioned as FFN)
In both models, the TCR encoding was concatenated to the peptide encoding and fed into the MLP. The MLP contains one hidden layer with as units as half of the concatenated vector
size and sigmoid is used on the output of the last layer to get a probability value. In both models, the activation in the MLP is Leaky ReLU. Dropout with a rate of 0.1 was set between layers.
Model Configurations
As mentioned, we used two models, the LSTM based model and the autoencoder based model. We trained the embeddings, the LSTM parameters and the MLP in the first model, and the TCR autoencoder, peptide LSTM encoder and MLP parameters in the second model. The trained TCR autoencoder parameters were loaded to the autoencoder based model and are trained again within all model parameters.
We used Binary Cross Entropy (BCE) loss. Since we get five times more negative samples than positive samples according to the described sampling method, the loss is weighted, respectively, by a factor of 5/6 for positive samples and by 1/6 for negative samples. The optimizer was Adam with a learning rate of 1e-3 and weight decay 1e-5. We used batching with batch size 50. The model was trained for 100 epochs. The models used 80% of the data for training and 20% for evaluation for all datasets.
All models were implemented with PyTorch library in Python programming language.
The prediction models were evaluated using Area Under the Curve (AUC) score.
Hyperparameters Tuning
Both LSTM based model and the Autoencoder based model hyperparameters were optimized using a grid search in the hyperparameters space. The hyperparameters to optimize were the embedding matrix dimension, the LSTM dimensions, learning rate, weight decay, activation functions, etc. All models were tested with the same grid search. Once the grid search was finished, we chose five new sets of training and test set and reported their results. Note that the results are quite robust to most parameter changes, and that the size of the TCR-peptide pair space is much larger than any of the training sets used during parameter tuning.
Experiments Configuration
At the broad level, the ERGO model was trained and designed to solve the TPP-I task. Since the train and the test set are chosen randomly for each training process (as described above), 5 trained models along with their matching train and test set were analyzed, for each database [McPAS (23) or VDJdb (27)] and model type (LSTM based or AE based). Train and test sizes are detailed in Supplementary Table S2.
Single Peptide Binding
For computing single-peptide binding score, samples from each test set were observed. For every peptide, we looked for the pairs in the test set containing that peptide (positive and negative samples). The ROC and AUC scores were computed according to the model prediction of those pairs. SPB scores were computed for the five most frequent peptides in each database (Table 1) and three human peptides appearing in Dash et al. (18) dataset (Table2). Results for peptides with more than 50 reported binding TCRs are in the Supplementary Table S4. Mean AUC
 Frontiers in Immunology | www.frontiersin.org 8
August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
 scores are reported. Test sizes for the SPB task are detailed in Supplementary Table S7.
Single protein scores are extracted similarly, by analyzing all pairs in the test set that contain a peptide of the specific protein.
Multi-Peptide Selection
At first, the number of classes k was set. Trained model prediction scores were extracted for each TCR in the test set, paired with every peptide from the top k frequent peptides in the relevant database. The TCR target was predicted to be the peptide which got the maximal score as the pair complement. Accuracy was computed using the true samples in the test set. This was done for class numbers ranging between 2 and 10, as well as 20 and 30. Mean accuracies are shown in Figure 2A. Test sizes for the MPS task are detailed in Supplementary Table S8.
TCR-Peptide Pairing
New test TCRs and new test peptides were deducted from the train and test sets. TPP-I score is the AUC of the model predictions of the original test set. TPP-II is the AUC of the predictions of the new test TCR positive and negative samples. TPP-III is the AUC of the predictions of new test TCR and new test peptide pairs, positive and negative samples.
Train Data Sub-sampling
All models were trained and evaluated using the same train and test partition. Every model train set was a sub-sample of the original train set, while the test set remained the same. Ten thousand new train samples were added at each iteration.
Missing Positions Training
Again, all models were evaluated with the same test set and a train/test partition. In this experiment, the train data was modified by dropping a single amino acid in a specific position at a time. Practically, this was done by deleting this position for all TCRs in the LSTM based model, or by nullifying the relevant position in the one-hot encoding of the TCRs in the AE based model. This experiment was repeated five times, Mean TPP-I scores are shown in Figure 2D.
TPP Per-Number of TCRs Per-Peptide
First, TCR records per peptide were counted in the original McPAS (23) and VDJdb (27) databases. Given a test set, the test pairs were divided into bins, according to the number of
REFERENCES
1. Davis MM, Bjorkman PJ. T-cell antigen receptor genes and T-cell recognition. Nature. (1988) 334:395–402. doi: 10.1038/334395a0
2. Krogsgaard M, Davis MM. How T cells ‘see’ antigen. Nat Immunol. (2005) 6:239–45. doi: 10.1038/ni1173
3. Rowen L, Koop BF, Hood L, Even J, Kanellopoulos J, Kourilsky P. The complete 685-kilobase DNA sequence of the human beta T cell receptor locus. Science. (1999) 272:1755–62. doi: 10.1126/science.272.52 69.1755
4. Glanville J, Huang H, Nau A, Hatton O, Wagar LE, Rubelt F, et al. Identifying specificity groups in the T cell receptor repertoire. Nature. (2017) 547:94– 8. doi: 10.1038/nature22976
TCR records per peptide in the original database. The differences between the bins were on an exponential scale. AUC score was computed for each bin. Mean AUC scores are shown in Figure 2F.
Comparison With NetTCR
The united IEDB and MIRA datasets were downloaded from https://github.com/mnielLab/netTCR. Unfortunately, the authors did not publish the IEDB train data separated from the MIRA test data, thus we had to evaluate ERGO in another train/test partition. We used 80% of the IEDB and MIRA data for training and the rest of it (20%) for testing. Additional “C” prefix and “F” suffix were added to each TCR sequence. The MIRA data was containing new test TCRs (but was not evaluated with new test peptides), therefore we compare NetTCR results with ERGO TPP-II scores (Supplementary Table S3).
DATA AVAILABILITY STATEMENT
Publicly available datasets were analyzed in this study. This data can be found here: http://friedmanlab.weizmann.ac.il/McPAS- TCR/ and https://vdjdb.cdr3.net/.
AUTHOR CONTRIBUTIONS
IS developed the formalism and implemented it. SD designed the TCR autoencoder formalism. NT-M developed the libraries and wrote the manuscript. YL supervised the work and wrote the manuscript. HB designed the initial formalism. All authors contributed to the article and approved the submitted version.
ACKNOWLEDGMENTS
We wish to thank Prof. Luning Prak for helpful critiques and suggestions. This manuscript has been released as a pre-print (47).
SUPPLEMENTARY MATERIAL
The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fimmu. 2020.01803/full#supplementary-material
5. 6. 7. 8.
Rudolph MG, Stanfield RL, Wilson IA. How TCRS bind MHCS, peptides, and coreceptors. Annu Rev Immunol. (2006) 24:419–66. doi: 10.1146/annurev.immunol.23.021704.115658
Rossjohn J, Gras S, Miles JJ, Turner SJ, Godfrey DI, McCluskey J. T cell antigen receptor recognition of antigen-presenting molecules. Annu Rev Immunol. (2015) 33:169–200. doi: 10.1146/annurev-immunol-032414-112334
Zhang S-Q, Parker P, Ma K-Y, He C, Shi Q, Cui Z, et al. Direct measurement of T cell receptor affinity and sequence from naïve antiviral T cells. Sci Transl Med. (2016) 8:341ra77. doi: 10.1126/scitranslmed.aaf1278
Cohen CJ, Gartner JJ, Horovitz-Fried M, Shamalov K, Trebska-McGowan K, Bliskovsky VV, et al. Isolation of neoantigen-specific T cells from tumor and peripheral lymphocytes. J Clin Invest. (2015) 125:3981– 991. doi: 10.1172/JCI82416
 Frontiers in Immunology | www.frontiersin.org 9
August 2020 | Volume 11 | Article 1803

Springer et al.
ERGO—TCR-Peptide Binding Prediction
 9. Page DB, Yuan J, Redmond D, Wen YH, Durack JC, Emerson R, et al. Deep sequencing of T-cell receptor DNA as a biomarker of clonally expanded TILs in breast cancer after immunotherapy. Cancer Immunol Res. (2016) 4:835–44. doi: 10.1158/2326-6066.CIR-16-0013
10. Schrama D, Ritter C, Becker JC. T cell receptor repertoire usage in cancer as a surrogate marker for immune responses. Semin Immunopathol. (2017) 39:255–68. doi: 10.1007/s00281-016-0614-9
11. Benichou J, Ben-Hamo R, Louzoun Y, Efroni S. Rep-Seq: uncovering the immunological repertoire through next-generation sequencing. Immunology. (2012) 135:183–91. doi: 10.1111/j.1365-2567.2011.03527.x
12. Emerson RO, DeWitt WS, Vignali M, Gravley J, Hu JK, Osborne EJ, et al. Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T cell repertoire. Nat Genet. (2017) 49:659– 65. doi: 10.1038/ng.3822
13. Pogorelyy MV, Minervina AA, Chudakov DM, Mamedov IZ, Lebedev YB, Mora T, et al. Method for identification of condition-associated public antigen receptor sequences. Elife. (2018) 7:e33050. doi: 10.7554/eLife.33050
14. deWitt WS, Smith A, Schoch G, Hansen JA, Matsen FA, Bradley P. Human T cell receptor occurrence patterns encode immune history, genetic background, and receptor specificity. Elife. (2018) 7:e38358. doi: 10.7554/eLife.38358
15. Madi A, Shifrut E, Reich-Zeliger S, Gal H, Best K, Ndifon W, et al. T-cell receptor repertoires share a restricted set of public and abundant CDR3 sequences that are associated with self-related immunity. Genome Res. (2014) 24:1603–12. doi: 10.1101/gr.170753.113
16. Wooldridge L, Ekeruche-Makinde J, van den Berg HA, Skowera A, Miles JJ, Tan MP, et al. A single autoimmune T cell receptor recognizes more than a million different peptides. J Biol Chem. (2012) 287:1168– 77. doi: 10.1074/jbc.M111.289488
17. Sewell AK. Why must T cells be cross-reactive? Nat Rev Immunol. (2012) 12:669–77. doi: 10.1038/nri3279
18. Dash P, Fiore-Gartland AJ, Hertz T, Wang GC, Sharma S, Souquette A, et al. Quantifiable predictive features define epitope-specific T cell receptor repertoires. Nature. (2017) 547:89–93. doi: 10.1038/nature22383
19. Jokinen E, Heinonen M, Huuhtanen J, Mustjoki S, Lähdesmäki H. TCRGP: determining epitope specificity of T cell receptors. bioRxiv [Preprint]. 542332 (2019). doi: 10.1101/542332
20. Gielis S, Moris P, Bittremieux W, de Neuter N, Ogunjimi B, Laukens K, et al. Detection of enriched T cell epitope specificity in full T cell receptor sequence repertoires. Front Immunol. (2019) 10:2820. doi: 10.3389/fimmu.2019. 02820
21. Jurtz VI, Jessen LE, Bentzen AK, Jespersen MC, Mahajan S, Vita R, et al. NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks. bioRxiv [Preprint]. (2018) 433706. doi: 10.1101/433706
22. Moris P, de Pauw J, Postovskaya A, Ogunjimi B, Laukens K, Meysman P. Treating biomolecular interaction as an image classification problem – a case study on T-cell receptor-epitope recognition prediction. bioRxiv [Preprint]. (2019) doi: 10.1101/2019.12.18.880146
23. Tickotsky N, Sagiv T, Prilusky J, Shifrut E, Friedman N. McPAS-TCR: a manually curated catalogue of pathology-associated T cell receptor sequences. Bioinformatics. (2017) 33:2924–9. doi: 10.1093/bioinformatics/btx286
24. Elman J. Finding structure in time. Cognitive Sci. (1990) 14:179– 211. doi: 10.1207/s15516709cog1402_1
25. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. (1997) 9:1735–80. doi: 10.1162/neco.1997.9.8.1735
26. Louzoun Y, Vider T, Weigert M. T-cell epitope repertoire as predicted from human and viral genomes. Mol Immunol. (2006) 43:559–69. doi: 10.1016/j.molimm.2005.04.017
27. Shugay M, Bagaev DV, Zvyagin IV, Vroomans RM, Crawford JC, Dolton G, et al. VDJdb: a curated database of T-cell receptor sequences with known antigen specificity. Nucleic Acids Res. (2018) 46:D419– 27. doi: 10.1093/nar/gkx760
28. Vacic V, Iakoucheva LM, Radivojac P. Two sample logo: a graphical representation of the differences between two sets of sequence alignments. Bioinformatics. (2006) 22:1536–7. doi: 10.1093/bioinformatics/btl151
29. Jia J, Cui J, Liu X, Han J, Yang S, Wei Y, et al. Genome- scale search of tumor-specific antigens by collective analysis of mutations, expressions and T-cell recognition. Mol Immunol. (2009) 46:1824–9. doi: 10.1016/j.molimm.2009.01.019
30. Laumont CM, Vincent K, Hesnard L, Audemard É, Bonneil É, Laverdure J-P, et al. Noncoding regions are the main source of targetable tumor-specific antigens. Sci Transl Med. (2018) 10:eaau5516. doi: 10.1126/scitranslmed.aau5516
31. Zhang S-Q, Ma K-Y, Schonnesen AA, Zhang M, He C, Sun E, et al. High- throughput determination of the antigen specificities of T cell receptors in single cells. Nat Biotechnol. (2018) 36:1156–9. doi: 10.1038/nbt.4282
32. Klinger M, Pepin F, Wilkins J, Asbury T, Wittkop T, Zheng J, et al. Multiplex identification of antigen-specific T cell receptors using a combination of immune assays and immune receptor sequencing. PLoS ONE. (2015) 10:e0141561. doi: 10.1371/journal.pone.0141561
33. Savola P, Kelkka T, Rajala HL, Kuuliala A, Kuuliala K, Eldfors S, et al. Somatic mutations in clonally expanded cytotoxic T lymphocytes in patients with newly diagnosed rheumatoid arthritis. Nat Commun. (2017) 8:15869. doi: 10.1038/ncomms15869
34. Birnbaum ME, Dong S, Garcia KC. Diversity-oriented approaches for interrogating T-cell receptor repertoire, ligand recognition, and function. Immunol Rev. (2012) 250:82–101. doi: 10.1111/imr.12006
35. Pogorelyy MV, Fedorova AD, McLaren JE, Ladell K, Bagaev DV, Eliseev AV, et al. Exploring the pre-immune landscape of antigen-specific T cells. Genome Med. (2018) 10:68. doi: 10.1186/s13073-018-0577-7
36. de Simone M, Rossetti G, Pagani M. Single cell T cell receptor sequencing: techniques and future challenges. Front Immunol. (2018) 9:1638. doi: 10.3389/fimmu.2018.01638
37. Yin W, Kann K, Yu M, Schütze H. Comparative study of CNN and RNN for natural language processing. arXiv Prepr. arXiv1702.01923 (2017).
38. HuY,WangZ,HuH,WanF,ChenL,XiongY,etal.ACME: pan-specific peptide–MHC class I binding prediction through attention-based deep neural networks. Bioinformatics. (2019) 35:4946–54. doi: 10.1093/bioinformatics/btz427
39. Zoete V, Irving M, Ferber M, Cuendet MA, Michielin O. Structure- based, rational design of T cell receptors. Front Immunol. (2013) 4:268. doi: 10.3389/fimmu.2013.00268
40. Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv Prepr. arXiv1301.3781. (2013).
41. Pennington J, Socher R, Manning CD. GloVe: global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). (2014) p. 1532– 43. doi: 10.3115/v1/D14-1162
42. Henikoff S, Henikoff JG. Amino acid substitution matrices from protein blocks. Proc Natl Acad Sci USA. (1992) 89:10915– 9. doi: 10.1073/pnas.89.22.10915
43. Kidera A, Konishi Y, Oka M, Ooi T, Scheraga HA. Statistical analysis of the physical properties of the 20 naturally occurring amino acids. J Protein Chem. (1985) 4:23–55. doi: 10.1007/BF01025492
44. Hammerl D, Rieder D, Martens JWM, Trajanoski Z, Debets R. Adoptive T cell therapy: new avenues leading to safe targets and powerful allies. Trends Immunol. (2018) 39:921–36. doi: 10.1016/j.it.2018.09.004
45. Kollman C, Spellman SR, Zhang M-J, Hassebroek A, Anasetti C, Antin JH, et al. The effect of donor characteristics on survival after unrelated donor transplantation for hematologic malignancy. Blood. (2016) 127:260– 7. doi: 10.1182/blood-2015-08-663823
46. Kanakry CG, Coffey DG, Towlerton AMH, Vulic A, Storer BE, Chou J, et al. Origin and evolution of the T cell repertoire after posttransplantation cyclophosphamide. JCI Insight. (2016) 1:e86252. doi: 10.1172/jci.insight.86252
47. Springer I, Besser H, Tickotsky-Moskovitz N, Dvorkin S, Louzoun Y. Prediction of specific TCR-peptide binding from large dictionaries of TCR- peptide pairs. bioRxiv [Preprint]. (2019) 650861. doi: 10.1101/650861
Conflict of Interest: The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.
Copyright © 2020 Springer, Besser, Tickotsky-Moskovitz, Dvorkin and Louzoun. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.
 Frontiers in Immunology | www.frontiersin.org 10
August 2020 | Volume 11 | Article 1803
</Text>
        </Document>
        <Document ID="D6F68732-F39D-41FB-B6D0-5DE17F63F045">
            <Title>Pretrained protein BERT model</Title>
            <Text>우리는 사전 학습된 protein sequence embedding인 TAPE를 사용하였다. TAPE model은 BERT를 기반으로 하여 bidirectional masked-token prediction과 next token prediction task와 더불어 residue간의 contact prediction과 remote homology detection task를 사용하여 self-supervised하게 사전학습 되었다. 
TAPE 사전학습 데이터==&gt;
TAPE model architecture==&gt; The model has 12 layers with 12 self-  attention heads (Equation 1) in each layer, which enables the model  to learn long distance interactions. For an input amino acid sequence  z = (z1, z2, ..., zL), the output of the model are L continuous vectors of  dimension 768 corresponding to the input amino acids 

We used the pretrained TAPE model based on BERTbase which has 12 encoding layers with 12 self-attention heads in each layer. The TAPE model was trained using unlabeled 31 million protein sequences with two tasks: next-token prediction and bidirectional masked-token prediction.   </Text>
        </Document>
        <Document ID="D2170201-5D37-4BE8-98B7-D495653C3F95">
            <Title>Five-fold CV results</Title>
        </Document>
        <Document ID="2149A706-900B-401D-8B2A-35F27D7482D2">
            <Title>기존예측 방법의 한계</Title>
        </Document>
        <Document ID="33AE285F-A332-4989-A3FB-A8810C82BEEA">
            <Title>BulikSullivan2018</Title>
            <Text>{BulikSullivan:2018cb}: 1.	Bulik-Sullivan, B. et al. Deep learning using tumor HLA peptide mass spectrometry datasets improves neoantigen identification. Nature Biotechnology 37, 1–17 (2018).

#

- Data: N/A</Text>
        </Document>
        <Document ID="0A607A33-4230-4BAE-B938-85C6E2483EBB">
            <Title>Input Embedding</Title>
            <Text>pMHC-I 결합에서의 아미노산 서열(peptide sequence + MHC contact residue sequence)는 아미노산들에 해당하는 단어들로 구성된 하나의  ‘sentence’로 인코딩된다.  For a given ‘sentence’, the input embeddings of the BERT-based language model are the sum of the token embeddings and the position embeddings, where the position embeddings are used to inject some information about the relative or absolute position of the tokens in the sequence as used in original BERT model[{Devlin:2018uk}]. An input embedding matrix의 사이즈는 49 x emb_dim이다. 여기서 49는 문장의 최대길이, 즉 최대 peptide 길이(=15) + MHC contact residue의 서열길이(=56)[{Nielsen:2007ga} + {Luo:2016iw}]이고 emb_dim는 embedding dimension이다.</Text>
            <Notes>- Token: 20개 아미노산
- Sentence: 펩타이드 서열 + MHC 결합사이트의 residue들에 대한 pseudo-sequence
- Position Encoding that used in BERT model


Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one token sequence. Throughout this work, a “sentence” can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the in- put token sequence to BERT, which may be a single sentence or two sentences packed together. 
We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ∈ RH, and the final hidden vector for the ith input token asTi ∈RH. 
For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2. 
</Notes>
        </Document>
        <Document ID="CE130A60-033E-4A6B-8FE8-FD96F2C91CED">
            <Title>Masked LM for adding prior knowledge</Title>
            <Text>Masked LM task를 통하여 pMHC-I 결합에 대응하는 문장 내에서의 단어들(amino acids)간의 깊은 양방향의 contextual relationship을 선행 학습시킬 수 있다. 선행학습에서의 masking procedure은 입력 문장의 15%의 토큰들을 임의로 선택하여 80% 확률로 [MASK] 토큰으로 치환하거나(80% of the time: Replace the word with the [MASK] token), 10%의 확률로 임의 단어로 치환하거나(10% of the time: replace the word with a random word), 10%의 확률로 unchange(10% of the time: Keep the word unchanged). Then, a masked token will be used to predict the original token with cross entropy loss. 임의의 단어를 replace할 때, 펩타이드 서열에서의 위치별 아미노산 선호도와 MHC 결합사이트에서의 진화적 서열 보존 등과 같은 prior knowledge를 부여한다. 펩타이드 서열에서의 아미노산 치환은 PSSM(position-specific score matrix)[{Peters:2005tu}, {Kim:2009dd}]을 기반으로 하고, MHC 분자의 결합사이트에서의 아미노산 치환은 BLOSUM50을 사용한 진화적 perspective를 기반으로 한다[{Henikoff:1992jn}].
</Text>
            <Notes>Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1. 
Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. 
In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. 
Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. 

A A.1 
Additional details for our experiments are presented in Appendix B; and 
Additional ablation studies are presented in Appendix C. 
We present additional ablation studies for BERT including: 
– Effect of Number of Training Steps; and – Ablation for Different Masking Proce- 
dures. 
Additional Details for BERT Illustration of the Pre-training Tasks 
We provide examples of the pre-training tasks in the following. 
Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by 
	•	80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy → my dog is [MASK]  
	•	10% of the time: Replace the word with a randomword,e.g.,my dog is hairy → my dog is apple  
	•	10% of the time: Keep the word unchanged,e.g.,my dog is hairy → my dog is hairy. The purpose of this is to bias the representation towards the actual observed word. 
	•	 The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.  Compared to standard languge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. 

Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. 
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] 
Label = IsNext 
Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] 
Label = NotNext 
￼



</Notes>
        </Document>
        <Document ID="CBAEBBAD-D3D0-4A9D-95C9-5786D015408D">
            <Title>Discussion</Title>
        </Document>
        <Document ID="CCCD2E8B-0001-4373-A143-32874E76AC92">
            <Title>말하고자 하는 것들</Title>
            <Text>Here, we present a BERT-based model employing self-supervised transfer learning for predicting epitope-specific TCR recognition. The predictive model was generated by fine-tuning the pretrained TAPE model using epitope-specific TCR CDR3βsequence datasets. We report that the fine-tuned model can achieve better performance in predicting the TCR recognition of COVID-19 epitopes. 
We anticipate our works to provide new directions for constructing a reliable model for predicting the immunogenic T-cell epitopes with limited training data and to be valuable for accelerating effective vaccine developments.
</Text>
        </Document>
        <Document ID="B7EFF44C-3867-4AA1-B15B-9C6148485B03">
            <Title>datasets</Title>
        </Document>
        <Document ID="211FB603-98F0-43EF-8F6C-4C6550579C7E">
            <Title>Research idea</Title>
        </Document>
        <Document ID="3735F4CE-B61A-47F4-BB52-5CAAFF9E42DD">
            <Title>Epitope-specific TCR 식별의 어려움</Title>
            <Text>
TCRs are generated by genomic rearrangement of the germline TCR loci from a large collection of variable(V), diversity(D) and joining(J) gene segments. During T cell development, TCRs are formed by chains through the V(D)J recombination in each locus independently. It is estimated that this rearrangement can result in the range of 1018 different TCRs, which provides enormous diversity of epitope-specific T cell repertoires.

T cells undergo non-homologous recombination during T cell development, which involves rearrangement of the germline TCR loci from a large collection of variable (V), diversity (D) and joining (J) gene segments as well as template-independent insertions and deletions at the V-D and D-J junctions [3, 4]. 
TCRs are formed by a pair of α- and β-chains (90-95% of  T cells) or γ and δ-chains (5-10%) and V(D)J recombination happens in each locus independently. It is estimated that this rearrangement can result in the range of 1018 different TCR  genes [5, 6] which provides enormous diversity for epitope-specific T cell repertoires.
The complementarity determining regions (CDRs) of a TCR determine whether the TCR recognizes and binds to an antigen or not [7]. Of these regions, CDR3 is the most variable and primarily interacts with the peptide, while CDR1 and CDR2 primarily interact with the peptide binding groove of the MHC protein presenting the peptide but they can also be directly in con- tact with the peptide [8, 9]. Dash et al. [10] noted that also a loop between CDR2 and CDR3 (IMGT positions 81-86 [11]), which they called CDR2.5, has sometimes been observed to make contact with pMHC in solved structures. 
It is well known that the CDR3β of a TCR is important in recognizing peptides presented to the T cell, but it still remains unclear which specific physicochemical or structural features of the CDR3β or of other parts of the TCR determine the antigen recognition specificity of the T cell. High-throughput sequencing of V- and J-segment enriched DNA has enabled large-scale characterization of TCR sequences, initially only for the CDR3β with bulk methods [6, 12] but recently for the whole paired TCRαβ at single-cell resolution using plate or droplet-based methods [13, 14]. Nevertheless, profiling of epitope-specific TCRs remains exhaustive as they require sample-consuming experiments with distinct pMHC-multimers for each epitope of interest. Therefore, there is a great need for models that examine which epitopes a TCR can recognize or to which TCRs an epitope can bind to [15]. Curated databases of experimentally verified TCR-peptide interactions have recently been launched, such as VDJdb, IEDB, and McPAS [16–18]. Such data sources enable more comprehensive, data-driven analysis of TCR- peptide interactions, and allow the use of statistical machine learning techniques for the afore- mentioned tasks. Yet only a few computational methods for predicting recognition between TCRs and epitopes [10, 19–22] and for clustering similar TCRs [9, 23, 24] have been published. In addition to supervised and unsupervised methods for predicting TCR-epitope interactions, computational methods and web services such as [25] have also been proposed to predict the structure of TCRs based on their amino acid sequences. 

The affinity of a TCR for a given peptide epitope and the specificity of the binding are governed by the heterodimeric αβ T-cell receptors (2). While both chains have been reported to be important to affect binding, we show here that for many TCR-peptide pairs the TCR’s binding to target MHC-peptide can be determined with high accuracy using the β-chain only. Including the alpha chain in the analysis is essential for better accuracy. However, as many experimental settings provide, only beta chains, a binding prediction tool based on these chains is of importance. 
Within the TCRβ chain, the complementarity-determining region 1 (CDR1) and CDR2 loops of the TCR contact the MHC alpha-helices while the hypervariable complementary determining regions (CDR3) interact mainly with the peptide (1, 2). In both TCRα and TCRβ chains, CDR3 loops have the highest sequence diversity and are the principal determinants of receptor binding specificity. 

T cells are defined by a heterodimeric surface receptor, the T cell receptor (TCR), that mediates recognition of pathogen- associated epitopes through interactions with peptide and major histocompatibility complexes (pMHCs). TCRs are generated by genomic rearrangement of the germline TCR locus, a process termed V(D)J recombination, that has the potential to generate marked diversity of TCRs (estimated to range from 1015 (ref. 1) to as high as 1061 (ref. 2) possible receptors).



</Text>
        </Document>
        <Document ID="7137F6B7-8ECF-4B97-84B6-1D2930871F1F">
            <Title>Classical methods</Title>
            <Text>Motif-based
PSSM
Machine learning-based</Text>
        </Document>
        <Document ID="74B43ABE-8B53-4697-B5B1-84A069120F8A">
            <Title>Predicting recognition between T cell receptors and epitopes with TCRGP</Title>
            <Text>      a1111111111 a1111111111 a1111111111 a1111111111 a1111111111
OPEN ACCESS
Citation: Jokinen E, Huuhtanen J, Mustjoki S, Heinonen M, La ̈hdesma ̈ki H (2021) Predicting recognition between T cell receptors and epitopes with TCRGP. PLoS Comput Biol 17(3): e1008814. https://doi.org/10.1371/journal.pcbi.1008814
Editor: Yanay Ofran, Bar Ilan University, ISRAEL Received: April 8, 2020
Accepted: February 17, 2021
Published: March 25, 2021
Copyright: © 2021 Jokinen et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Data Availability Statement: We have only utilized previously published data. For convenience, all files are available at our Github-pages together with the software implementations (https://github.com/ emmijokinen/TCRGP, https://github.com/janihuuh/ tcrgp_manu_hcc). We have obtained the Dash data and the control TCR sequences from https://doi. org/10.1038/nature22383, and the raw data is available at Sequence Read Archive (https://www. ncbi.nlm.nih.gov/sra/) under accession number SRP101659. The VDJdb data has been obtained from VDJdb: a curated database of T-cell receptor sequences with known antigen specificity (https:// vdjdb.cdr3.net/). The HBV and SARS-CoV-2
RESEARCH ARTICLE
Predicting recognition between T cell receptors and epitopes with TCRGP
Emmi JokinenID1*, Jani HuuhtanenID2,3, Satu MustjokiID2,3,4, Markus HeinonenID1,5, Harri La ̈ hdesma ̈ ki1*
1 Department of Computer Science, Aalto University, Espoo, Finland, 2 Translational Immunology Research program and Department of Clinical Chemistry and Hematology, University of Helsinki, Helsinki, Finland,
3 Hematology Research Unit Helsinki, Helsinki University Hospital Comprehensive Cancer Center, Helsinki, Finland, 4 iCAN Digital Precision Cancer Medicine Flagship, Helsinki, Finland, 5 Helsinki Institute for Information Technology, Espoo, Finland
* emmi.jokinen@aalto.fi (EJ); harri.lahdesmaki@aalto.fi (HL)
Abstract
Adaptive immune system uses T cell receptors (TCRs) to recognize pathogens and to con- sequently initiate immune responses. TCRs can be sequenced from individuals and meth- ods analyzing the specificity of the TCRs can help us better understand individuals’ immune status in different disorders. For this task, we have developed TCRGP, a novel Gaussian process method that predicts if TCRs recognize specified epitopes. TCRGP can utilize the amino acid sequences of the complementarity determining regions (CDRs) from TCRα and TCRβ chains and learn which CDRs are important in recognizing different epitopes. Our comprehensive evaluation with epitope-specific TCR sequencing data shows that TCRGP achieves on average higher prediction accuracy in terms of AUROC score than existing state-of-the-art methods in epitope-specificity predictions. We also propose a novel analysis approach for combined single-cell RNA and TCRαβ (scRNA+TCRαβ) sequencing data by quantifying epitope-specific TCRs with TCRGP and identify HBV-epitope specific T cells and their transcriptomic states in hepatocellular carcinoma patients.
          Author summary
Humans have dedicated cells called T cells that recognize potentially harmful invaders using T cell receptors (TCRs) and protect us from infections. Each person has billions of T cells and millions of different kinds of TCRs that enable us to recognize a large variety of invaders. When for example a virus enters the body, T cells with TCRs recognizing that virus multiply and fight it. After such attacks, T cells that recognized the virus, remain in the body, forming an immunological memory. T cells thus characterize a person’s infec- tion history, but in order to reveal it, we need to determine which epitopes, molecular sig- natures of the invaders, different TCRs can recognize. We can characterize which TCRs recognize certain epitopes experimentally, but this is a time and sample consuming task and is not often possible with scarce patient samples such as biopsies. However, previously produced experimental data has enabled us to develop a computational method, TCRGP, that can predict which epitopes a TCR recognizes with a higher accuracy than previous
           PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814
March 25, 2021
1 / 27
 
PLOS COMPUTATIONAL BIOLOGY
Predicting recognition between T cell receptors and epitopes with TCRGP
 specific TCR sequences are available in the immunoSEQ Analyzer database (https://clients. adaptivebiotech.com) at https://clients. adaptivebiotech.com/pub/cheng-2018-sciimmunol and https://clients.adaptivebiotech.com/pub/covid- 2020, respectively. Finally, the single-cell data for T cells from liver cancer patients is available at Gene Expression Omnibus (https://www.ncbi.nlm.nih. gov/geo) under accession number GSE98638.
Funding: This study was supported by Academy of Finland https://www.aka.fi/en) under grant number 314442 to SM, by 647355 (M-IMM project); H2020 European Research Council (ERC), https:// erc.europa.eu to SM, by ERA PerMed (JAKSTAT- TARGET consortium) http://www.erapermed.eu to SM, by Finnish special governmental subsidy for health sciences, research and training, https:// minedu.fi/en/subsidies to SM, by The Sigrid Juselius Foundation, https://sigridjuselius.fi/en to SM and by the Cancer Foundation of Finland, https://syopasaatio.fi/en to SM. HL received support under grant numbers 314445, 335436 (Terva program: Heal-Art consortium); from Academy of Finland, https://www.aka.fi/en, 311584 (Quantifying molecular networks at single-cell level), from Academy of Finland, https://www.aka. fi/en, 313271 (ICT 2023 programme: TensorMed consortium); from Academy of Finland, https:// www.aka.fi/en, and from Cancer Foundation of Finland, https://syopasaatio.fi/en. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
Competing interests: The authors have declared that no competing interests exist.
This is a PLOS Computational Biology Methods paper.
Introduction
The adaptive immune system implements various complex mechanisms for surveillance against both pathogens and pathological cells arising in our body. To initiate an adequate adaptive immune response, a peptide, called epitope, must first be bound by the major histo- compatibility complex (MHC) class I or II molecule expressed on the surface of a nucleated cell or a professional antigen-presenting cell, respectively. The peptide-MHC (pMHC) com- plex is then presented to T cells which can recognize the complex via T cell receptor (TCR) proteins, consequently leading to T cell activation and proliferation by clonal expansion [1]. During clonal expansion, a fraction of T cells gain a long-living memory phenotype and there- fore a clonal population of T cells with identical TCR rearrangements remain for years against the recognized antigen [2], thus forming a potentially decodable immunological signature. Learning these signatures could have implications in broad range of clinical applications including infectious diseases, autoimmunity and tumor immunology.
T cells undergo non-homologous recombination during T cell development, which involves rearrangement of the germline TCR loci from a large collection of variable (V), diver- sity (D) and joining (J) gene segments as well as template-independent insertions and deletions at the V-D and D-J junctions [3, 4]. TCRs are formed by a pair of α- and β-chains (90-95% of T cells) or γ and δ-chains (5-10%) and V(D)J recombination happens in each locus indepen- dently. It is estimated that this rearrangement can result in the range of 1018 different TCR genes [5, 6] which provides enormous diversity for epitope-specific T cell repertoires.
The complementarity determining regions (CDRs) of a TCR determine whether the TCR recognizes and binds to an antigen or not [7]. Of these regions, CDR3 is the most variable and primarily interacts with the peptide, while CDR1 and CDR2 primarily interact with the peptide binding groove of the MHC protein presenting the peptide but they can also be directly in con- tact with the peptide [8, 9]. Dash et al. [10] noted that also a loop between CDR2 and CDR3 (IMGT positions 81-86 [11]), which they called CDR2.5, has sometimes been observed to make contact with pMHC in solved structures.
It is well known that the CDR3β of a TCR is important in recognizing peptides presented to the T cell, but it still remains unclear which specific physicochemical or structural features of the CDR3β or of other parts of the TCR determine the antigen recognition specificity of the T cell. High-throughput sequencing of V- and J-segment enriched DNA has enabled large-scale characterization of TCR sequences, initially only for the CDR3β with bulk methods [6, 12] but recently for the whole paired TCRαβ at single-cell resolution using plate or droplet-based methods [13, 14]. Nevertheless, profiling of epitope-specific TCRs remains exhaustive as they require sample-consuming experiments with distinct pMHC-multimers for each epitope of interest. Therefore, there is a great need for models that examine which epitopes a TCR can recognize or to which TCRs an epitope can bind to [15]. Curated databases of experimentally verified TCR-peptide interactions have recently been launched, such as VDJdb, IEDB, and McPAS [16–18]. Such data sources enable more comprehensive, data-driven analysis of TCR-
 methods. As a computational method, TCRGP spares resources from time and sample consuming experimental validations and offers an interesting way to analyze publicly available TCR data.
                                         PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 2 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 peptide interactions, and allow the use of statistical machine learning techniques for the afore- mentioned tasks. Yet only a few computational methods for predicting recognition between TCRs and epitopes [10, 19–22] and for clustering similar TCRs [9, 23, 24] have been published. In addition to supervised and unsupervised methods for predicting TCR-epitope interactions, computational methods and web services such as [25] have also been proposed to predict the structure of TCRs based on their amino acid sequences.
We propose a method called TCRGP which builds on non-parametric modelling using Gaussian process (GP) classification. The probabilistic formulation of GPs allows robust model inference already from small data sets, which is a great benefit as currently there exists very limited amounts of reported TCR-epitope interactions in curated databases. As the space of all TCRs that can recognize a certain epitope is potentially very large, it is important to avoid overfitting to the limited sample of TCRs that is available. Indeed, TCRGP clearly out- performs the current state-of-the art methods for predicting the epitope specificity of TCRs. At the same time TCRGP can scale to exploit extremely large data sets of epitope-specific TCRs, which we expect to become more common in the future. We also analyze the effects of utilizing different sections of the TCR amino acid sequence and learn which of them are most impor- tant and examine how the number of TCRs used for training affects the predictions. Finally, we demonstrate the potential of TCRGP by analyzing single-cell RNA+TCRαβ-sequencing data from hepatocellular carcinoma patients.
Results
Gaussian process classifier
Gaussian processes (GP) are a flexible class of models that have become popular in machine learning and statistics with various applications in molecular biology, bioinformatics and other fields [26–30]. We have developed TCRGP, a GP based probabilistic classifier which can be trained to predict TCRs’ specificity to any epitope given sufficient training data for the spec- ified epitope. GPs implement a Bayesian nonparametric kernel method for learning from data. They differ from standard parametric models in that instead of learning the parameters of a predefined function, they define priors for the entire class of nonlinear functions and learn a suitable function for the prediction task. Properties of GPs are defined by the kernel function, which is a function of objects that we want to classify. TCRGP uses Gaussian process classifica- tion with variational inference [31, 32]
To train a GP classifier for predicting if TCRs recognize a certain epitope, a set of training data consisting of TCRs that are known to recognize and to not recognize that epitope are required. TCRGP can utilize CDR3 amino acid sequences from both TCR α- and β-chains as well as CDR1, CDR2, and CDR2.5 sequences which can be determined from Vα- and Vβ- genes. The different CDRs are then aligned within each CDR type using the IMGT definitions and a numerical presentation of them is formed using principal components of a modified BLOSUM62 substitution matrix (see Fig 1A).
Using the numerical presentations, a separate covariance matrix is created for each CDR type. The covariance matrices measure similarity between all pairs of TCRs (separately for each CDR type). The GP then learns optimal parameters for the kernel functions that define the covariance matrices for each CDR type and learns simultaneously an optimal combination of these kernel functions. As the model can determine itself the weights for each CDR type, it can learn which CDRs are important for the classification task. Due to the probabilistic nature of GPs, we can use marginal likelihood maximisation for the optimisation and do not require additional data for validating the learned parameters. This is a great benefit, when there exists limited amounts of epitope-specific TCRs, which is currently the case with many epitopes.
             PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 3 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
  Fig 1. TCRGP pipeline for training a classifier to predict if new unseen TCRs recognize a certain epitope. (A) Sequence preparation. For the training, TCRs specific to the epitope of interest and control sequences not recognizing the epitope are required. TCRGP can utilize CDR3, CDR1, CDR2, and CDR2.5 sequences from both TCR α- and β-chains. A separate alignment is created for each CDR type and the aligned sequences are given numerical presentations. We use principal components of a modified BLOSUM62 substitution matrix to encode each amino acid. We utilize all 21 components, but in this illustration we only show the first components. (B) Using the numerical presentation, we create separate covariance matrices for each CDR type. During the training of the Gaussian process classifier, an optimal combination of the base kernels and their parameters are learned. (C) When the classifier has been trained, we can make probabilistic predictions for new TCRs.
https://doi.org/10.1371/journal.pcbi.1008814.g001
Moreover, by using sparse variational inference, the GPs can also scale to extremely large data sets when more data is available (see Fig 1B).
Once the TCRGP classifier has been trained, it can be used to provide predictions to new, previously unseen, TCRs. The certainty of the predictions is encoded in the predicted values: If the prediction for a TCR is close to one, it is very likely to recognize the epitope in question, if it is close to zero, the TCR very likely does not recognize the epitope. However, if the predic- tion is around 0.5, the model cannot give a prediction with much certainty to that TCR (see Fig 1C). A more detailed description of TCRGP can be found from the Materials and Methods section.
We use two data sets to demonstrate TCRGP’s accuracy in predicting TCR epitope specific- ity: a recently published data set of tetramer sorted TCR sequences for 10 epitopes, introduced by Dash et al. [10] (Dash data), and a new dataset of medium and high quality epitope-specific TCR sequences extracted from VDJdb database [16] (VDJdb data). The Dash data provides a large set of epitope-specific paired TCRαβ-data and the VDJdb data provides a comprehensive selection of available epitope-specific TCRβ-data currently available. We also considered using using TCRs from IEDB [17] and McPAS [18], but they had significant overlap with VDJdb
         PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 4 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 and their collections of TCRβs were not as extensive. Both of the selected data sets are com- bined with a set of background TCRs, also presented by Dash et al. [10] that are not expected to recognize the epitopes in the two data sets. Our work is accompanied by an efficient soft- ware implementation that contains trained models for predicting TCRs’ specificity to epitopes involved in data sets used in this study as well as tools for building new epitope specificity models from new datasets. The implementation and used data sets are available at github.com/ emmijokinen/TCRGP.
Significance of utilizing different CDRs
To evaluate the benefit of using different CDRs, we used the Dash data which includes 4635 pMHC-tetramer sorted single-cell sequenced TCRαβ clonotypes from 10 epitope-specific rep- ertoires. We trained our TCRGP model using either only CDR3 or also with CDR1, CDR2, and CDR2.5 from TCRα, TCRβ, or both. We applied leave-one-subject-out cross-validation as described in Materials and Methods Section. Fig 2A presents the cross-validation results for a single BMLF1280-288-epitope from EBV and demonstrates how the classification results vary between different subjects likely due to the variety of the TCRs.
AUROC scores of the predictions for different combinations of CDRs and α/β chains are summarized in Fig 2B. For comparison, we also trained TCRdist [10] in the same manner. Fig 2B shows that both methods, TCRGP and TCRdist, perform on average better when using TCRβ than when using TCRα, although using both α- and β-chains generally provides the best results. There are few exceptions, as shown in Fig 2B–2D and S1 Fig and Table 1. For example, with epitope pp65 both methods perform better when using CDR3α instead of CDR3β. Overall TCRGP is better than TCRdist in utilizing information from CDRs other than CDR3. TCRGP achieves higher AUROC scores on average when trained using all CDRs instead of only CDR3, whereas with TCRdist the AUROC scores seem to be similar or better when only CDR3 is utilized. Notably TCRGP outperforms TCRdist in prediction accuracy for 57 of the 60 comparisons (Table 1 and S1 Fig). To quantify the significance of accuracy differences between different methods, we applied Wilcoxon signed-rank test to paired AUROC scores across epitopes. We used the test to study three questions: do AUROC scores differ between TCRGP and previous methods, is it better to use all CDRs or only CDR3s, and which chains should be used to produce best results. The Benjamini-Hochberg corrected P-values for these tests can be found from S2 File. These results support the above observations: TCRGP outper- forms TCRdist, with TCRGP it is beneficial to use all CDRs, and the use of β chain generally gives better AUROC scores than α chain, although it is best to use both chains. It is notewor- thy, that although Dash et al. [10] use leave-one-subject-out control, the results shown here differ somewhat from their results. The differences are likely caused by differences in the back- ground TCRs as we have selected them randomly from the larger set of background TCRs pro- vided by Dash et al.
Fig 2B–2D, Table 1, and S1 Fig also show that the AUROC scores can have notable differ- ences between different epitopes even when the same combinations of CDRs and α/β chains have been utilized. Some of these differences may be explained by the differences in the num- ber of available training samples, for example for CMV-epitope pp65495-503 there were only 76 TCRs from 6 subjects in the Dash data, which may have contributed to a lower prediction accuracy. To address this, we evaluated the models also using leave-one-out cross-validation with only unique, private TCRs to see how the models perform when predictions are done only on new TCRs. With both TCRGP and TCRdist, the average AUROC scores improve slightly (S3 Fig), demonstrating that the models can predict if completely new TCR sequences are specific to these epitopes and that the larger number of TCRs used for training (due to the
                    PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 5 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
  Fig 2. Epitope-specificity prediction with the Dash data. (A) The left panel shows the cross-validated ROC curves for each subject in the Dash data for BMLF1280-288, when TCRGP has been trained using all CDRs from TCRα and TCRβ. The mean AUROC score is 0.905. The right panel shows the cross-validated ROC curve for all subjects and also the corresponding threshold values. From this figure we can determine which threshold values correspond to different true positive rates (TPRS) and false positive rates (FPRS). (B) Violin plots present the distributions of AUROC scores for the different epitopes obtained with models utilizing varying CDRs. The blue parts of the violin plots illustrate the AUROC scores of predictions made by TCRGP for all the epitopes. The orange sides illustrate the AUROC scores obtained with TCRdist. Each point within a violin plot presents the mean AUROC score obtained for one epitope. The used chains (α and/or β) and CDRs (three or all) are indicated below each panel. (C) Comparison of AUROC scores obtained with TCRGP and TCRdist using only CDR3 from TCRαβ, TCRβ, or TCRα for each epitope separately. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using CDR3 from α- and β-chains (blue line). (D) Comparison of AUROC scores obtained with TCRGP and TCRdist using all CDRs from TCRαβ, TCRβ, or TCRα for each epitope separately. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using all CDRs from α- and β-chains (blue line). (E) Fractions of total weight given to kernels corresponding to different CDRs, when TCRGP has been trained to predict which TCRs are specific to the epitopes in the Dash data using all CDRs from both TCR chains.
https://doi.org/10.1371/journal.pcbi.1008814.g002
larger folds in leave-one-out cross validation) improve the model performances. A summary of all prediction results is shown in S1 File.
To better understand the significance of the different CDRs for TCR-pMHC recognition, we also examined more closely how TCRGP weighted the kernels created for the different CDRs, when all CDRs from both chains were utilized. Fig 2C illustrates which CDRs were found important for the different epitopes. As one might expect, with most of the epitopes most weight was given to the CDR3β, but with all epitopes some weight was also given to other CDRs, suggesting that the classifier benefits from utilizing several CDRs. This is in agreement with an alignment of 52 TCR sequences from TCR-pMHC PDB structure complexes, which demonstrates that all CDRs can be within 5Å of a peptide [9]. For example with CMV-
epitope pp65495-503, experimental characterization of the structure (PDBid 3GSN) showed
      PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 6 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP Table 1. Mean AUROC scores for the Dash data using leave-one-subject-out cross-validation. TCRGP models and TCRdist models were trained using either only
 CDR3s or all CDRs from TCRαβ, TCRβ, or TCRα.
   Method
TCRGP
        chains
αβ
β
α
αβ
β
                  cdrs
3
all
3
all
3
all
3
all
3
all
3
                         mean
0.871
0.913
0.830
0.883
0.823
0.856
0.834
0.828
0.807
0.797
0.781
                         BMLF1280-288
0.876
0.906
0.818
0.877
0.862
0.877
0.875
0.830
0.816
0.774
0.821
                         pp65495-503
0.702
0.821
0.587
0.871
0.759
0.787
0.645
0.691
0.636
0.670
0.672
                         M158-66
0.781
0.833
0.782
0.834
0.769
0.768
0.769
0.768
0.774
0.729
0.716
                         PB1-F262-70
0.847
0.937
0.822
0.803
0.753
0.801
0.820
0.862
0.802
0.753
0.729
                         NP366-374
0.906
0.928
0.897
0.907
0.821
0.847
0.874
0.818
0.841
0.788
0.781
                         PA224-233
0.952
0.975
0.939
0.961
0.882
0.930
0.914
0.914
0.887
0.862
0.869
                         PB1703-711
0.929
0.953
0.913
0.919
0.862
0.929
0.913
0.819
0.901
0.844
0.849
                         m139419-426
0.877
0.888
0.721
0.806
0.844
0.821
0.773
0.785
0.656
0.779
0.769
                         M38316-323
0.926
0.922
0.917
0.924
0.868
0.902
0.921
0.918
0.920
0.914
0.844
                         M45985-993
0.920
0.962
0.891
0.923
0.851
0.916
0.876
0.879
0.846
0.838
0.796
             TCRdist
 α
all
   0.732 EBV 0.778 CMV 0.822 IAV 0.686 IAV 0.636 IAV 0.709 IAV 0.777 IAV 0.711 mCMV 0.683 mCMV 0.789 mCMV 0.772
                      https://doi.org/10.1371/journal.pcbi.1008814.t001
 CDR3β, CDR1β, CDR3α and CDR1α to be within 5Å of the peptide, and also CDR2β within 5.8Å of the peptide. Another TCR-pMHC-complex structure (PDBid 5D2L) for the same pp65495-503-epitope suggests that CDR2β was also within 5 Å of the peptide. Indeed, the opti- mized weights for the pp65495-503-epitope (Fig 2C) show some correspondence to the observed contacts. Although with many epitopes CDR3β was considered as the most important CDR, this was not the case with all epitopes. For example with mCMV-epitope m139419-426 CDR3α is more important for the prediction, while with EBV-epitope BMLF1280-288 most of the weight was given to CDR2α and CDR2β. Still, also with BMLF1280-288 epitope the CDR3β alone pro- vided discriminative information, as demonstrated by the high AUROC score (0.818) when TCRGP was trained only with CDR3β-sequences (see Table 1 or S1 Fig).
Comparisons to other methods
Currently only a few methods for predicting the epitope-specificity of TCRs have been pub- lished, including TCRdist [10] and a random forest (RF) classifier by De Neuter et al. [19]. TCRdist uses a BLOSUM62 based distance measure between amino acids and defines the dis- tance between two (aligned) TCRs as a weighted sum over the amino acid distances. Using this distance measure, they determine if a TCR is closer to an epitope-specific cluster of TCRs or to a cluster of background TCRs. De Neuter et al. use feature vectors formed by biophysical and other features extracted from TCR amino acid sequences to train a RF classifier. Gielis et al. [33] have later published a web server TCRex that is based on the RF classifier. However, we have utilized the implementation from the original article to be able to train models with the same training sets as with the other models in the comparisons. More recently also methods relying on neural networks have been proposed, such as DeepTCR, NetTCR, and Ergo [20–22]. DeepTCR relies on convolutional networks and uses one-hot encoding to present CDR3 and V/D/J-genes from α- and β-chains, or a subset of these, to learn embeddings for the sequences. NetTCR, which is intended for HLA-A�02:01 restricted epitopes, also uses convolutional neural networks, but utilizes BLOSUM50 encoding for presenting the CDR3β and the epitope sequence. Ergo uses LSTM (long-short term memory) network to encode the peptide sequence and either LSTM or an autoencoder to encode the CDR3 and utilize feed forward networks for predictions. Here we provide comparisons between TCRGP, TCRdist, RF, and DeepTCR.
         PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 7 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
  Fig 3. Epitope specificity prediction with the VDJdb data. (A) The left panel shows the cross-validated ROC curves for each subject in the VDJdb data for HCV NS31436-1444-epitope, when TCRGP has been trained using TCRα and TCRβ with all CDRs. The mean AUROC score is 0.944. The right panel shows the cross-validated ROC curve for all subjects and also the threshold values for classification are shown. From this figure we can determine which threshold values correspond to different true positive rates (TPRS) and false positive rates (FPRS). (B) One violin plot presents the distribution estimate of mean AUROC scores obtained with one method for all epitopes in our VDJdb data. Below each violin plot there is the name of the method used and in the brackets which CDRβs have been used (3 for CDR3, all for CDR1, CDR2, CDR2.5, and CDR3). Each point within a violin plot presents the mean AUROC score obtained for one epitope. RF refers to the Random Forest TCR-classifier of De Neuter et al. [19]. RF using only CDR3β has not been included in this figure as it could not provide predictions for all of the 22 epitopes. (C) Comparison of AUROC scores obtained with the different methods for each epitope separately. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using all CDRβs (orange line) (D) For each epitope from the VDJdb dataset, TCRGP models were trained using different numbers of unique epitope-specific TCRβs, always complemented with the same number of control TCRβs. For each point of the learning curve the model was trained with 100 random samples of the TCRβs, using either CDR1, CDR2, CDR2.5, and CDR3 (blue curves), or only CDR3 (orange curves). The darker lines show the mean of the predictions and the shaded areas ± the standard deviation for the 100 folds. The points indicate the tested sample sizes. Here learning curves for four peptides are shown. (E) Leave-one-out cross-validated AUROC scores correlate with the diversity and number of samples (Pearson correlation -0.66). The sizes of the circles indicate the number of unique TCRs used for training.
https://doi.org/10.1371/journal.pcbi.1008814.g003
To compare TCRGP to these other methods, we experimented with the VDJdb data and we again used leave-one-subject-out cross-validation as described in Materials and Methods Sec- tion. We trained TCRGP and TCRdist using only CDR3β and then also with the other CDRβs. Fig 3A shows the ROC curves when TCRGP was trained with all CDRβs to predict which TCRs are specific to the HCV-epitope NS31436-1444.
RF and DeepTCR also utilize the CDR3β sequence, but use Vβ-gene instead of the other CDRβs. RF could also utilize the J-gene and DeepTCR J- and D-genes, but unfortunately, the background TCR data set from Dash et al. [10] did not contain information of these genes.
      PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 8 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 Table 2. Mean AUROC scores for the VDJdb data using leave-one-subject-out cross-validation. TCRGP models and TCRdist models were trained using TCRβ with either only CDR3β or all CDRβs. RF models and DeepTCR models were trained using only CDR3β or CDR3β with Vβ-gene, from which the other CDRβs can be derived from.
     Method
BMLF1280-288
TCRGP
TCRdist
RF
            CDRs
3
all
3
all
3
3, V-gene
3
                 mean
0.806
0.863
0.743
0.783
NaN
0.796
0.776
                 pp65123-131
0.852
0.792
0.804
0.761
0.820
0.790
0.834
                 pp65417-426
0.912
0.968
0.829
0.874
0.930
0.930
0.890
                 pp65495-503
0.892
0.904
0.793
0.786
0.900
0.890
0.860
                 0.926
0.947
0.863
0.872
0.930
0.910
0.897
                 BZLF1190-197
0.887
0.899
0.764
0.857
0.840
0.870
0.807
                 BRLF1109-117
0.682
0.756
0.729
0.762
0.770
0.780
0.670
                 M158-66
0.881
0.933
0.832
0.852
0.910
0.890
0.868
                 HA306-318
0.706
0.748
0.753
0.790
NaN
0.550
0.685
                 NS31073-1081
0.695
0.783
0.492
0.600
0.660
0.730
0.625
                 NS31406-1415
0.678
0.766
0.698
0.676
0.560
0.560
0.631
                 NS31436-1445
0.819
0.945
0.746
0.799
0.850
0.850
0.824
                 VP2249-57
0.801
0.814
0.721
0.713
0.850
0.860
0.858
                 NS4B214-222
0.825
0.823
0.692
0.695
0.890
0.870
0.649
                 NS3133-142
0.864
0.914
0.878
0.920
0.900
0.920
0.906
                 NS3133-142
0.734
0.859
0.842
0.893
0.530
0.710
0.850
                 p2430-40
0.894
0.886
0.836
0.808
0.880
0.880
0.818
                 p2448-56
0.817
0.946
0.716
0.800
0.720
0.810
0.624
                 p24128-135
0.770
0.740
0.541
0.594
0.720
0.720
0.719
                 p24131-140
0.838
0.859
0.701
0.724
0.760
0.790
0.740
                 p24161-180
0.750
0.992
0.769
0.975
0.780
0.890
0.846
                 p24223-231
0.702
0.894
0.686
0.819
NaN
0.520
0.651
                 Nef90-97
0.798
0.818
0.667
0.657
0.780
0.800
0.809
              CMV
CMV CMV EBV
EBV
EBV
IAV
IAV HCV HCV HCV HSV-2 YFV DENV1 DENV3-4 HIV-1 HIV-1 HIV-1 HIV-1 HIV-1 HIV-1 HIV-1
https://doi.org/10.1371/journal.pcbi.1008814.t002
DeepTCR
3, V-gene
0.777
0.775 0.860 0.874 0.907 0.782 0.564 0.893 0.602 0.626 0.704 0.849 0.823 0.664 0.876 0.843 0.858 0.794 0.771 0.656 0.844 0.703 0.830
However, according to De Neuter et al., not much weight was given to the J-gene at least in their experiments. Moreover, when RF and DeepTCR utilize CDR3β and Vβ-gene and TCRGP and TCRdist use all CDRs, all four methods get the same sequence information, although in a slightly different form, as the other CDRβs are derived from the Vβ-gene.
Fig 3B shows the distribution estimates of mean AUROC scores for each model trained for the 22 different epitopes. With the VDJdb data, we can see that TCRGP and TCRdist both per- form better, when all CDRβs have been utilized. Remarkably, TCRGP achieves higher mean AUROC scores than the other methods when using all CDRs, but also when only the CDR3β is utilized. AUROC scores for the different epitopes are presented in Table 2 and S4 Fig and Fig 3C illustrate how the AUROC scores vary between different methods separately for each epitope. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using all CDRβs. We can see that with most epitopes there are large differences in the accuracies of the different methods.
In the VDJdb data, there were also TCRs that appeared in samples collected from multiple subjects (see Table 3). We therefore trained the models also using leave-one-out cross-valida- tion with only unique TCRs. In this case we considered a TCR to be unique if it had a unique combination of CDR3β amino acid sequence and Vβ-gene, as we only utilized the TCRβ. As with the Dash data above, our results (S5 and S6 Figs) show that the models can predict if completely new sequences are specific to these epitopes, thus demonstrating their use for
                                                     PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 9 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 10 / 27
 Table 3. Datasets. Dash data: The data set constructed by Dash et al. [10] contains epitope-spcefic TCRs for Epstein-Barr virus (EBV), human Cytomegalovirus (CMV), Influenza A virus (IAV) and mouse Cytomegalovirus (mCMV). VDJdb data: Data set gathered from VDJdb contains epitope-specific TCRs for Cytomegalovirus (CMV), Epstein-Barr virus (EBV), Influenza A virus (IAV), Hepati- tis C virus (HCV), Herpes Simplex virus type 2 (HSV-2), Yellow Fever virus (YFV), Dengue virus type 1 (DENV1), Dengue virus type 3 (DENV3-4), and Human immunodeficiency virus type 1 (HIV- 1). For the MHC chains we show here only the allele group. For some epitopes there are TCRs for which there exists more detailed information and some variation, which are shown in S3 File.
Species
Epitope species
Epitope gene
Epitope
Dash data MHC chain 1 HLA-A� 02:01 HLA-A� 02:01 HLA-A� 02:01
MHC chain 2
Subjects Samples Unique TCRαβs 6 76 69
10 61 60
Human
EBV
BMLF1280-288 pp65495-503
GLCTLVAML NLVPMVATV GILGFVFTL LSLRNPILV ASNENMETM SSLENFRAYV SSYRRPVGI TVYGFCLL SSPPMFRV HGIRNASFI
- - - - - - - - - -
Mouse
CMV IAV IAV IAV IAV IAV mCMV mCMV mCMV
M158-66 PB1-F262-70
Db Db Db Kb Kb Kb Db
15 275 237 9 117 117 24 305 263 15 324 293 34 642 584 8 87 87 14 158 143 13 291 271
Human CMV CMV CMV
pp65123-131 pp65417-426 pp65495-503 BMLF1280-288 BZLF1190-197 BRLF1109-117 M158-66 HA306-318 NS31073-1081 NS31406-1415 NS31436-1445 VP2249-57 NS4B214-222 NS3133-142 NS3133-142 p2430-40 p2448-56 p24128-135 p24131-140 p24161-180 p24223-231 Nef90-97
IPSINVHHY TPRVTGGGAM NLVPMVATV GLCTLVAML RAKFKQLL YVLDHLIVV GILGFVFTL PKYVKQNTLKLAT CINGVCWTV KLVALGINAV ATDALMTGY RPRGEVRFL LLWNGPMAV GTSGSPIVNR GTSGSPIINR KAFSPEVIPMF TPQDLNTML EIYKRWII KRWIILGLNK FRDYVDRFYKTLRAEQASQE GPGHKARVL FLKEKGGL
VDJdb data HLA-B� 35 HLA-B� 07 HLA-A� 02 HLA-A� 02 HLA-B� 08 HLA-A� 02 HLA-A� 02
B2M
B2M
B2M
B2M
B2M
B2M
B2M HLA-DRB1� 01,04 B2M
17 65 58
EBV EBV EBV IAV IAV HCV HCV HCV HSV-2 YFV DENV1 DENV3-4 HIV-1 HIV-1 HIV-1 HIV-1 HIV-1 HIV-1 HIV-1
HLA-DRA� 01 HLA-A� 02 HLA-A� 02 HLA-A� 01 HLA-B� 07 HLA-A� 02 HLA-A� 11 HLA-A� 11 HLA-B� 57 HLA-B� 42,81 HLA-B� 08 HLA-B� 27 HLA-DRA�01 HLA-B� 07 HLA-B� 08
B2M
B2M
B2M
B2M
B2M
B2M
B2M
B2M
B2M
B2M HLA-DRB1�01,07,11,15, HLA-DRB5�01 B2M
29 184 122 103 413 242 54 299 152 17 225 149 6 66 51 50 239 138 11 56 50 7 76 39 4 65 65 7 152 139 5 68 29 5 223 198 11 65 59 8 51 46 44 134 104 21 52 40 12 81 60 27 212 141 17 141 95 1 62 53 21 104 78
https://doi.org/10.1371/journal.pcbi.1008814.t003
NP366-374 PA224-233 PB1703-711 m139419-426 M38316-323 M45985-993
B2M

PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 epitope specificity prediction for previously unseen TCRs. A summary of all prediction results is shown in S1 File.
To affirm the above observations, we again applied Wilcoxon signed-rank test. We used the test to study two questions: do AUROC scores differ between TCRGP and previous methods, and is it better to use all CDRβs rather than only CDR3βs. The Benjamini-Hochberg corrected P-values for these tests (see S2 File) support the visual observations we made from Fig 3B.
Significance of the number of training samples
To assess how the number of epitope-specific TCRs affects the performance of TCRGP classi- fier, we trained our model using different numbers of epitope-specific TCRs from the VDJdb data. We selected all unique TCRs for each epitope and took 100 random samples from them for each training set size and used the remaining TCRs for testing. The epitope-specific TCRs in the training and test sets were accompanied with equal numbers of randomly chosen con- trol TCRs. Learning curves for four epitopes are shown in Fig 3D and learning curves for all 22 epitopes in S7 Fig. In general, the predictive performance of the TCRGP classifiers improve when more training samples are available. Indeed, we observed a negative correlation between TCRs’ diversity and sample size ratio and prediction accuracy (Fig 3E). We also examined if the diversities of epitope-specific TCRs within and between subjects differ. We found that although the diversities of TCRs specific to different epitopes vary, on average diversities within and between subjects are similar (panel B in S8 Fig). However, there is still variation in the diversities between pairs of subjects (panel C in S8 Fig).
These learning curves also further demonstrate the benefit of using multiple CDR sequences: With most of the epitopes using all CDRs produces better or comparable AUROC scores with all sample sizes, although there are a few epitopes with which the AUROC scores are higher when utilizing only the CDR3β if the sample sizes are very small (�40). These results also suggest that with many epitopes it may be more beneficial to sequence a moderate amount of TCRs in such precision that in addition to the CDR3 also the V-gene and allele (and thus the CDR1, CDR2, and CDR2.5) can be determined, than to sequence large amounts of only CDR3s. These findings are in line with the weights learned by TCRGP for each CDR3 for the individual epitopes, as we can see in the case of CMV-epitope pp65495-503, EBV-epitope BMLF1280-288 and IAV-epitope M158-66 (see Fig 2). With pp65495-503 most weight was given
to CDR3β and thus information from other CDRs was not considered as beneficial; with BMLF1280-288 almost no weight was given to CDR3β and in the learning curves there is a clear improvement when all CDRβs are used; with M158-66 some weight was given to CDR3β, but most weight fell to CDR2β and correspondingly there is a small improvement in the learning curves, when all CDRβs are utilized. Overall, the learning curves show that TCRGP can learn an accurate predictor even from a small data set, thus making it applicable to the currently existing TCR-peptide interaction data sets. On the other hand, our results also show that TCRGP’s prediction accuracy increases along with increasing number of training examples, enabling analysis of larger TCR-peptide interaction data sets in the future.
Discriminating between epitope-specific TCRs
To further validate the performance of TCRGP, we also tested it in a setting were we use only TCRs from the VDJdb data so that TCRs specific to one epitope are considered as positive and TCRs specific to the other epitopes are considered as negative. We only utilized unique TCRs for each epitope and used stratified 200-fold cross validation. We observed some cross-reactiv- ity between TCRs specific to some of the epitopes, for example DENV1 epitope NS3133-142 and DENV3-4 epitope NS3133-142 have many TCRs that are specific to both epitopes. When each of
          PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 11 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
  Fig 4. Mean AUROC scores obtained from TCRGP models with different kinds of data. Blue violin plots show results of models trained with only CDR3β and orange violin plots of models trained with all CDRβs. (A) Mean AUROC scores from TCRGP models for the 22 epitopes in VDJdb data, when TCRs specific to one epitope are considered as positive and TCRs specific to the other 21 epitopes are considered as control data. 200-fold stratified cross validation was used for the evaluation. (B) Same as (A), but only the 885 unique TCRs specific to the seven HLA-A�02 restricted epitopes are used.
https://doi.org/10.1371/journal.pcbi.1008814.g004
the models were trained, we therefore excluded TCRs from the control data that were known to recognize the epitope in question. Fig 4A shows that TCRGP performs well also with this kind of data, as the mean AUROC scores are very similar to those achieved when using leave- one-out cross-validation and the control data from Dash et al. [10] (see S5 and S9 Figs).
The VDJdb data we have used contains epitopes restricted by MHCs with different HLA types. Structural data from TCR-pMHC complexes indicate that CDR1 and CDR2 are likely to interact more with the MHC than with the epitope. To see if using other CDRβs than CDR3β is beneficial also when all epitope-specific and control TCRs can recognize epitopes restricted by MHCs of the same HLA-type, we performed the above test with only TCRs specific to HLA-A�02 restricted epitopes (see Table 3 and S3 File for information about the HLA types). With this criteria we obtained 885 TCRs specific to seven epitopes (pp65495-503, BMLF1280-288, BRLF1109-117, M158-66, NS31073-1081, NS31406-1415, NS4B214-222). Fig 4B and S9 Fig show that using all CDRβs instead of only CDR3β is beneficial also in this case and that the predictive performance remains good. In fact, the mean AUROC scores in this case (0.794 and 0.821 when only CDR3β and all CDRβs were used) are higher than the mean AUROC scores obtained for the same seven HLA-A�02 restricted epitopes, when TCRs for all 22 epitopes were utilized (0.746 and 0.786, respectively), see S9 Fig. This analysis supports our finding that CDRs other than CDR3 contain information that is useful for predicting TCR-epitope recog- nition. A summary of all prediction results is shown in S1 File.
Evaluation on TCR data from independent studies
The above experiments have demonstrated TCRGP’s accuracy when the classifier has been tested with TCR sequences from new subjects. Next we tested TCRGP’s robustness to general- ize between independent studies. Specifically, we tested how TCRGP performs when trained on the epitope-specific TCR sequences from one study and tested on sequences from another independent study. We performed this experiment for the nine epitopes from the VDJdb data that have a study with at least 50 TCRs for training the model and another independent study with at least 10 TCRs for testing. When there were multiple studies that fulfilled this criteria, we chose the two studies with most unique TCRs for training and testing, respectively. When only CDR3β was utilized, we obtained a mean AUROC score of 0.813, and with all CDRβs the
             PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 12 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 AUROC score was 0.865, see S1 File for AUROC scores for all these epitopes and information of the number of TCRs for each study.
We also performed a leave-one-study-out cross-validation for all 10 epitopes that have always at least 50 TCRs for training and 10 TCRs for testing when divided to folds based on the study they were obtained from. For this experiment we only used studies with at least five TCRs. We achieved AUROC scores of 0.838 and 0.861 when utilizing CDR3β and all CDRβs, respectively (see S1 File).
Additionally, we trained a classifier for COVID-19 epitope S-protein269-277 (YLQPRTFLL) with the 352 epitope-specific TCRβs from the recent study of Shomuradova et al. [34] and tested the classifier with the 415 in-frame S-protein269-277-specific TCRs from the ImmuneR- ACE study launched by Adaptive Biotechnologies and Microsoft (https://immunerace. adaptivebiotech.com, June 10, 2020 dataset). The immuneRACE data did not have sufficient information of the Vβ-gene for us to utilize all CDRβs, so the classifier was trained and tested only with CDR3β. Nonetheless, we achieved an AUROC score of 0.895 on the independent test data.
These experiments show that TCRGP is robust to overfitting and has high accuracy also when testing with TCRs from independent studies.
Leveraging TCRGP in single-cell RNA+TCRαβ-sequencing data analysis
We next demonstrate how TCRGP can be utilized to implement a novel analysis of combined single-cell RNA and TCRαβ (scRNA+TCRαβ) sequencing data. Hepatocellular carcinoma (HCC) is one of the leading causes for cancer-related deaths worldwide [35]. Globally the predominant cause of HCC is considered to be Hepatitis B virus (HBV) as half of the HCC patients are estimated to be chronic HBV carriers [36]. During the course of natural infection, HBV integrates itself into the genome of the hepatocytes and thus a proportion of the HCC cells expresses HBV antigens [37]. Therefore, the malignant cells could be targeted by HBV- specific T-cell clonotypes and the high-dimensional characterization of these clonotypes could be crucial in understanding the viral control of HBV-infection and its association to HCC. To address this previously unanswered question we used TCRGP to analyze a dataset of single-cell RNA and TCRαβ of T cells from HBsAg-positive HCC-patients from blood, non-malignant liver tissue and tumour tissue published by Zheng et al. [38] (the Zheng data, see Materials and methods for details).
We utilized HBV-reactive T cell populations mapped by Cheng et al. [39] (the Cheng data) to train TCRGP classifiers (see Materials and methods for details) to enable prediction for the unselected TCR repertoire in the Zheng data against the four epitopes (HBVcore169, HBVcore195, HBVpol282, HBVpol387) (Fig 5A).
Of the 789 CD8+ cells from Zheng data analyzed with TCRGP, 108 cells (13.688%), were predicted to be reactive against any of the four HBV epitopes with at least a TCRGP-based probability of 85%, most of which against HBVcore195-epitope (59 cells) (Fig 5A–5C). On the contrary, 176 cells were predicted to be reactive against common viruses (CMV = 22 (three epitopes), EBV = 88 (three epitopes) and Influenza A = 66 cells (two epitopes)) (Fig 5B), sug- gesting that HBV was the most common target for antigen-specific T cells in HCC patients.
After unsupervised clustering of the CD8+ cells’ scRNA-seq data, we received 6 different phenotypes that were similar to the phenotypes described by Zheng et al. [38], but had the exhausted cells divided into 3 different clusters instead of one (naïve, effector, memory, exhausted 1, exhausted 2 and exhausted 3) (Fig 5C). Interestingly, cells in exhausted 3 cluster showed the highest enrichment of the clonotypes targeting HBVcore195-epitope (Fisher’s exact
                   PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 13 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
  Fig 5. Analysis of HBV-specific T cells in HCC patients. (A) Schematics for the analysis of single-cell RNA and TCRαβ sequencing data using TCRGP and multimer-sorted data. (B) Numbers of cells predicted to recognize different epitopes by TCRGP with probability of at least 85%. HBV- reactivity was assessed by four different TCRGP classifiers trained against four different HBV-epitopes (HBVcore169, HBVcore195, HBVpol282, HBVpol387). Other predictions were made using the models trained with the VDJdb data. (C) Dimensionality reduced representation (UMAP) of the 1189 CD8+ T cells from HBV+ HCC-patients from peripheral blood, normal adjacent tissue and tumour tissue. Encircled dots represent the T cells predicted to be HBV-reactive by TCRGP. (D) The frequencies of T cells predicted to recognize different HBV-epitopes in each cluster. (E) Z-score normalized mean expressions of known canonical markers to assess CD8+ cell phenotypes (naïve, cytotoxic, costimulatory inhibitory, and effector memory markers) in the three different exhausted cell clusters. Exhausted 3 was predicted to be enriched for HBV-targeting T cells (p = 3e-06, p.adj = 0.001).
https://doi.org/10.1371/journal.pcbi.1008814.g005
test p = 3e-06, Benjamini-Hochberg corrected for multiple testing p.adj = 0.001), but not to any other epitope-specific clonotypes (Fig 5D and 5E). Enrichment of HBV targeting clono- types was not significant with more stringent TCRGP prediction probabilities possibly due to small number of cells. By calculating exhaustion score for each T cell, we found that exhausted 3 cluster was the most exhausted (Mann-Whitney U test against exhausted 2 p = 0.003, against exhausted 1 p = 0.002) and the least cytotoxic cluster (p = 0.02 and p = 1e-05). Further, gene-
    PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 14 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 level analysis showed high expression of TIGIT and HAVCR2 (encoding TIM-3), which have been associated with late-stage exhaustion after long antigen exposure. Upregulated pathways for exhaustion cluster 3 were IL2-STAT5 signaling pathway (exh3 vs exh1 q = 0.022 and exh3 vs exh2 q = 0.000) and myogenesis pathway (q = 0.016 and q = 0.001).
In summary, TCRGP was able to identify a T cell cluster that was enriched with HBV-tar- geting clonotypes, which was the most exhausted and least functional. This could provide a mechanism for loss of immunological control against HBV after chronic antigen stimulation which could aid in pathogenesis towards HCC. The implementation for the analysis and the datasets used are available at https://github.com/janihuuh/tcrgp_manu_hcc.
Discussion
In this paper we have demonstrated that when we have sufficient amount of experimentally produced epitope-specific TCR-sequencing data available for training a classifier, TCRGP can predict with a relatively high accuracy if previously unseen TCRs recognize an epitope. How- ever, the exact number of TCRs required to achieve a certain level of accuracy varies greatly between the different epitopes. This likely reflects the fact that different epitopes can be more selective in choosing their TCR interactions. In other words, TCRs that recognize one epitope can be more diverse than the TCRs that recognize another epitope [10], and if the TCRs are very heterogeneous, it requires more sampling to get a representative sample of these TCRs for the model training. In addition, we have shown that also CDRs other than CDR3β can provide useful information for the classification task, although it depends on the epitope in question which of the CDRs are most important. Although computational methods cannot replace experimental measurements in determining exact epitope-specificities of TCRs, they may be used to complement them for example when analyzing existing unselected TCR repertoire data or to guide experimental designs for ex vivo measurements.
In this work we have provided a comprehensive analysis of current epitope-specificity prediction algorithms on a large set of publicly available epitope-specific TCR sequences. With the currently available epitope-specific TCR sequence data we have been able to come this far, but as more data becomes available with modern high-throughput techniques pre- sented recently [40, 41], new possibilities will rise. With a larger variety of pMHC complexes and TCRs that recognize them, we hope to be able to better consider the cross-reactivity of TCRs, similarities between different epitopes, and the significance of the HLA-types of the MHC proteins presenting the epitopes and perhaps even predict if a TCR can recognize a previously unseen epitope. As the proposed GP formalism has been shown to scale to very large data sets with up to billion data points [42], it should be well suited for the arising challenge.
The previous supervised algorithms developed are presented in the case of epitope-specific data, but we believe that to answer clinically relevant questions we need to address the unse- lected repertoire data which is far more numerous in size and more easily produced. Therefore we presented a novel workflow for analysis of scRNA+TCRαβ data in a clinically relevant question, showing the power of determining the epitope-specifity in silico to reveal underlying transcriptomic heterogeneity of the epitope-specific T cells, which has been previously difficult to perform. As the number of scRNA+TCRαβ and conventional TCRβ sequencing data in clinical settings is increasing [43–50], we expect that models like ours can be applied to a vari- ety of research questions where exhaustive ex vivo pMHC-multimer assays are not feasible. In conclusion, we propose that TCRGP could be useful in the diagnosis and follow-up of infec- tious diseases, in autoimmune disorders and cancer immunotherapy.
        PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 15 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 Materials and methods
Data
T cell receptor sequences. Our experiments focus on TCRs formed by a pair of α- and β- chains, as those are the most common type of TCRs [51]. The CDR3 sequence is formed by V(D)J recombination, but CDR1, CDR2, and CDR2.5 sequences are determined completely by the V-gene and allele [3]. We align the CDR3 sequence by adding a gap at the top of the loop, following the IMGT numbering [11]. Dash et al. [10] provide a table of all V-gene and allele combinations and the corresponding CDR1, CDR2, and CDR2.5 amino acid sequences aligned according to IMGT definitions [11]. Our method can utilize the aligned amino acid sequences of all these CDRs from either one or both of the α- and β-chains of the TCR. Fig 1 shows a few examples of TCR sequences and their alignment.
Datasets. In our experiments, we use a data set collected by Dash et al. [10] (from hereon referred to as Dash data), which contains epitope-specific paired TCRα and TCRβ chains for three epitopes from humans and for seven epitopes from mice, see Table 3 for details.
We also gather a new data set (VDJdb data) from VDJdb (http://https://vdjdb.cdr3.net, downloaded 9th October 2018) [16], which is a database that contains TCR sequences with known antigen specificity. Every entry in VDJdb has been given a confidence score between 0 and 3 (0: critical information missing, 1: medium confidence, 2: high confidence, 3: very high confidence). We constructed our data set so that we selected all epitopes that have at least 50 TCRβ sequences with a confidence score at least 1 and found 22 such epitopes. Twenty of these epitopes are MCH class I restricted and two are MHC class II restricted and have varying HLA-types, see Table 3 for details. VDJdb also contains TCRα sequences, but since these are not in general paired with the corresponding TCRβ sequences, we chose to only experiment with the TCRβ sequences. The VDJdb data and the Dash data have some overlap for TCRs specific to three epitopes: In the VDJdb data 34 (27 unique) of the 413 (242 unique) TCRs
for pp65495-503, 30 (27) of 299 (152) for BMLF1280-288, and 74 (61) of 239 (138) for M158-66 can also be found from the Dash data.
For the training and testing of the models, we also required some background TCRs that we do not expect to recognize the epitopes in our data sets. For this purpose, we randomly sampled the required amount of TCRs from sets of background TCRs constructed by Dash et al. [10]. They report that the human α- and β-chains have been obtained from Howie et al. [52], who have collected blood from two healthy adults. The mouse α-chains they have gath- ered from short read archive (SRA) projects SRP010815 [53], and SRP059581 [54] and mouse β-chains from SRA projects SRP059581 [54], SRP015131 [55], and SRP004475. To create paired α- and β-chains they randomly paired the unpaired α- and β-chains from the reper- toires for the corresponding organism.
Construction of training and test sets. For the evaluation of the methods developed by us and others, we needed to divide our data sets for training and testing. Both of the data sets we use determine the subjects from whom the TCRs in the data have been obtained from. We therefore chose to use leave-one-subject-out (LOSO) cross-validation, where we leave out all TCRs from one subject, train the model with all the other TCRs, test it with the TCRs left out, and repeat this for all subjects. The epitope-specific TCRs are always complemented with an equal number of background TCRs in both training and test sets, except for the experiments presented in Section Discriminating between epitope-specific TCRs, were we use only the VDJdb data. We thought this would be the most realistic procedure for the evaluation, as this is likely how these kinds of models will be applied to new data: A model is trained with some set of TCRs and then predictions would be made for TCRs sequenced from an individual from who we have not seen any TCR sequences beforehand. We have chosen to create separate
                  PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 16 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 models for each epitope, as this approach automatically handles potential cross-reactivity between different epitopes—we only have to select such TCRs for training the model that we can assume to recognize that one epitope or to not recognize that epitope. If we then want to predict TCR’s specificity to one or several epitopes, we can apply one or several models and get separate predictions for each of the epitopes.
In the Dash data the average number of TCRs per fold varied between 6 (for pp65123-131) and 22 (for M45985-993), and in the VDJdb data between 3 (for p2430-40) and 45 (for NS4B214-222). The number of subjects, TCRs and unique TCRs for each epitope can be found from Table 3.
The VDJdb data contains TCR sequences from multiple studies, many of which have used same conventions for naming their subjects. Therefore we used the combination of the PMID of the publication and the subject id as the subject identifier. For two epitopes, p24223-231 and NS31406-1415 there were very few separate subjects, only one and four, respectively. With these epitopes we used 5-fold cross-validation instead of the LOSO cross-validation.
We also evaluated the methods using leave-one-out (LOO) cross-validation with only unique TCR sequences. With LOO, the model is trained with all but one TCR and then tested with the left out TCR, and this procedure is repeated for each TCR. This way, we could utilize the maximal amount of training samples and also evaluate the model performance solely on new TCRs. We considered a TCR to be unique when it consists of a unique combination of CDR3 amino acid sequences and V-genes from both α- and β-chains, when both are available, and from the β-chains, when only TCRβ is utilized.
TCR repertoire diversity. To estimate the diversity of the epitope-specific TCRs for each epitope, we developed a diversity measure following the example of Dash et al. [10]. The Simp- son’s diversity index was then generalized to account for the similarity of TCRs by utilizing the Gaussian kernel function as follows:
  0XN􏰀 1XN jjxi 􏰀 xjjj2!1􏰀 1 B s2exp􏰀 2l2 C
diversity 1⁄4 B i1⁄40 j1⁄4iþ1 C B@ 12 ð N 􏰀 1 Þ N CA
: ð1Þ
   Here σ2 is the kernel variance and l is the lengthscale of the Gaussian kernel used by TCRGP, and xi and xj are feature vectors for the TCRs i, j 2 [1, N]. The kernel variance and lengthscale were set to the average values used for the 22 epitopes in the VDJdb data (σ2 = 5.52, l = 2.50).
TCRGP classifier
Sequence representation. Computational methods require the data to have some presen- tation, that they can utilize. Character sequences with variable lengths often provide some challenges as many methods rely on numerical inputs of fixed sizes. One solution is to com- pare subsequences of same length instead of the complete sequences, which is what for exam- ple Generic String kernel (GSkernel) does [56]. However, by aligning the sequences more approaches become applicable. According to the IMGT definitions [11] CDR3s can be aligned by introducing a gap in the middle of the sequence (i.e. top of the loop). Alignments for CDR1s, CDR2s, and CDR2.5s can be found from http://www.imgt.org. When the sequences are aligned, all the sequences within a CDR class (1, 2, 2.5 or 3) have the same length (see
Fig 1A).
Weobservesequencesa1 a2���aL ofaminoacidsaj 2A1⁄4fA;R;N;...;􏰀 gatalignedposi-
tions j = 1, . . ., L. The alignment guarantees that all sequences have the same possibly padded length L. We encode the amino acids a with global feature vectors φðaÞ 2 RD that associate a
     PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 17 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 D-length real-valued code with each of the 21 amino acids including the gap symbol. The sequences are then encoded as data vectors x by concatenating the L feature vectors into a D � L length column vectors x 1⁄4 ðφða1ÞT;...;φðaLÞTÞT 2 X. We collect a dataset of N sequences intoamatrixX1⁄4ðx1;...;xNÞT 2RN�DL withrowsassequencesandcolumnsasindividual amino acid features in aligned order. Each sequence is associated with a class label yi 2 {0, 1} that indicates whether the sequence was epitope-specific or not. We collect the class labels into an output vector y = (y1, . . ., yN)T 2 {0, 1}N.
We can observe amino acid sequences of the four complementarity determining regions (CDR) {1, 2, 2.5, 3} for both the α- and β-chains from a single TCR. Sequence data for each chain and CDR combination has an individual alignment and sequence length. We denote the data as (Xα,1, Xα,2, Xα,2.5, Xα,3, Xβ,1, Xβ,2, Xβ,2.5, Xβ,3, y).
Substitution matrices such as BLOSUM62 [57] describe the similarity of each amino acid. We modified the BLOSUM62 to include also the gap used in alignments and scaled the matrix values between 0 and 1. The similarity of the gap and all amino acids were set to zero and
the value on the diagonal was set to the smallest value on the diagonal. The resulting matrix
B 2 R21�21 is then positive semidefinite. We apply eigendecomposition B = VSVT, where the column vectors of V encode orthogonal projections of the amino acids on the rows. We use the row vectors of V, indexed by the amino acids a from the modified BLOSUM62, as our descriptions φ(a) = Va,: with a feature representation φ(a)TSφ(b) = [B]ab for any two amino acids a; b 2 A. It is possible to use also different substitution models and feature vectors obtained from different sources, or to even use the so-called one-hot-encoding, but here we relied only on the eigenvectors of the (gap-extended) BLOSUM62.
Gaussian process classification. We use Gaussian process (GP) classification [58] to pre- dict if a TCR recognizes a certain epitope or not. Gaussian processes model Gaussian distribu- tions of non-parametric and non-linear functions. We apply a link function to squash the function values to a range [0, 1] suitable for classification. GPs have a clear advantage of char- acterizing the prediction uncertainty with class probabilities instead of point predictions. GPs naturally model sequences through kernel functions focusing on sequence similarity as the explaining factor for class predictions.
We use a GP function f to predict the latent epitope-specificity score f ðxÞ 2 R of a sequence x. A zero-mean GP prior
f ðxÞ � GPð0; kðx; x0ÞÞ;
defines a distribution over functions f(x) whose mean and covariance are
E1⁄2f ðxÞ� 1⁄4 0 cov1⁄2fðxÞ;fðx0Þ� 1⁄4 kðx;x0Þ;
where k(�, �) is the kernel function. We use the standard squared exponential kernel on the vec- torized feature representation,
!
kðx;x0jyÞ1⁄4s2exp 􏰀 ðx􏰀 x0ÞTðx􏰀 x0Þ ; ð2Þ 2‘2
where l is the length-scale parameter, σ2 is the magnitude parameter and θ = (l, σ2). For any collection of TCR sequences X = (x1, . . ., xN), the function values follow a multivariate normal distribution
pðfÞ 1⁄4 N ðfj0; KXXÞ; ð3Þ PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 18 / 27
     
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 wheref1⁄4ðfðx1Þ;...;fðxNÞÞT 2RN collectsallfunctionpredictionsofthesequences,and KXX 2 RN�N is the sequence similarity matrix with [KXX]ij = k(xi, xj). The key property of Gaussian processes is that they couple all predictions to be dependent. The Gaussian process predicts similar epitope values f(x), f(x0) for sequences x, x0 if they are similar according to the kernel k(x, x0).
The latent function f(x) represents an unbounded real-valued classification score, which we turn into a classification likelihood by the probit link function F : R7!1⁄20; 1�,
1Zf �1�
FðfÞ1⁄4pffiffiffiffiffiffi exp 􏰀 2t2 dt: ð4Þ
2p 􏰀 1
The joint model then decomposes into a factorized Bernoulli likelihood and Gaussian
  prior,
1⁄4
pðy; fÞ 1⁄4 pðyjfÞpðfÞ ð5Þ "#
YN
BerðyijFðfiÞÞ � N ðfj0; KXXÞ; ð6Þ
i1⁄41
where fi is a shorthand for f(xi). The objective of Gaussian process modelling is to infer the posterior distribution p(f|y), which is intractable for many non-Gaussian likelihoods. Addi- tionally inferring the kernel hyper-parameters θ entails computing the marginalized evi- dence
pðy;yÞ 1⁄4 Epðf;yÞ1⁄2pðyjfÞ�; ð7Þ
which is also intractable in general and has a limiting cubic complexity OðN3Þ[58]. We tackle the scalability with sparse Gaussian processes [59] and the intractability with stochas- tic variational inference [32].
Variational inference for low-rank GP approximation. We consider low-rank sparse Gaussian processes by augmenting the system with M inducing landmark pseudo-sequences zj 2Xwithassociated(label)functionvaluesuj 1⁄4fðzjÞ2R.Wecollectallinducingpoints into structures Z = (z1, . . ., zM)T and u = (u1, . . ., uM)T. By conditioning the GP with these val- ues we obtain the augmented Gaussian process joint model
pðy; f; uÞ 1⁄4 pðyjfÞpðfjuÞpðuÞ ð8Þ
pðfjuÞ 1⁄4 NðfjAu;KXX 􏰀 AKZZATÞ ð9Þ
pðuÞ 1⁄4 N ðuj0; KZZ Þ ð10Þ
A1⁄4K K􏰀1; ð11Þ XZ ZZ
where KXX 2 RN�N is the kernel between observed sequences, KXZ is between observed and induced sequences and KZZ is between induced sequences. The matrix A projects the M induc- ing points to the full observation space of N sequences.
    PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 19 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 Next, we define a variational approximation for the inducing points,
qðuÞ 1⁄4 N ðujm; SÞ ð12Þ
Z
qðfÞ 1⁄4
1⁄4NðfjAm;KXX þAðS􏰀 KZZÞATÞ; ð14Þ
where m 2 RM and S ⪰ 0 2 RM�M are free variational parameters to be optimized. It can be shown that minimizing the Kullback-Leibler divergence KL[q(u)||p(u|y)] between the approxi- mative posterior q(u) and the true low-rank posterior p(u|y) is equivalent to maximizing the evidence lower bound (ELBO) [60]
Xn
pðyÞ� EqðfiÞ1⁄2logpðyijfiÞ�􏰀 KL1⁄2qðuÞjjpðuÞ�: ð15Þ
i1⁄41
The log expectation is tractable for Probit likelihoods [61], while the KL term similarly has a closed form for two Gaussian densities.
Due to the small data regime we choose the optimal assignment of selecting Z = X and u = y, which corresponds to the full Gaussian variational approximation of [62], while for larger datasets the inducing landmark points can also be optimised [32]. We then optimize the ELBO (Eq 15) with respect to the variational parameters m and S as well as the kernel hyperpara- meters θ, that is, the lengthscales lcr and weights wcr.
Finally, predictions f� of new test sequences X� � X follow a variational predictive poste- rior
Z
pðf�juÞpðujyÞdu ð16Þ pðf � juÞqðuÞdu ð17Þ
pðfjuÞqðuÞdu ð13Þ
     pðf�jyÞ 1⁄4 Z
�
1⁄4Nðf�jA�m;KX�X� þA�ðS􏰀 KZZÞAT�Þ; ð18Þ
where A� indicates projection from the landmark points Z to the new sequences X� . The pre- dictive distribution is a Gaussian distribution for the latent test values f� , from which the distri- butions of the test labels can be retrieved through the link function. We have implemented our model using GPflow, a Python package for building GP models using TensorFlow [63, 64]. We utilized GPflow’s variational GP model (VGP) and scalable variational GP model (SVGP) [31, 32].
Multiple kernel learning. When a TCR binds to a pMHC, its CDR3s and CDR1s from both α- and β-chains are in direct contact with the peptide most of the time. CDR2s and CDR2.5s can contact the peptide, but they usually only interact with the MHC, see Glanville et al. [9] for an overview of contacts between different CDRs and peptides. Dash et al. [10] took this into account by giving fixed weights for the distances between amino acids within different CDRs, giving more weight to the CDR3. As it varies which CDRs can be in contact with differ- ent peptides, we did not want to determine the importance of these different CDRs before- hand, but instead created separate kernels for each CDR and let our model decide which of them are important. We define the kernel as a convex combination of the base kernels for the
       PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 20 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP four CDR regions r and the two chains c,
XX
kðx;x0Þ 1⁄4
where the weights wcr � 0 are non-negative. While we can utilize all CDRs from both chains, it
is also possible to use any subset of them.
Single-cell RNA+TCRαβ-sequencing data analysis
TCRGP classifiers for HBV-epitopes. Recently, Cheng et al. [39] mapped HBV-reactive T cell populations by exhaustively screening the whole HBV genome with an HLA-class I restricted multiplexed pMHC-tetramer strategy and characterized T cells against four interest- ing HBV-epitopes from two antigens with TCRβ-sequencing (the Cheng data). Utilizing the TCRs specific to HBV-epitopes HBVcore169, HBVcore195, HBVpol282, and HBVpol387 (here core refers to core protein and pol to polymerase protein) from the Cheng data and control sequences from Dash et al. [10] we trained a TCRGP classifier for each epitope. We utilized all epitope-specific TCRs from which we could determine also CDR1β, CDR2β, and CDR2.5β in addition to CDR3β and complemented these epitope-specific TCRs with the same amount of control TCRs. We considered TCRs which were predicted to recognize the epitopes with at least 85% probability as epitope-specific. The amounts of epitope-specific TCRs and AUROC scores obtained from leave-one-subject-out cross-validations for each epitope are shown in Table 4. We used TCRGP and VGP with all epitopes except for HBVpol387, with which we used SVGP with 700 inducing points due to the high number of samples.
Single-cell RNA+TCRαβ data. Zheng et al. [38] published a dataset (the Zheng data) of single-cell RNA and TCRαβ of T cells from HBsAg-positive HCC-patients from blood, non- malignant liver tissue and tumour tissue. The unnormalized expression count data of T cells passing the quality control were fetched from GEO (GSE98638) along with the TCRαβ- sequences inferred from the full-transcript single-cell RNA-sequencing data and inferred phenotypic states as described by Zheng et al. As the TCRβ-sequenced training data for HBV- specific epitopes was HLA-A restricted, we focused our analysis only on T cells capable of pep- tide recognition in HLA-A restricted manner, namely clusters CD8-LEF1, CD8-CX3CR1, CD8-LAYN and CD8-GZMK. The data was log-normalized to 10 000 counts per cell and scaled accordingly with the Seurat 3.0.2. [65] package for R 3.5.2.
Clustering. The highly variable genes (HVGs) were chosen to be the genes showing the highest mean to variance ratio (min expression = 0.5, max expression 3, min variance 0.5) with the FindVariableFeatures-function. The linear dimensionality reduction was calculated with PCA for the scaled expression matrix containing only HVGs. Non-linear dimensionality reduction was performed with UMAP for principal components that had standard deviation &gt;2 using the standard parameters with the RunUMAP-function. To receive a better grouping for the selected cells, we used a graph-based clustering approach implemented in the Seurat tool. To find the shared nearest neighbor graph, the function FindNeighbors was used with the
Table 4. HBV-epitopes for which TCRGP classifiers were trained. The numbers of epitope-specific TCRs and sub- jects, and mean AUROC scores from leave-one-subject-out cross-validations are shown.
 r2f1;2;2:5;3g c2fa;bg
wcrkcrðx;x0;ycrÞ;
ð19Þ
        Samples
 Subjects
   699
  9
   588
  12
   459
   12
   1348
  12
 Epitope
HBVcore169
HBVcore195
HBVpol282
HBVpol387 https://doi.org/10.1371/journal.pcbi.1008814.t004
AUROC
0.756 0.847 0.880 0.760
            PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021
21 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 same amount of PCs as with UMAP. To determine optimal clustering, FindClusters-functions was used with several parameter values for the resolution parameter, ranging from [0.1, 3]. The optimal clustering was decided by agreement of grouping in the UMAP-embedding and the labels from clustering by visual interpretation.
The cytotoxic and exhaustion signatures for the clusters were calculated as cell-wise mean expression of cytotoxic (NKG7, CCL4, CST7, PRF1, GZMA, GZMB, IFNG, CCL3) and exhaus- tion genes (CTLA4, PDCD1, HAVCR2, TIGIT, LAG3). The difference between the signatures was assessed with Mann-Whitney U test for individual cells in clusters. Gene Set Enrichment Analysis (GSEA) between the clusters was performed on genes that were detected at least in 0.1% of the cells and had at least log fold-change of 0.01 between the tested cells. The gene list was ordered based on the fold-change. Overlap with HALLMARK-category was assessed and the False Discovery Rate (FDR) calculated while the number of permutations was 1000.
Enrichment analysis. The one-sided Fisher’s test for enrichment of epitope-specific T cells to different phenotypes was calculated independently for individual and pooled patients, epitopes and tissues. The obtained P-values were adjusted with Benjamini-Hochberg proce- dure for false-discovery.
Supporting information
S1 Fig. Mean AUROC scores for the Dash data using leave-one-subject-out cross-valida- tion. TCRGP models (left column) and TCRdist models (right column) were trained using either only CDR3 or all CDRs from TCRα, TCRβ, or both.
(PDF)
S2 Fig. Distribution estimates and epitope-by-epitope method comparisons of mean AUROC scores for the Dash data using leave-one-out cross-validation with unique TCRs. (A) The blue parts of the violin plots illustrate the AUROC scores of predictions made by TCRGP for all the epitopes and the orange parts illustrate the AUROC scores obtained with TCRdist. Each point within a violin plot presents the mean AUROC score obtained for one epitope. The used chains (α and/or β) and CDRs (three or all) are indicated below each panel. (B) Comparison of AUROC scores obtained with TCRGP and TCRdist using only CDR3 from TCRαβ, TCRβ, or TCRα for each epitope separately. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using CDR3 from α- and β-chains (blue line).(C) Comparison of AUROC scores obtained with TCRGP and TCRdist using all CDRs from TCRαβ, TCRβ, or TCRα for each epitope separately. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using all CDRs from α- and β-chains (blue line).
(PDF)
S3 Fig. Mean AUROC scores for the Dash data using leave-one-out cross-validation with unique TCRs. TCRGP models (left column) and TCRdist models (right column) we trained using either TCRα, TCRβ, or both and either with only CDR3 or all CDRs.
(PDF)
S4 Fig. Mean AUROC scores for the VDJdb data using leave-one-subject-out cross-valida- tion. TCRGP models and TCRdist models (the first two columns) were trained using TCRβ with either only CDR3 or all CDRs. RF models and DeepTCR models (the last two columns) were trained using the CDR3β and the Vβ-gene, from which the other CDRs can be derived from.
(PDF)
 PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 22 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 S5 Fig. Distribution estimates and epitope-by-epitope method comparisons of mean AUROC scores for the VDJdb data using leave-one-out cross-validation with unique TCRs. (A) One violin plot presents the mean AUROC scores obtained with one method for all epitopes in the VDJdb data. Below each violin plot there is the name of the method used and in the brackets which CDRs have been used (3 for CDR3, all for CDR1, CDR2, CDR2.5, and CDR3). Each point within a violin plot presents the mean AUROC score obtained for one epi- tope. � RF (Random Forest TCR-classifier of De Neuter et al. [19]) could not produce predic- tions for all epitopes. The AUROC scores for RF have been obtained without these epitopes (8 and 4 epitopes were left out when only CDR3β was used and when also Vβ-gene was used, respectively, see S6 Fig). (B) Comparison of AUROC scores obtained with the different meth- ods for each epitope separately. The epitopes have been arranged in increasing order of AUROC scores obtained by TCRGP using all CDRβs (orange line).
(PDF)
S6 Fig. Mean AUROC scores for the VDJdb data using leave-one-out cross-validation.
Only unique TCRs have been utilized. TCRGP models and TCRdist models (the first two col- umns) were trained using TCRβ with either only CDR3 or all CDRs. RF models and DeepTCR models (the last two columns) were trained using the CDR3β and the Vβ-gene, from which the other CDRs can be derived from.
(PDF)
S7 Fig. Learning curves. With each epitope from the VDJdb dataset, TCRGP models were trained using different numbers of unique epitope-specific TCRs, always complemented with the same number of control TCRs. For each point of the learning curve the model was trained with 100 random samples of the TCRs, using either CDR1, CDR2, CDR2.5, and CDR3 (blue curves), or only CDR3 (orange curves). The darker lines show the mean of the predictions and the shaded areas +/- the standard deviation for the 100 folds.
(PDF)
S8 Fig. Diversity of TCRs within and between subjects. (A) Equation for computing
the diversity between two subjects s and t. The diversity between multiple subjects can
be computed similarly. (B) Scatter plot of diversities. Vertical axis shows diversity of epi- tope-specific TCRs between subjects and horizontal axis shows average diversity of TCRs within each subject. Diversities are computed for 21 epitopes from the VDJdb data (this data contained TCRs from only one subject for HIV-1 epitope p223-231, which was therefore left out from this figure). Diversities between subjects seem to be slightly larger than within subjects (on average 4.3% larger, Pearson correlation 0.82). (C) Pairwise diversities between subjects, where diversities within subjects are shown on the diagonal. Subjects are sorted
by increasing diversity and only subjects with at least two TCRs have been included in this figure.
(PDF)
S9 Fig. Comparisons with different control data. Comparisons with TCRGP with different control data using either all CDRβs (left column) or only CDR3βs (right column). 1: mean AUROC scores from leave-one-out cross validation when equal number of epitope-specific and control TCRs are used in training and testing (same as in S6 Fig). 2: Mean AUROC scores from stratified 200-fold cross validation when TCRs specific to other epitopes in the VDJdb data have been used as control data. 3: Otherwise same as 2, but only TCRs specific to epitopes restricted by MHC of type HLA-A�02 have been used for training and testing.
(PDF)
    PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 23 / 27
 
PLOS COMPUTATIONAL BIOLOGY Predicting recognition between T cell receptors and epitopes with TCRGP
 S1 File. Result tables summarizing the prediction accuracy results presented in Figs 2B, 3B and 4A, and S1–S6 and S9 Figs, and a list of studies from which the VDJdb data consists of.
(XLSX)
S2 File. Benjamini-Hochberg corrected P-values from Wilocoxon signed rank tests for method comparisons with the Dash data and the VDJdb data.
(XLSX)
S3 File. Detailed information of the MHCs for different epitopes in the VDJdb data.
(XLSX)
Acknowledgments
We would like to acknowledge the computational resources provided by the Aalto Science-IT.
Author Contributions
Conceptualization: Emmi Jokinen, Jani Huuhtanen, Satu Mustjoki, Markus Heinonen, Harri La ̈hdesma ̈ki.
Data curation: Emmi Jokinen, Jani Huuhtanen.
Formal analysis: Emmi Jokinen, Jani Huuhtanen.
Funding acquisition: Satu Mustjoki, Harri La ̈hdesma ̈ki.
Investigation: Emmi Jokinen, Jani Huuhtanen, Markus Heinonen, Harri La ̈hdesma ̈ki.
Methodology: Emmi Jokinen, Jani Huuhtanen, Markus Heinonen, Harri La ̈hdesma ̈ki.
Project administration: Harri La ̈hdesma ̈ki.
Resources: Satu Mustjoki, Harri La ̈hdesma ̈ki.
Software: Emmi Jokinen, Jani Huuhtanen.
Supervision: Satu Mustjoki, Markus Heinonen, Harri La ̈hdesma ̈ki.
Validation: Emmi Jokinen, Jani Huuhtanen.
Visualization: Emmi Jokinen, Jani Huuhtanen.
Writing – original draft: Emmi Jokinen, Jani Huuhtanen, Markus Heinonen, Harri La ̈hdesma ̈ki.
Writing – review &amp; editing: Emmi Jokinen, Jani Huuhtanen, Satu Mustjoki, Markus Heino- nen, Harri La ̈hdesma ̈ki.
References
1. Davis MM, Bjorkman PJ. A model for T cell receptor and MHC/peptide interaction. In: Mechanisms of
Lymphocyte Activation and Immune Regulation II. New York City: Springer; 1989. p. 13–16.
2. Miles JJ, Silins SL, Brooks AG, Davis JE, Misko I, Burrows SR. T-cell grit: large clonal expansions of virus-specific CD8+ T cells can dominate in the peripheral circulation for at least 18 years. Blood. 2005; 106(13):4412–4413. https://doi.org/10.1182/blood-2005-06-2261
3. Bassing CH, Swat W, Alt FW. The mechanism and regulation of chromosomal V(D)J recombination. Cell. 2002; 109(2):S45–S55. https://doi.org/10.1016/S0092-8674(02)00675-X
4. Cabaniols JP, Fazilleau N, Casrouge A, Kourilsky P, Kanellopoulos JM. Most α/β T cell receptor diver- sity is due to terminal deoxynucleotidyl transferase. Journal of Experimental Medicine. 2001; 194 (9):1385–1390. https://doi.org/10.1084/jem.194.9.1385
          PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 24 / 27
 
PLOS COMPUTATIONAL BIOLOGY
Predicting recognition between T cell receptors and epitopes with TCRGP
 5.
6.
7. 8. 9.
10.
11. 12.
13.
14. 15. 16.
17.
18.
19.
20.
21. 22. 23.
24. 25.
26.
Arstila TP, Casrouge A, Baron V, Even J, Kanellopoulos J, Kourilsky P. A direct estimate of the human αβ T cell receptor diversity. Science. 1999; 286(5441):958–961. https://doi.org/10.1126/science.286. 5441.958
Robins HS, Campregher PV, Srivastava SK, Wacher A, Turtle CJ, Kahsai O, et al. Comprehensive assessment of T-cell receptor β-chain diversity in αβ T cells. Blood. 2009; 114(19):4099–4107. https:// doi.org/10.1182/blood-2009-04-217604 PMID: 19706884
Lefranc MP, Lefranc G. The T cell receptor FactsBook. Cambridge, Massachusetts: Academic Press; 2001.
Rudolph MG, Stanfield RL, Wilson IA. How TCRs bind MHCs, peptides, and coreceptors. Annu Rev Immunol. 2006; 24:419–466. https://doi.org/10.1146/annurev.immunol.23.021704.115658
Glanville J, Huang H, Nau A, Hatton O, Wagar LE, Rubelt F, et al. Identifying specificity groups in the T cell receptor repertoire. Nature. 2017; 547(7661):94. https://doi.org/10.1038/nature22976 PMID: 28636589
Dash P, Fiore-Gartland AJ, Hertz T, Wang GC, Sharma S, Souquette A, et al. Quantifiable predictive features define epitope-specific T cell receptor repertoires. Nature. 2017; 547(7661):89. https://doi.org/ 10.1038/nature22383 PMID: 28636592
Lefranc M. The IMGT unique numbering for immunoglobulins, T-cell receptors, and Ig-like domains. Immunologist. 1999; 7(4):132–136.
Freeman JD, Warren RL, Webb JR, Nelson BH, Holt RA. Profiling the T-cell receptor beta-chain reper- toire by massively parallel sequencing. Genome research. 2009; 19(10):1817–1824. https://doi.org/10. 1101/gr.092924.109
Dash P, McClaren JL, Oguin TH, Rothwell W, Todd B, Morris MY, et al. Paired analysis of TCRα and TCRβ chains at the single-cell level in mice. The Journal of clinical investigation. 2011; 121(1):288–295. https://doi.org/10.1172/JCI44752 PMID: 21135507
Han A, Glanville J, Hansmann L, Davis MM. Linking T-cell receptor sequence to functional phenotype at the single-cell level. Nature biotechnology. 2014; 32(7):684–692. https://doi.org/10.1038/nbt.2938
Mo ̈ sch A, Raffegerst S, Weis M, Schendel DJ, Frishman D. Machine learning for cancer immunothera- pies based on epitope recognition by T cell receptors. Frontiers in Genetics. 2019; 10.
Shugay M, Bagaev DV, Zvyagin IV, Vroomans RM, Crawford JC, Dolton G, et al. VDJdb: a curated database of T-cell receptor sequences with known antigen specificity. Nucleic acids research. 2017; 46 (D1):D419–D427. https://doi.org/10.1093/nar/gkx760
Vita R, Mahajan S, Overton JA, Dhanda SK, Martini S, Cantrell JR, et al. The immune epitope database (IEDB): 2018 update. Nucleic acids research. 2018; 47(D1):D339–D343. https://doi.org/10.1093/nar/ gky1006
Tickotsky N, Sagiv T, Prilusky J, Shifrut E, Friedman N. McPAS-TCR: a manually curated catalogue of pathology-associated T cell receptor sequences. Bioinformatics. 2017; 33(18):2924–2929. https://doi. org/10.1093/bioinformatics/btx286
De Neuter N, Bittremieux W, Beirnaert C, Cuypers B, Mrzic A, Moris P, et al. On the feasibility of mining CD8+ T cell receptor patterns underlying immunogenic peptide recognition. Immunogenetics. 2018; 70 (3):159–168. https://doi.org/10.1007/s00251-017-1023-5 PMID: 28779185
Jurtz VI, Jessen LE, Bentzen AK, Jespersen MC, Mahajan S, Vita R, et al. NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks. bioRxiv. 2018; p. 433706.
Sidhom JW, Larman HB, Pardoll DM, Baras AS. DeepTCR: a deep learning framework for revealing structural concepts within TCR Repertoire. bioRxiv. 2018; p. 464107.
Springer I, Besser H, Tickotsky-Moskovitz N, Dvorkin S, Louzoun Y. Prediction of specific TCR-peptide binding from large dictionaries of TCR-peptide pairs. bioRxiv. 2019; p. 650861.
Zhang H, Liu L, Zhang J, Chen J, Ye J, Shukla S, et al. Investigation of antigen-specific T-cell receptor clusters in human cancers. Clinical Cancer Research. 2020; 26(6):1359–1371. https://doi.org/10.1158/ 1078-0432.CCR-19-3249 PMID: 31831563
Thakkar N, Bailey-Kellogg C. Balancing sensitivity and specificity in distinguishing TCR groups by CDR sequence similarity. BMC bioinformatics. 2019; 20(1):241. https://doi.org/10.1186/s12859-019-2864-8
Wong WK, Marks C, Leem J, Lewis AP, Shi J, Deane CM. TCRBuilder: multi-state T-cell receptor struc- ture prediction. Bioinformatics. 2020; 36(11):3580–3581. https://doi.org/10.1093/bioinformatics/ btaa194
Cheng L, Ramchandran S, Vatanen T, Lietze ́n N, Lahesmaa R, Vehtari A, et al. An additive Gaussian process regression model for interpretable non-parametric analysis of longitudinal data. Nature commu- nications. 2019; 10(1):1798. https://doi.org/10.1038/s41467-019-09785-8 PMID: 30996266
                                PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 25 / 27
 
PLOS COMPUTATIONAL BIOLOGY
Predicting recognition between T cell receptors and epitopes with TCRGP
 27.
28.
29.
30.
31. 32. 33.
34.
35. 36.
37.
38.
39.
40.
41.
42. 43.
44.
45.
46.
47.
Jokinen E, Heinonen M, La ̈ hdesma ̈ ki H. mGPfusion: predicting protein stability changes with Gaussian process kernel learning and data fusion. Bioinformatics. 2018; 34(13):i274–i283. https://doi.org/10. 1093/bioinformatics/bty238
Romero PA, Krause A, Arnold FH. Navigating the protein fitness landscape with Gaussian processes. Proceedings of the National Academy of Sciences. 2013; 110(3):E193–E201. https://doi.org/10.1073/ pnas.1215251110
Clifton L, Clifton DA, Pimentel MA, Watkinson PJ, Tarassenko L. Gaussian processes for personalized e-health monitoring with wearable sensors. IEEE Transactions on Biomedical Engineering. 2012; 60 (1):193–197. https://doi.org/10.1109/TBME.2012.2208459
Chu W, Ghahramani Z, Falciani F, Wild DL. Biomarker discovery in microarray gene expression data with Gaussian processes. Bioinformatics. 2005; 21(16):3385–3393. https://doi.org/10.1093/ bioinformatics/bti526
Opper M, Archambeau C. The variational Gaussian approximation revisited. Neural computation. 2009; 21(3):786–792. https://doi.org/10.1162/neco.2008.08-07-592
Hensman J, Matthews AGdG, Ghahramani Z. Scalable variational Gaussian process classification. In: Artificial Intelligence and Statistics; 2015.
Gielis S, Moris P, Bittremieux W, De Neuter N, Ogunjimi B, Laukens K, et al. Detection of Enriched T Cell Epitope Specificity in Full T Cell Receptor Sequence Repertoires. Frontiers in immunology. 2019; 10:2820. https://doi.org/10.3389/fimmu.2019.02820 PMID: 31849987
Shomuradova AS, Vagida MS, Sheetikov SA, Zornikova KV, Kiryukhin D, Titov A, et al. SARS-CoV-2 epitopes are recognized by a public and diverse repertoire of human T-cell receptors. medRxiv. 2020. https://doi.org/10.1016/j.immuni.2020.11.004 PMID: 33326767
Bosetti C, Turati F, La Vecchia C. Hepatocellular carcinoma epidemiology. Best Pract Res Clin Gastro- enterol. 2014; 28(5):753–770. https://doi.org/10.1016/j.bpg.2014.08.007
Hassan MM, Hwang LY, Hatten CJ, Swaim M, Li D, Abbruzzese JL, et al. Risk factors for hepatocellular carcinoma: synergism of alcohol with viral hepatitis and diabetes mellitus. Hepatology. 2002; 36 (5):1206–1213. https://doi.org/10.1053/jhep.2002.36780 PMID: 12395331
Wang Y, Wu MC, Sham JST, Tai LS, Fang Y, Wu WQ, et al. Different expression of hepatitis B surface antigen between hepatocellular carcinoma and its surrounding liver tissue, studied using a tissue micro- array. J Pathol. 2002; 197(5):610–616. https://doi.org/10.1002/path.1150 PMID: 12210080
Zheng C, Zheng L, Yoo JK, Guo H, Zhang Y, Guo X, et al. Landscape of infiltrating T cells in liver cancer revealed by single-cell sequencing. Cell. 2017; 169(7):1342–1356. https://doi.org/10.1016/j.cell.2017. 05.035 PMID: 28622514
Cheng Y, Zhu YO, Becht E, Aw P, Chen J, Poidinger M, et al. Multifactorial heterogeneity of virus-spe- cific T cells and association with the progression of human chronic hepatitis B infection. Science immu- nology. 2019; 4(32):eaau6905. https://doi.org/10.1126/sciimmunol.aau6905 PMID: 30737354
Bentzen AK, Such L, Jensen KK, Marquard AM, Jessen LE, Miller NJ, et al. T cell receptor fingerprinting enables in-depth characterization of the interactions governing recognition of peptide–MHC complexes. Nature biotechnology. 2018; 36(12):1191. https://doi.org/10.1038/nbt.4303 PMID: 30451992
Zhang SQ, Ma KY, Schonnesen AA, Zhang M, He C, Sun E, et al. High-throughput determination of the antigen specificities of T cell receptors in single cells. Nature biotechnology. 2018; 36(12):1156. https:// doi.org/10.1038/nbt.4282 PMID: 30418433
Salimbeni H, Deisenroth M. Doubly stochastic variational inference for deep Gaussian processes. In: Advances in Neural Information Processing Systems; 2017. p. 4588–4599.
Li H, van der Leun AM, Yofe I, Lubling Y, Gelbard-Solodkin D, van Akkooi AC, et al. Dysfunctional CD8 T cells form a proliferative, dynamically regulated compartment within human melanoma. Cell. 2019; 176(4):775–789. https://doi.org/10.1016/j.cell.2018.11.043 PMID: 30595452
Azizi E, Carr AJ, Plitas G, Cornish AE, Konopacki C, Prabhakaran S, et al. Single-cell map of diverse immune phenotypes in the breast tumor microenvironment. Cell. 2018; 174(5):1293–1308. https://doi. org/10.1016/j.cell.2018.05.060 PMID: 29961579
Guo X, Zhang Y, Zheng L, Zheng C, Song J, Zhang Q, et al. Global characterization of T cells in non- small-cell lung cancer by single-cell sequencing. Nature Medicine. 2018; 24(7):978. https://doi.org/10. 1038/s41591-018-0045-3 PMID: 29942094
Zhang L, Yu X, Zheng L, Zhang Y, Li Y, Fang Q, et al. Lineage tracking reveals dynamic relationships of T cells in colorectal cancer. Nature. 2018; 564(7735):268. https://doi.org/10.1038/s41586-018-0694-x PMID: 30479382
Sade-Feldman M, Yizhak K, Bjorgaard SL, Ray JP, de Boer CG, Jenkins RW, et al. Defining T cell states associated with response to checkpoint immunotherapy in melanoma. Cell. 2018; 175(4):998– 1013. https://doi.org/10.1016/j.cell.2018.10.038 PMID: 30388456
                                        PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 26 / 27
 
PLOS COMPUTATIONAL BIOLOGY
Predicting recognition between T cell receptors and epitopes with TCRGP
 48.
49.
50.
51. 52.
53.
54.
55.
56.
57. 58. 59. 60.
61. 62. 63. 64. 65.
Emerson RO, DeWitt WS, Vignali M, Gravley J, Hu JK, Osborne EJ, et al. Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T cell repertoire. Nature genetics. 2017; 49(5):659. https://doi.org/10.1038/ng.3822 PMID: 28369038
Savola P, Kelkka T, Rajala H, Kuuliala A, Kuuliala K, Eldfors S, et al. Somatic mutations in clonally expanded cytotoxic T lymphocytes in patients with newly diagnosed rheumatoid arthritis. Nature Com- munications. 2017; 8:15869. https://doi.org/10.1038/ncomms15869 PMID: 28635960
Tumeh PC, Harview CL, Yearley JH, Shintaku IP, Taylor EJ, Robert L, et al. PD-1 blockade induces responses by inhibiting adaptive immune resistance. Nature. 2014; 515(7528):568. https://doi.org/10. 1038/nature13954 PMID: 25428505
Allison TJ, Winter CC, Fournie ́ JJ, Bonneville M, Garboczi DN. Structure of a human γδ T-cell antigen receptor. Nature. 2001; 411(6839):820. https://doi.org/10.1038/35081115
Howie B, Sherwood AM, Berkebile AD, Berka J, Emerson RO, Williamson DW, et al. High-throughput pairing of T cell receptor α and β sequences. Science translational medicine. 2015; 7(301):301ra131– 301ra131. https://doi.org/10.1126/scitranslmed.aac5624 PMID: 26290413
Genolet R, Stevenson BJ, Farinelli L, Østerås M, Luescher IF. Highly diverse TCRα chain repertoire of pre-immune CD8+ T cells reveals new insights in gene recombination. The EMBO journal. 2012; 31 (7):1666–1678. https://doi.org/10.1038/emboj.2012.48
Ruggiero E, Nicolay JP, Fronza R, Arens A, Paruzynski A, Nowrouzi A, et al. High-resolution analysis of the human T-cell receptor repertoire. Nature communications. 2015; 6:8081. https://doi.org/10.1038/ ncomms9081 PMID: 26324409
Ndifon W, Gal H, Shifrut E, Aharoni R, Yissachar N, Waysbort N, et al. Chromatin conformation governs T-cell receptor Jβ gene segment usage. Proceedings of the National Academy of Sciences. 2012; 109 (39):15865–15870. https://doi.org/10.1073/pnas.1203916109 PMID: 22984176
Giguere S, Marchand M, Laviolette F, Drouin A, Corbeil J. Learning a peptide-protein binding affinity predictor with kernel ridge regression. BMC bioinformatics. 2013; 14(1):82. https://doi.org/10.1186/ 1471-2105-14-82
Henikoff S, Henikoff JG. Amino acid substitution matrices from protein blocks. Proceedings of the National Academy of Sciences. 1992; 89(22):10915–10919. https://doi.org/10.1073/pnas.89.22.10915
Rasmussen CE, Williams CKI. Gaussian processes for machine learning. Cambridge, Massachusetts: The MIT Press; 2006.
Snelson E, Ghahramani Z. Sparse Gaussian processes using pseudo-inputs. In: Advances in neural information processing systems; 2006. p. 1257–1264.
Blei DM, Kucukelbir A, McAuliffe JD. Variational inference: A review for statisticians. Journal of the American Statistical Association. 2017; 112(518):859–877. https://doi.org/10.1080/01621459.2017. 1285773
Hegde P, Heinonen M, Kaski S. Variational zero-inflated Gaussian processes with sparse kernels. In: Uncertainty in Artificial Intelligence; 2018.
Nickisch H, Rasmussen C. Approximations for binary Gaussian process classification. Journal of Machine Learning Research. 2008; 9:2035–2078.
Matthews AGdG, van der Wilk M, Nickson T, Fujii K, Boukouvalas A, Le‘on-Villagr‘a P, et al. GPflow: A Gaussian process library using TensorFlow. Journal of Machine Learning Research. 2017; 18(40):1–6.
Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems; 2015. Available from: https://www.tensorflow.org/.
Butler A, Hoffman P, Smibert P, Papalexi E, Satija R. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nature biotechnology. 2018; 36(5):411. https://doi.org/ 10.1038/nbt.4096
                         PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1008814 March 25, 2021 27 / 27
 </Text>
        </Document>
        <Document ID="061EEA07-833D-4E40-BAFD-A51E8784031D">
            <Title>Results and Discussion</Title>
        </Document>
        <Document ID="309B27F9-950F-4526-AA34-912B23342F53">
            <Title>Evaluatiing the model</Title>
            <Text>The final fine-tuned model was evaluated using two external test datasets containing COVID-19 epitope(YLQPRTFLL)-specific TCR CDR3beta sequence data. The F1 score , the harmonic mean of precision and recall, was used to quantify the prediction performance, where an F1 sore reaches its best value at 1 and worst value at 0. The F1 score is defined as:

#

where TP, FP, and FN are the numbers of true positives, false positives, and false negatives, respectively. 


</Text>
        </Document>
        <Document ID="3BF3D60B-CD52-460B-A6EB-84A48DF20D50">
            <Title>새로운 폴더</Title>
        </Document>
        <Document ID="A13CDD87-D5DE-4999-8044-B83E1DE4111F">
            <Title>Bioinformatics에서의 사례, TAPE, BERTMHC</Title>
            <Text>최근에 자기주도 학습 접근방식은 단백질 서열 패턴에 대한 학습모델에도 적용되고 있다[{Rao:2019uu}, {Heinzinger:2019eu}, {Nambiar:2020}]. Tasks Assessing Protein Embeddings(TAPE) 모델은 Pfam 데이터베이스[{Rocklin et al., 2017}]에서의 3,100만개의 unlabeled 단백질 서열을 사용하여 AA contact prediction loss와 remote homology detection loss와 같은 protein-specific loss로 self-supervised하게 사전 학습된 모델이다. TAPE 사전학습 모델은 secondary structure prediction, AA contact prediction, remote homology detection, fluorescence landscape prediction, protein stability landscape prediction 등의 supervised downstream task들에서 유용하다는 것을 증명하였다.
BERTMHC[{Jun Cheng et al., 2020}]는 펩타이드-MHC class II binding과 presentation 예측을 위해 사전학습된 TAPE 모델을 미세튜닝하여 생성된 BERT-based 모델이다. It has demonstrated that models employing self-supervised pretraining from large corpora of protein sequences can archive better performance in both peptide-MHC-II binding and presentation prediction tasks in case where training data is limited.
</Text>
        </Document>
        <Document ID="22D1321D-0D74-4189-83E8-87583C00EC81">
            <Title>Abelin2017</Title>
            <Text>MS-identified peptides eluted from MHC class I
 - {Abelin:2017cn}: Abelin, J. G. et al. Mass Spectrometry Profiling of HLA-Associated Peptidomes in Mono-allelic Cells Enables More Accurate Epitope Prediction. Immunity 46, 315–326 (2017).

 - MS identified HLA-bound peptides; presented on the cell
 - HLA-bound peptides can be directly identified via immunopurification and LC-MS/MS. We processed class I HLA-deficient cell lines (30 million–90 million B721.221-derived cells), each stably transduced to express one of 16 different class I HLA alleles
 - Data: /Users/hym/projects/nplm/.data/mhcflurry/curated_training_data.with_mass_spec.csv</Text>
        </Document>
        <Document ID="06054169-6534-4549-B459-CF3477B02344">
            <Title>Fine-tuning and evaluating the model</Title>
            <Text>선행학습된 언어모델은 MS-identified EL 데이터셋을 사용하여 fine-tuning 되었다. MS-identified EL 데이타는 positive-high, positive, positive-intermediate, positive-low, and negative로 label 된 정성적 affinity 데이터이므로 pre-trained 언어모델의 last output layer for binding affinity prediction를 4개의 label에 대한 one-hot encoding layer로 replace  한 후 fine-tuning을 수행하였다.
Decoy(negative) 데이터는 임의로 생성하였다(?). NetMHCpan, MHCflurry 등에서 random으로 negative 데이터를 생성했던 방법대로 하였다?

Training for fine-tuning the pre-trained model was carried out in standard 5-fold cross-validation. The training dataset were split into 5 equal sized subset in 5-fold cross-validation. For each CV round, a single subset was retained as the validation data for testing current model, and the remaining subsets were used for training current model. In a single cross-validation round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs [{Prechelt:2012ct}]. Training proceeds with the Adam optimizer[{Kingma:2014us}] using a minibatch size of 128.
</Text>
        </Document>
        <Document ID="41A49D52-FD76-4156-8552-441B274BADED">
            <Title>Finetuning results</Title>
            <Text>As shown in Figure 3, the final validation accuracies were 0.784 and 0.944 in two fine-tuning rounds, respectively. In first fine-tuning round, the validation accuracy was relatively low and the difference between training and validation scores was larger due to more general training dataset and more trainable encoding layers. On the other hand, in the second fine-tuning round, due to more specific training dataset and less trainable encoding layers, the validation accuracy was significantly high, and the difference between the training and validation scores was reduced. This fine-tuning strategy of the pre-trained model could allow us to generate a final model with high predictive performance for a specific task, while avoiding model overfitting.</Text>
        </Document>
    </Documents>
</SearchIndexes>