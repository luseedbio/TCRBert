{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue109;\red0\green0\blue233;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c50196;\cssrgb\c0\c0\c93333;
}
\deftab720
\pard\pardeftab720\sl340\sa240\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 \expnd0\expndtw0\kerning0
We will omit an exhaustive background description of the model architecture and refer readers to \cf3 Vaswani et al. \cf2 (\cf3 2017\cf2 )[\{Vaswani:2017ul\}] as well as excellent guides such as \'93The Annotated Transformer.\'94[{\field{\*\fldinst{HYPERLINK "https://nlp.seas.harvard.edu/2018/04/03/attention.html"}}{\fldrslt 
\fs24 \cf4 \ul \ulc4 https://nlp.seas.harvard.edu/2018/04/03/attention.html}}
\fs24 \cf4 \ul \ulc4 ]\cf2 \ulnone \

\fs29\fsmilli14667 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.
\fs21\fsmilli10667 \cf3 \up10  
\fs29\fsmilli14667 \cf2 \up0 We primarily report results on two model sizes: BERT
\fs21\fsmilli10667 \dn6 BASE 
\fs29\fsmilli14667 \up0 (L=12, H=768, A=12, Total Param- eters=110M) and BERT
\fs21\fsmilli10667 \dn6 LARGE 
\fs29\fsmilli14667 \up0 (L=24, H=1024, A=16, Total Parameters=340M). \
BERT
\fs21\fsmilli10667 \dn6 BASE 
\fs29\fsmilli14667 \up0 was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.
\fs21\fsmilli10667 \cf3 \up10 4 
\fs24 \cf2 \up0 \
\
}