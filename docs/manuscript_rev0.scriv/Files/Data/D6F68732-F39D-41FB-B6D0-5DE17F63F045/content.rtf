{\rtf1\ansi\ansicpg949\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset129 AppleMyungjo;\f1\froman\fcharset0 Palatino-Roman;\f2\froman\fcharset0 Palatino-Italic;
}
{\colortbl;\red255\green255\blue255;\red59\green52\blue26;\red170\green170\blue170;}
{\*\expandedcolortbl;;\csgenericrgb\c23137\c20392\c10196;\csgenericrgb\c66667\c66667\c66667;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 \'bf\'ec\'b8\'ae\'b4\'c2
\f1  
\f0 \'bb\'e7\'c0\'fc
\f1  
\f0 \'c7\'d0\'bd\'c0\'b5\'c8
\f1  protein sequence embedding
\f0 \'c0\'ce
\f1  TAPE
\f0 \'b8\'a6
\f1  
\f0 \'bb\'e7\'bf\'eb\'c7\'cf\'bf\'b4\'b4\'d9
\f1 . TAPE model
\f0 \'c0\'ba
\f1  BERT
\f0 \'b8\'a6
\f1  
\f0 \'b1\'e2\'b9\'dd\'c0\'b8\'b7\'ce
\f1  
\f0 \'c7\'cf\'bf\'a9
\f1  bidirectional masked-token prediction
\f0 \'b0\'fa
\f1  next token prediction task
\f0 \'bf\'cd
\f1  
\f0 \'b4\'f5\'ba\'d2\'be\'ee
\f1  residue
\f0 \'b0\'a3\'c0\'c7
\f1  contact prediction
\f0 \'b0\'fa
\f1  remote homology detection task
\f0 \'b8\'a6
\f1  
\f0 \'bb\'e7\'bf\'eb\'c7\'cf\'bf\'a9
\f1  self-supervised
\f0 \'c7\'cf\'b0\'d4
\f1  
\f0 \'bb\'e7\'c0\'fc\'c7\'d0\'bd\'c0
\f1  
\f0 \'b5\'c7\'be\'fa\'b4\'d9
\f1 . \
TAPE 
\f0 \'bb\'e7\'c0\'fc\'c7\'d0\'bd\'c0
\f1  
\f0 \'b5\'a5\'c0\'cc\'c5\'cd
\f1 ==>\
TAPE model architecture==> \cf2 The model has 12 layers with 12 self-  attention heads (Equation 1) in each layer, which enables the model  to learn long distance interactions. For an input amino acid sequence  z = (z1, z2, ..., zL), the output of the model are L continuous vectors of  dimension 768 corresponding to the input amino acids
\f2\i \cf3 \uc0\u8232 
\f1\i0 \cf0 \
\
We used the pretrained TAPE model based on BERTbase which has 12 encoding layers with 12 self-attention heads in each layer. The TAPE model was trained using unlabeled 31 million protein sequences with two tasks: 
\f2\i next-token prediction
\f1\i0  and bidirectional 
\f2\i masked-token prediction
\f1\i0 .   }