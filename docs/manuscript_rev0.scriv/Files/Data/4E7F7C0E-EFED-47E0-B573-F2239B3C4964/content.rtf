{\rtf1\ansi\ansicpg949\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;\f1\froman\fcharset0 Palatino-Italic;\f2\fnil\fcharset129 AppleMyungjo;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 Figure 2 shows our model architecture. Input amino acid sequences concatenated by epitope and CDR3beta sequences were first encoded into tokens using a tokenizer, where each token is a integer code for a single amino acid. Each token is then embedded into a 768 dimensional vector in the pre-trained TAPE model based on BERTbase which has 12 encoding layers with 12 self-attention heads in each layer.  The TAPE model was pre-trained using unlabeled 31 million protein sequences with two tasks: 
\f1\i next-token prediction
\f0\i0  and bidirectional 
\f1\i masked-token prediction, 
\f2\i0 including further supervised features on contact prediction and remote homology detection
\f0 . The output of the pre-trained TAPE model is the hidden states of the first token. The final classifier consisted 2-layer feed forward network is used to predict either binder or not.}