{\rtf1\ansi\ansicpg949\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;\f1\fnil\fcharset129 AppleMyungjo;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red16\green16\blue16;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c7451\c7843\c7451;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 For pre-training the initial TAPE model, the positive dataset containing epitope-specific TCR CDR3beta sequences was compiled from three data sources on May 2021: Dash et al[\{Dash:2017go\}] containing \cf2 epitope-specific paired TCR\uc0\u945  and TCR\u946  c\cf0 hains for  three epitopes from humans and for seven epitopes from mice, two manually curated databases that contains pathology-associated TCR sequences, such as VDJdb[\{Bagaev:2019hf\}]({\field{\*\fldinst{HYPERLINK "https://vdjdb.cdr3.net"}}{\fldrslt https://vdjdb.cdr3.net}}) and McPAS-TCR[\{Tickotsky:2017bo\}]({\field{\*\fldinst{HYPERLINK "http://friedmanlab.weizmann.ac.il/McPAS-TCR/"}}{\fldrslt http://friedmanlab.weizmann.ac.il/McPAS-TCR/}}).  Every entry in VDJdb has been given a confidence score between 0  and 3 (0: critical information missing, 1: medium confidence, 2: high confidence, 3: very high  confidence). We selected all VDJdb entries \cf2 with a confidence score at least 1. After selecting all epitopes that have at least 20 CDR3beta sequences and removing duplicates with the same \{epitope, CDR3beta\} from the positive dataset,  the dataset contained 12,619 positive data points covering 80 epitopes. To increase the specificity of our model, it was necessary to add more epitope-specific CDR3b sequence data to training dataset as negative examples which were expected to interact between TCRs and epitopes. The background CDR3b sequences were obtained from Howie at al., who have collected blood from two healthy donors. A negative example generated by combining the epitopes from the positive dataset and randomly selected TCR CDR3beta sequences derived from two healthy donors[\{Howie:2015\}]. \cf0 Table 1 summarizes the epitope-specific \cf2 CDR3beta sequence data
\f1  for the pre-training dataset(see Table S1 for the pre-training dataset in detail). 
\f0 \
The initial TAPE model was trained \cf0 while freezing the embedding layer and top two encoding layers, where the weights of the layers were not updated. The freezing layers was extended to top six encoding layers in the next fine-tuning tasks. The training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs[\{\cf3 \expnd0\expndtw0\kerning0
Prechelt:2012 \cf0 \kerning1\expnd0\expndtw0 \}].  We used the Adam optimizer[\{\cf3 \expnd0\expndtw0\kerning0
Kingma D:2014\cf0 \kerning1\expnd0\expndtw0 \}] with learning rate 0.0001 and 128 batch size in all epochs. PyTorch deep learning library({\field{\*\fldinst{HYPERLINK "https://pytorch.org"}}{\fldrslt https://pytorch.org}}) were used for implementing our models.}