{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\froman\fcharset0 Times-Italic;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red253\green183\blue144;\red0\green0\blue109;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgenericrgb\c99400\c71900\c56300;\cssrgb\c0\c0\c50196;
}
\deftab720
\pard\pardeftab720\sl340\sa240\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 \cb3 \expnd0\expndtw0\kerning0
Unlike \cf4 Peters et al. \cf2 (\cf4 2018a\cf2 ) and \cf4 Radford et al. \cf2 (\cf4 2018\cf2 ), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure \cf4 1\cf2 . 
\fs24 \
\pard\pardeftab720\sl340\sa240\partightenfactor0

\fs29\fsmilli14667 \cf2 \cb1 Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right 
\f1\i \cf2 or 
\f0\i0 \cf2 right-to-left,\cf2 \cb3  since bidirectional conditioning would allow each word to indirectly \'93see itself\'94, and the model could trivially predict the target word in a multi-layered context. 
\fs24 \cf2 \cb1 \

\fs29\fsmilli14667 \cf2 In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \'93masked LM\'94 (MLM), although it is often referred to as a 
\f1\i \cf2 Cloze 
\f0\i0 \cf2 task in the literature (\cf4 Taylor\cf2 , \cf4 1953\cf2 ). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (\cf4 Vincent et al.\cf2 , \cf4 2008\cf2 ), we only predict the masked words rather than recon- structing the entire input. 
\fs24 \cf2 \

\fs29\fsmilli14667 \cf2 Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace \'93masked\'94 words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, T
\fs21\fsmilli10667 \cf2 \dn6 i 
\fs29\fsmilli14667 \cf2 \up0 will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix \cf4 C.2\cf2 . 
\fs24 \cf2 \
}