{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset129 AppleSDGothicNeo-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 pMHC-I \'b0\'e1\'c7\'d5\'bf\'a1\'bc\'ad\'c0\'c7 \'be\'c6\'b9\'cc\'b3\'eb\'bb\'ea \'bc\'ad\'bf\'ad(peptide sequence + MHC contact residue sequence)\'b4\'c2 \'be\'c6\'b9\'cc\'b3\'eb\'bb\'ea\'b5\'e9\'bf\'a1 \'c7\'d8\'b4\'e7\'c7\'cf\'b4\'c2 \'b4\'dc\'be\'ee\'b5\'e9\'b7\'ce \'b1\'b8\'bc\'ba\'b5\'c8 \'c7\'cf\'b3\'aa\'c0\'c7  \'a1\'aesentence\'a1\'af\'b7\'ce \'c0\'ce\'c4\'da\'b5\'f9\'b5\'c8\'b4\'d9.  For a given \'a1\'aesentence\'a1\'af, the input embeddings of the BERT-based language model are the sum of the token embeddings and the position embeddings, where the position embeddings are used to inject some information about the relative or absolute position of the tokens in the sequence as used in original BERT model[\{Devlin:2018uk\}]. An input embedding matrix\'c0\'c7 \'bb\'e7\'c0\'cc\'c1\'ee\'b4\'c2 49 x emb_dim\'c0\'cc\'b4\'d9. \'bf\'a9\'b1\'e2\'bc\'ad 49\'b4\'c2 \'b9\'ae\'c0\'e5\'c0\'c7 \'c3\'d6\'b4\'eb\'b1\'e6\'c0\'cc, \'c1\'ef \'c3\'d6\'b4\'eb peptide \'b1\'e6\'c0\'cc(=15) + MHC contact residue\'c0\'c7 \'bc\'ad\'bf\'ad\'b1\'e6\'c0\'cc(=56)[\{Nielsen:2007ga\} + \{Luo:2016iw\}]\'c0\'cc\'b0\'ed emb_dim\'b4\'c2 embedding dimension\'c0\'cc\'b4\'d9.}