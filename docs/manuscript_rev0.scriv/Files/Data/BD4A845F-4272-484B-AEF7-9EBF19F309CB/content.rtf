{\rtf1\ansi\ansicpg949\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;}
{\colortbl;\red255\green255\blue255;\red16\green16\blue16;}
{\*\expandedcolortbl;;\cssrgb\c7451\c7843\c7451;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 We performed fine-tuning the pre-trained model in two rounds, switching the freezing layers. In the first fine-tuning round, the model was trained while freezing the embedding layer and top two encoding layers, where the weights of the layers were not updated. The freezing layers was extended to top six encoding layers in the second fine-tuning round. In each fine-tuning round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs[\{\cf2 \expnd0\expndtw0\kerning0
Prechelt:2012 \cf0 \kerning1\expnd0\expndtw0 \}].  We used the Adam optimizer[\{\cf2 \expnd0\expndtw0\kerning0
Kingma D:2014\cf0 \kerning1\expnd0\expndtw0 \}] with learning rate 0.0001 and 128 batch size in all epochs. PyTorch deep learning library({\field{\*\fldinst{HYPERLINK "https://pytorch.org"}}{\fldrslt https://pytorch.org}}) were used for implementing our model.}