{\rtf1\ansi\ansicpg949\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;\f1\fnil\fcharset129 AppleMyungjo;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 As shown in Figure 3, the final validation accuracies were 0.784 and 0.944 in two fine-tuning rounds, respectively. In first fine-tuning round, the validation accuracy was relatively 
\f1 low
\f0  and the 
\f1 difference between training and validation scores was larger due to more general training dataset and more trainable encoding layers. On the other hand, in the second fine-tuning round, due to more specific training dataset and less trainable encoding layers, the validation accuracy was significantly high, and the difference between the training and validation scores was reduced. This fine-tuning strategy of the pre-trained model could allow us to generate a final model with high predictive performance for a specific task, while avoiding model overfitting.}