{\rtf1\ansi\ansicpg949\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Palatino-Roman;\f1\fnil\fcharset129 AppleMyungjo;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 Figure 1 shows the schematic representation of overall training process of our model. The initial model was cloned from the pretrained TAPE model based on BERT, adding a classification layer at the end. First, the initial TAPE model was pre-trained using epitope-TCR CDR3b sequence data while
\f1  freezing the embedding layer and 
\f0 top two encoding layers. \
Next, two fine-tuned models were generated from the pre-training model using IEDB epitope-specific TCR CDR3b sequence data and COVID-19 epitope-specific TCR CDR3b sequence data, respectively, while freezing the embedding layer and top six encoding layers.\
}