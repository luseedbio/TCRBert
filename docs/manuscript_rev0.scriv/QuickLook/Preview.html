<html>

<head>
<title>manuscript</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<!-- NOTE: margin property = top-right-bottom-left -->
<style type="text/css">

	body {background-color: #e2e2e2}
    
    p.topLevelItemTitle {
    	margin: 30px 0px 5px 0px;
    	font-family: Times, Times New Roman, Palatino, Cochin, Serif;
    	font-size: 30px;
    	}
    	
    p.folderTitle {
    	margin: 30px 0px 5px 0px;
    	font-family: Times, Times New Roman, Palatino, Cochin, Serif;
    	font-size: 18px;
    	}
    
    p.itemTitle {
    	margin: 30px 0px 5px 0px;
    	font-family: Helvetica, Arial, Sans-Serif;
    	font-size: 12px;
    	font-weight: bold;
    	}
    	
    p.itemText {
    	margin: 0px 0px 10px 0px;
    	font-family: Helvetica, Arial, Sans-Serif;
    	font-size: 12px;
    	}
    	
    p.itemTextStart {
    	margin: 30px 0px 10px 0px;
    	font-family: Helvetica, Arial, Sans-Serif;
    	font-size: 12px;
    	}
    
    .page {border: 1px solid #c0c0c0; background: #fff}
    
    hr {
        border: none;
        height: 1px;
        color: #d4d4d4;
        background-color: #d4d4d4;
      	}
      	
    hr.afterTitle {
    	margin-top: -3px;
    }
      	
    hr.afterText {
    	margin-top: 30px;
    }
      	
    ul {
      	list-style-type: none;
      	padding-left: 30px;
      	}
      	
</style>

</head>

<body>

<table border="0" width="100%" cellspacing="3">
<tr>
<td>

<table class="page" width="100%" cellspacing="10" cellpadding="2">

<!-- Top margin -->
<!-- Not needed, because of the padding above titles. -->
<!--<tr><td height="15px"></td></tr>-->

<tr>

<!-- 42 + 30 of list indent = 72 - one inch. -->
<!-- Actually that ends up too much, so we do 25 + 30 = 55px. -->
<td width="25px">

<td valign="top">

<ul>
<li>
<p class="topLevelItemTitle">Introduction</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">적응면역시스템에서의 T-cell의 중요성</p>
<p class="itemText">대부분의 백신 후보 물질 개발은 중화항체를 생성하는 B cell에 촛점을 맞추고 있지만, 기본적인 접근법은 T cell을 기반으로 하고 있다. 혈액에서 순환하는 T 세포는 종종 증상이 나타나기 전에 감염된 세포를 탐지하고 면역 반응을 일으키거나 감염된 세포를 직접 제거하는 적응 면역 체계의 핵심적인 역할을 담담한다. 따라서, 암 면역치료와 효과적인 백신 개발에 있어 T cell 면역 반응을 효과적으로 유도할 수 있는 면역원성을 갖는 에피토프를 식별하는 일은 매우 중요한 일이다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">Beyond neutralizing antibodies produced by B cells, cytotoxic CD8 T cells and the helper CD4 T cells are essential to clear viruses. Circulating in the blood, T cells are leading the first response to any virus in adaptive immune ...</p>
</li>
<li>
<p class="itemTitle">T-cell 면역유도에서 TCR의 역할 및 중요성</p>
<p class="itemText">T-cell은 dimeric surface protein인 T-cell receptor(TCR)을 통하여 세포 표면으로 제시된 MHC-bound epitope을 인지하고 활성화되고 증폭된다.</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">The peptide-MHC (pMHC) complex is then presented to T cells which can recognize the complex via T cell receptor (TCR) proteins, consequently leading to T cell activation and proliferation by clonal expansion [1]. During clonal expansion, a fraction of T cells gain a long-living memory phenotype and therefore a clonal population of T cells with identical TCR rearrangements remain for years ...</p>
</li>
<li>
<p class="itemTitle">Epitope-specific TCR 식별의 어려움</p>
<p class="itemText"></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">TCRs are generated by genomic rearrangement of the germline TCR loci from a large collection of variable(V), diversity(D) and joining(J) gene segments. During T cell development, TCRs are formed by chains through the V(D)J recombination in each locus independently. It is estimated that this rearrangement can result in the range of 1018 different TCRs, which provides enormous diversity of epitope-specific T cell repertoires.</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">T cells undergo non-homologous recombination during T cell development...</p>
</li>
<li>
<p class="itemTitle">Epitope specificity 예측의 필요성</p>
<p class="itemText">이러한 TCR diversity에도 불구하고 최근의 연구들은 특정한 공통 target epitope을 인지하는 TCRs는 종종 공통적인 서열 특징들을 공유한다는 것을 제시하였고 결과적으로 epitope specificity를 데이터 기반으로 예측할 수 있는 가능성을 제시하였다[{Dash:2017go},{Glanville:2017js}].</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">NGS, Database, machine learning methods</p>
<p class="itemText">아주 최근의 공동의 데이터 수집의 노력[{Borrman:2017kl}, {Tickotsky:2017bo}{Mahajan:2018bj},{Bagaev:2019hf}]과 새롭게 떠오르는 high throughput TCR 시퀀싱 기술[{Klinger:2015ew},{Bentzen:2016hy}]의 발달로 인하여 기계학습 방법을 통한 epitope-specific TCR recognition의 모델링을 시작하기에 충분한 데이터셋이 확보되고 있는 상황이다[{Zvyagin:2019ds}]. 현재까지 PSSM[{Glanville:2017js}], 가우시안 프로세스[{Jokinen:2021bn}], random forest[{Gielis:2019dl}], CNN[{Jurtz:2018jt}], joint deep learning model[{Jokinen:2021bn}], 그리고 deep generative model[{Isacchini:2021jo}], NLP-based deep learning...</p>
</li>
<li>
<p class="itemTitle">기존방법의 한계점</p>
<p class="itemText">현재까지 큐레이트된 데이터베이스에서 보고된 TCR-epitope interactions 데이터의 수는 매우 제한적이다: VDJdb[{Bagaev:2019hf}] and McPAS-TCR[{Tickotsky:2017bo}] databases contain ~20k and ~55k epitope-specific TCR sequences<span class="Apple-converted-space">  </span>as of October 2019, respectively. 특히, 최근의 딥러닝 기반의 방법들은 predictive power를 높이기 위해서 비교적 많은 수의 학습데이터를 필요로 하기 때문에 신뢰성 높은 예측 모델을 구축하는데 한계가 있다.</p>
</li>
<li>
<p class="itemTitle">Supervised pretraining in NLP, BERT</p>
<p class="itemText">Self Supervised Representation Learning in NLP</p>
</li>
<li>
<p class="itemTitle">Bioinformatics에서의 사례, TAPE, BERTMHC</p>
<p class="itemText">최근에 자기주도 학습 접근방식은 단백질 서열 패턴에 대한 학습모델에도 적용되고 있다[{Rao:2019uu}, {Heinzinger:2019eu}, {Nambiar:2020}]. Tasks Assessing Protein Embeddings(TAPE) 모델은 Pfam 데이터베이스[{Rocklin et al., 2017}]에서의 3,100만개의 unlabeled 단백질 서열을 사용하여 AA contact prediction loss와 remote homology detection loss와 같은 protein-specific loss로 self-supervised하게 사전 학습된 모델이다. TAPE 사전학습 모델은 secondary structure prediction, AA contact prediction, remote homology detection, fluorescence landscape prediction, protein stability landscape predictio...</p>
</li>
<li>
<p class="itemTitle">말하고자 하는 것들</p>
<p class="itemText">Here, we present a BERT-based model employing self-supervised transfer learning for predicting epitope-specific TCR recognition. The predictive model was generated by fine-tuning the pretrained TAPE model using epitope-specific TCR CDR3βsequence datasets. We report that the fine-tuned model can achieve better performance in predicting the TCR recognition of COVID-19 epitopes.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">We anticipate our works to provide new directions for constructing a reliable model for predicting the immunogenic T-cel...</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="topLevelItemTitle">Materials and Methods</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Overall training process</p>
<p class="itemText">Figure 1 shows the schematic representation of overall training process of our model. The initial model was cloned from the pretrained TAPE model based on BERT, adding a classification layer at the end. First, the initial TAPE model was pre-trained using epitope-TCR CDR3b sequence data while freezing the embedding layer and top two encoding layers.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">Next, two fine-tuned models were generated from the pre-training model using IEDB epitope-specific TCR CDR3b sequence data and COVID-19 epitope-spec...</p>
</li>
<li>
<p class="itemTitle">Model architecture</p>
<p class="itemText">Figure 2 shows our model architecture. Input amino acid sequences concatenated by epitope and CDR3beta sequences were first encoded into tokens using a tokenizer, where each token is a integer code for a single amino acid. Each token is then embedded into a 768 dimensional vector in the pre-trained TAPE model based on BERTbase which has 12 encoding layers with 12 self-attention heads in each layer.<span class="Apple-converted-space">  </span>The TAPE model was pre-trained using unlabeled 31 million protein sequences with two tasks: next-...</p>
</li>
<li>
<p class="itemTitle">Datasets</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Fine-tuning datasets</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Positive datasets</p>
<p class="itemText">For the first fine-tuning round, the positive dataset containing epitope-specific TCR CDR3beta sequences was compiled from three data sources on May 2021: Dash et al[{Dash:2017go}] containing epitope-specific paired TCRα and TCRβ chains for<span class="Apple-converted-space">  </span>three epitopes from humans and for seven epitopes from mice, two manually curated databases that contains pathology-associated TCR sequences, such as VDJdb[{Bagaev:2019hf}](https://vdjdb.cdr3.net) and McPAS-TCR[{Tickotsky:2017bo}](http://friedmanlab.weizmann...</p>
</li>
<li>
<p class="itemTitle">Negative examples</p>
<p class="itemText">To increase the specificity of our model, it was necessary to add more epitope-specific<span class="Apple-converted-space">  </span>TCR CDR3beta sequence data to each fine-tuning dataset<span class="Apple-converted-space">  </span>as negative examples which were expected to interact between TCRs and epitopes. The background CDR3beta sequences were obtained from Howie at al., who have collected blood from two healthy donors. A negative example generated by combining the epitopes from the positive dataset and randomly selected TCR CDR3beta sequences derived from two healthy donors[...</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">Evaluation datasets</p>
<p class="itemText">For evaluating our model, the model was tested using two independent datasets: the dataset contained COVID-19 S-protein269-277(YLQPRTFLL)<span class="Apple-converted-space">  </span>with the 352 epitope-specific TCRβs from the recent study of Shomuradova et al. [34](from hereon referred to as Shomuradova dataset) and the dataset contained 415 COVID-19 S-protein269-277-specific TCRs from the ImmuneRACE study launched by Adaptive Biotechnologies and Microsoft (https://immunerace.adaptivebiotech.com, June 10, 2020 dataset, from hereon refer...</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">Finetuning the model</p>
<p class="itemText">We performed fine-tuning the pre-trained model in two rounds, switching the freezing layers. In the first fine-tuning round, the model was trained while freezing the embedding layer and top two encoding layers, where the weights of the layers were not updated. The freezing layers was extended to top six encoding layers in the second fine-tuning round. In each fine-tuning round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each e...</p>
</li>
<li>
<p class="itemTitle">Evaluatiing the model</p>
<p class="itemText">The final fine-tuned model was evaluated using two external test datasets containing COVID-19 epitope(YLQPRTFLL)-specific TCR CDR3beta sequence data. The F1 score , the harmonic mean of precision and recall, was used to quantify the prediction performance, where an F1 sore reaches its best value at 1 and worst value at 0. The F1 score is defined as:</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">where TP, FP, and FN are the numbers of true positives, false positives, and false negatives, respectively.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">Interpreting position-specific attention weights</p>
<p class="itemText">COVID-19 epitope(YLQPRTFLL)의 TCR 인지에 있어서 epitope-TCR CDRbeta 서열에서의 아미노산 위치별 기여도를 알아내기 위하여, </p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="topLevelItemTitle">Results and Discussion</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Finetuning results</p>
<p class="itemText">As shown in Figure 3, the final validation accuracies were 0.784 and 0.944 in two fine-tuning rounds, respectively. In first fine-tuning round, the validation accuracy was relatively low and the difference between training and validation scores was larger due to more general training dataset and more trainable encoding layers. On the other hand, in the second fine-tuning round, due to more specific training dataset and less trainable encoding layers, the validation accuracy was significantly hig...</p>
</li>
<li>
<p class="itemTitle">Evaluation results</p>
<p class="itemText">The final fine-tuned model was evaluated using two external test datasets containing COVID-19 epitope(YLQPRTFLL)-specific TCR CDR3beta sequence data. The F1 scores were significantly high, 0.938 and 0.968 for Shomuradova dataset and ImmuneCODE dataset, respectively. In particular , our model outperformed TCRGP model[{Emmi Jokinen, 2021}] for ImmuneCODE dataset .</p>
</li>
<li>
<p class="itemTitle">Attention analysis</p>
</li>
</ul>
<hr class="afterTitle"/>

<li>
<p class="topLevelItemTitle">Conclusion</p>
</li>
</ul>

</td>
<td width="55px">
</td>
</tr>

<!-- Bottom margin -->
<tr><td height="15px"></td></tr>

</table>

</td>
</tr>
</table>

</body>
</html>
