<html>

<head>
<title>manuscript</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<!-- NOTE: margin property = top-right-bottom-left -->
<style type="text/css">

	body {background-color: #e2e2e2}
    
    p.topLevelItemTitle {
    	margin: 30px 0px 5px 0px;
    	font-family: Times, Times New Roman, Palatino, Cochin, Serif;
    	font-size: 30px;
    	}
    	
    p.folderTitle {
    	margin: 30px 0px 5px 0px;
    	font-family: Times, Times New Roman, Palatino, Cochin, Serif;
    	font-size: 18px;
    	}
    
    p.itemTitle {
    	margin: 30px 0px 5px 0px;
    	font-family: Helvetica, Arial, Sans-Serif;
    	font-size: 12px;
    	font-weight: bold;
    	}
    	
    p.itemText {
    	margin: 0px 0px 10px 0px;
    	font-family: Helvetica, Arial, Sans-Serif;
    	font-size: 12px;
    	}
    	
    p.itemTextStart {
    	margin: 30px 0px 10px 0px;
    	font-family: Helvetica, Arial, Sans-Serif;
    	font-size: 12px;
    	}
    
    .page {border: 1px solid #c0c0c0; background: #fff}
    
    hr {
        border: none;
        height: 1px;
        color: #d4d4d4;
        background-color: #d4d4d4;
      	}
      	
    hr.afterTitle {
    	margin-top: -3px;
    }
      	
    hr.afterText {
    	margin-top: 30px;
    }
      	
    ul {
      	list-style-type: none;
      	padding-left: 30px;
      	}
      	
</style>

</head>

<body>

<table border="0" width="100%" cellspacing="3">
<tr>
<td>

<table class="page" width="100%" cellspacing="10" cellpadding="2">

<!-- Top margin -->
<!-- Not needed, because of the padding above titles. -->
<!--<tr><td height="15px"></td></tr>-->

<tr>

<!-- 42 + 30 of list indent = 72 - one inch. -->
<!-- Actually that ends up too much, so we do 25 + 30 = 55px. -->
<td width="25px">

<td valign="top">

<ul>
<li>
<p class="topLevelItemTitle">Introduction</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">백신개발을 위한 T-cell epitope 식별의 중요성</p>
<p class="itemText">대부분의 백신 후보 물질 개발은 중화항체를 생성하는 B cell에 촛점을 맞추고 있지만, 기본적인 접근법은 T cell을 기반으로 하고 있다. 혈액에서 순환하는 T 세포는 종종 증상이 나타나기 전에 감염된 세포를 탐지하고 면역 반응을 일으키거나 감염된 세포를 직접 제거하는 적응 면역 체계의 모든 바이러스에 대한 첫 번째 반응을 주도한다. SARS-CoV에서 T 세포가 효과적인 초기 면역대응과 바이러스 사멸에 중요하다는 사실이 최근의 연구에서 밝혀진 바 있다[{Oh:2019hx}, {Channappanavar:2014gj}, {Channappanavar:2014we}, {Yang:2007ei}]. 따라서, 암 면역치료와 효과적인 백신 개발에 있어서 T cell 면역 반응을 효과적으로 유도할 수 있는 면역원성을 갖는 에피토프를 식별하는 일은 매우 중요한 일이다.</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">Beyond neutralizing antibodies produced by B cells, cytotoxic CD8 T c...</p>
</li>
<li>
<p class="itemTitle">Immunogenic T-cell epitope 예측의 중요성</p>
<p class="itemText">바이러스에서 유래한 펩타이드의 약 1 % 미만이 면역 반응을 유도하기에 충분한 강도로 MHC 클래스 I 분자에 결합 할 것으로 추정되었다.[{Yewdell:uq}]. 펩타이드의 MHC 분자에 대한 결합 특이성은 MHC 분자의 residue들의 분자 구조와 물리화학적 구조에 의해 결정된다. MHC 단백질을 인코딩하는 유전자는 매우 높은 다형성을 갖고 있다. 인간 MHC, 즉 huma n leukocyte antigen(HLA) 단백질은 에 대해 3개의 HLA-I 형 주요 유전좌(HLA-A, -B, 그리고 -C)와 9개의 HLA-II 형 주요 유전자((HLA-DPA1, -DPB1, -DQA1, -DQB1, -DRA, -DRB1, -DRB3, -DRB4, and -DRB5)로부터 인코딩 된다. 현재 IMGT/HLA 데이터베이스(https://www.ebi.ac.uk/ipd/imgt/hla/stats.html)에 등록된<span class="Apple-converted-space">  </span>HLA-I 유전형은 250,597개, HLA-II 유전형은 7,723...</p>
</li>
<li>
<p class="itemTitle">Immunogenic T-cell epitope 예측의 한계점</p>
<p class="itemText">Presented pMHC complex 중 약 5% 미만의 펩타이드 만이 TCR에 의해 인지되고 T 세포의 면역반응을 유도할 수 있다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">현재까지 알려진 예측 도구들 대부분은 BA + LP 데이터만을 학습한 것들이기 때문에 immunogenic epitope을 예측하는 데 한계가 있다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">현재까지 알려진 immunogenic peptide의 데이터 수는 수만개 정도로 신뢰성 높은 예측 모델을 구축하기에 한계가 있다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">기존의 예측방법들</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Classical methods</p>
<p class="itemText">Motif-based</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">PSSM</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">Machine learning-based</p>
</li>
<li>
<p class="itemTitle">Deep learning methods</p>
<p class="itemText">ANN</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">ConvNet</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">DeepLigand</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">기존예측 방법의 한계</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Not all binding peptides are presented on the cell</p>
<p class="itemText">MHC와 결합하는 모든 펩타이드가 Tumor-specific 신항원 펩타이드는 아니다. [{Biotechnol:fe}]. MHC 분자와 결합하는 펩타이드 중에 오직 30% 미만이 tumor cell에서 process 되고 이중 일부만이 T-cell을 유도하고 clinically-relevant immunogenicity를 갖는 neopeptide 일 수 있다[{Vitiello:2017dm}, {Ott:2017ft}].</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this bindin...</p>
</li>
<li>
<p class="itemTitle">MS-based immunopeptidome studies</p>
<p class="itemText">Abelin, Bassni의 MS-based direct identification of presented peptides</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">Thus, identification of MHC-bound peptides by MS holds great promise for the generation of large-scale data sets characterizing the peptidome specific for individual MHC molecules (15, 17) and potentially for the identification of T cell epitopes (18). However, it is clear that, within the foreseeable future, the number of MHC molecules characterized by such MS studies will remain limited. In this context, significant efforts ...</p>
</li>
<li>
<p class="itemTitle">ML/DL using MS peptidome data</p>
<p class="itemText">NetMHCpan4.0</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">MHCflurry 1.2.0</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">DeepLigand</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">ML/DL 방법의 한계점</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">NetMHCPan40</p>
<p class="itemText">#</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">NetMHCpan4.0[{Jurtz:2017bw}] is a modified affinity model that includes an additional output node for training on natural ligand observations from mass spectrometry data. However, it assumes that affinity and uncorrelated display selection processes share the same sequence features[{Zeng:2019ds}].<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">NetMHCpan4.0은 전형적인 Multi-Task learning 방법을 사용하여 BA와 EL 데이터의 동시에 저레벨의 sequence 패턴을 공유하며 예측 모델을 학습시킨다. 이를 통하여 BA 데이터에 편향되지 않고(오버피팅 되지 않고) EL 데이터의 feature가 가미된(반영된) 좀 더 범용적인 예측 모델을 만들어 낼 수 있다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">그러나, 문제...</p>
</li>
<li>
<p class="itemTitle">MHCflurry</p>
<p class="itemText"></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">#</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">MHCflurry[{ODonnell:2018fv}]는 펩타이드 결합 예측에서의 FP를 줄이기 위해 IEDB 결합데이터셋으로 학습한 모델을 MS elution 데이터셋을 사용하여 성능을 평가하여 높은 성능을 보이는 예측 모델들을 ensembl하여 최종 예측 모델을 구축하였다.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">MHCflurry only considers the natural ligands as peptides that also have high binding affinity to the MHCs, and thus it remains an affinity model that doesn’t consider other peptide features that influence peptide presentation. </p>
</li>
<li>
<p class="itemTitle">DeepLigand</p>
<p class="itemText">#</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">Fig. 1. Schematics of DeepLigand. DeepLigand consists of a binding affinity prediction module and a peptide embedding module. The affinity prediction module takes as input a pair of MHC and peptide sequence and predicts the mean and variance their binding affinity using a deep residual network. The peptide embedding module is a deep language model (ELMo) that embeds each peptide into a vector representation. The outputs from the two modules are concatenated and provided as input to one fully ...</p>
</li>
</ul>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">BERT?</p>
<p class="itemText">Transformer의 encoder를 사용하여 self-attention 기반의 BERT(Bidirectional Encoder Representations from Transformers)[{Devlin:2018uk}]는 대용량 corpus의 semi-supervised 사전학습을 통해 contextual word representation(language model)을 구축하고 이를 specific한 task의 비교적 적은 양의 데이터에 대한 supervised fine-tuning을 통해 최종 예측 모델을 구축하는 방법으로 11개의 자연어 처리 task(SQuAD1.1 등)에서 SOTA를 달성하면서 최근에 크게 주목받고 있다. </p>
</li>
<li>
<p class="itemTitle">Research idea</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">양방향 자기주목을 통한 pMHC 결합패턴 학습</p>
<p class="itemText">양방향 자기주목 기반의 언어모델인 BERT를 도용함으로써, 펩타이드-MHC 결합과 펩타이드 제시에서의 아미노산 간의 상호작용 패턴- 장단거리 상호작용, 집합적 상호작용, 일반적 상호작용 등-을 펩타이드 길이와 상관없이 효과적으로 반영하는 contextual representation(language model)을 사전 훈련 시킬 수 있다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">BERT-based 전이학습을 통해 데이터 수 한계 극복이 가능</p>
<p class="itemText">또한, BERT의 전이학습 접근 방식을 따라서, BA 데이터를 사용하여 선행 학습된 언어모델을 MS로 식별된 eluted ligand 데이터를 사용한 fine-tuning을 통하여 범용적이면서 동시에 정확한 reliable한 예측 모델을 구축해 낼 수 있을 것이다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">말하고자 하는 연구내용</p>
<p class="itemText">본 논문에서는<span class="Apple-converted-space">  </span>MHC-bound 후보 (neo-)peptide의 예측을 위한 BERT를 도용한 전이학습(semi-supervised?) 방법을 제안한다.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">MHC-펩타이드 binding affinity 데이터를 사용한 선행학습을 통해 MHC-peptide의 결합에 대한 contextual language model을 construct한다. The pre-trained model을 특정 MS-identified natural ligand 데이터셋을 사용한 fine-tuning을 통해 최종 (Neo-) 펩타이드 예측 모델을 구축하고 cross-validation을 통해 성능을 검증한다. 또한, final predictive 모델을 external dataset을 사용하여 독립 성능 검증을 수행하고 다른 예측 방법들 보다 범용성과 정확도가 우수하다는 것을 증명한다.</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"><span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="topLevelItemTitle">Materials and Methods</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">전체 학습과정</p>
<p class="itemText">#</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">2) BA 데이터셋을 semi-supervised하게 pretraining 하여 펩타이드-MHC 결합에서의 아미노산 상호작용 패턴에 대한contextual language model을 구축한다. 이 때 언어모델의 범용성(generality)을 위해 입력<span class="Apple-converted-space">  </span>펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 삭제 또는 치환하여 선행학습을 수행한다. 펩타이드와 MHC 서열에서의 아미노산 삭제 또는 치환은 임의로 특정 비율의 서열 단편에 대해 수행하되 펩타이드 서열의 경우는 알려진 위치특이적 점수행렬(PSSM: Position-Specific Scoring Matrix)에 기반한 편향성을 주고, MHC 서열의 경우는 MHC 결합사이트의 진화적 보존성에 기반한 편향성을 부여한다.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">3) The pre-trained model을 특정 MS-identified natural ligand 데이터셋을 사용한 fine-tuning을 통해 최종 (Neo-) 펩타이드 예측 모델을 구축...</p>
</li>
<li>
<p class="itemTitle">Datasets</p>
<p class="itemText">{Devlin:2018uk}은 BooksCorpus (800M words)와 English Wikipedia (2,500M words) 같은<span class="Apple-converted-space">  </span>대용량의 Corpus의 텍스트를 사용하여 unsupervised한 manner로 pretraining을 수행하여 corpus에서의 contextual 한 word representations(embedding vector로 구현되는)를 통한 언어모델을 구축한 후 이를 11개의 NLP task에서의 specific한 task dataset을 사용하여 fine-tuning을 수행하여 SOTA 성능을 보이는 최종 예측 모델을 구축하였다.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">MHC와 결합하는 모든 펩타이드가 Tumor-specific 신항원 펩타이드는 아니다. [{Biotechnol:fe}]. MHC 분자와 결합하는 펩타이드 중에 오직 30% 미만이 tumor cell에서 process 되고 이중 일부만이 T-cell을 유도하고 clinically-relevant immunogen...</p>
</li>
<hr class="afterText"/>
<ul>
<li>
<p class="itemTitle">Pre-training datasets</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Source</p>
<p class="itemText">- The dataset for pretraining the model consisted of BA measurement data and MS-identified EL ligand data, excluding tumor-associated neopeptides used in fine-tuning the model.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">- The binding affinity measurement dataset for pretraining the model was compiled from a snapshot of the Immune Epitope Database (IEDB)[{Vita:2015bh}] MHC ligands downloaded on Jan. 28, 2019(http://www.iedb.org/doc/mhc_ligand_full.zip) and the BA dataset augmented with the BD2013 dataset[{Kim:2014jg}](http://tools.iedb.or...</p>
</li>
<li>
<p class="itemTitle">Data preprocessing</p>
<p class="itemText">&lt;IEDB&gt;</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">The IEDB entries were filtered to MHC allele class = I, Epitope Object Type = Linear peptide and Allele Name consistent with human HLA class I nomenclature with four-digit typing (that is, regex: “^HLA-[ABC]\\*[0–9]{2}:[0–9]{2}$”), resulting in a dataset with 0000 quantitative or qualitative measurements. Redundant ligand data with the same ligand sequence and MHC molecule were removed. A peptide was considered a binder if it had a quantitative affinity of &lt;500 nM or qualitative label of ...</p>
</li>
<li>
<p class="itemTitle">Exploring datasets</p>
<p class="itemText">Final dataset은 구성되었다. 총 0000, 000 alleles, % binders</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">Fine-tuning and evaluation datasets</p>
<p class="itemText">임상학적 신생항원 샘플들에 대한 선행학습 된 모델의 미세튜닝과 예측 성능을 평가하기 위해 [{Sarkizova:2019fu}]의 예측 모델 성능 평가에서 사용된 데이터셋을 utility 하였다. The evaluation dataset consisted of<span class="Apple-converted-space">  </span>51,531(n) HLA-bound ligands identified by LC-MS/MS from 11 patient-derived tumor cell lines and 999 x n random decoys from the human proteome. To estimate prediction power, we used the fraction of correctly predicted binders in the top 0.1% of the dataset(that is, PPV = true positive call / all positive call). We advocated for the PPV evaluation metric...</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">Encoding pMHC binding data into a sentence</p>
<p class="itemText">pMHC-I 결합데이터는 peptide 서열과 MHC-I 분자의 contact residue 서열을 concat하여 하나의 문장으로 encoding 될 수 있다. 여기서 concat 된 아미노산 서열에서 아미노산들은 word들로 treated 되어 질 수 있다(Figure-&gt;).</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">우리는 HLA-bound 사전훈련 데이터셋부터 20개의(or 22(Sec, Pyl)) 단어로 구성된 vocab을 갖는 하나의<span class="Apple-converted-space">  </span>sentence corpus를 구축할 수 있다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">Model Architecture</p>
<p class="itemText">Predictive model의 구조는 BERT base model을 참조하여 구성되었다[{Devlin:2018uk}].<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M).<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<li>
<p class="itemTitle">Input Embedding</p>
<p class="itemText">pMHC-I 결합에서의 아미노산 서열(peptide sequence + MHC contact residue sequence)는 아미노산들에 해당하는 단어들로 구성된 하나의<span class="Apple-converted-space">  </span>‘sentence’로 인코딩된다.<span class="Apple-converted-space">  </span>For a given ‘sentence’, the input embeddings of the BERT-based language model are the sum of the token embeddings and the position embeddings, where the position embeddings are used to inject some information about the relative or absolute position of the tokens in the sequence as used in original BERT model[{Devlin:2018uk}]. An input embedding matrix의 사이즈는 49 x emb_dim이다. 여기서 49는...</p>
</li>
<li>
<p class="itemTitle">Pre-traing model</p>
<p class="itemText">pMHC-I 결합 언어모델은 were pre-trained using two tasks, such as masked language model, adding prior knowledge and binding affinity prediction, in semi-supervised manner as did in BERT model,<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">Masking task에서의 loss는 cross entropy loss이고 binding affinity prediction task는 mean square loss가 사용되었고 total loss는 두 loss의 합이 사용되었다.</p>
</li>
<hr class="afterText"/>
<ul>
<li>
<p class="itemTitle">Masked LM for adding prior knowledge</p>
<p class="itemText">Masked LM task를 통하여 pMHC-I 결합에 대응하는 문장 내에서의 단어들(amino acids)간의 깊은 양방향의 contextual relationship을 선행 학습시킬 수 있다. 선행학습에서의 masking procedure은 입력 문장의 15%의 토큰들을 임의로 선택하여 80% 확률로 [MASK] 토큰으로 치환하거나(80% of the time: Replace the word with the [MASK] token), 10%의 확률로 임의 단어로 치환하거나(10% of the time: replace the word with a random word), 10%의 확률로 unchange(10% of the time: Keep the word unchanged). Then, a masked token will be used to predict the original token with cross entropy loss. 임의의 단어를 replace할 때, 펩타이드 서...</p>
</li>
<li>
<p class="itemTitle">Binding Affinity Prediction</p>
<p class="itemText">In training the binding affinity prediction model with mean square error loss, the pMHC-I binding affinity values were transformed using 1-log(a)/log(50,000), where a is the measured binding affinity to scale between 0 and 1[Nielsen, 2003]. </p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="itemTitle">Fine-tuning and evaluating the model</p>
<p class="itemText">선행학습된 언어모델은 MS-identified EL 데이터셋을 사용하여 fine-tuning 되었다. MS-identified EL 데이타는 positive-high, positive, positive-intermediate, positive-low, and negative로 label 된 정성적 affinity 데이터이므로 pre-trained 언어모델의 last output layer for binding affinity prediction를 4개의 label에 대한 one-hot encoding layer로 replace<span class="Apple-converted-space">  </span>한 후 fine-tuning을 수행하였다.</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">Decoy(negative) 데이터는 임의로 생성하였다(?). NetMHCpan, MHCflurry 등에서 random으로 negative 데이터를 생성했던 방법대로 하였다?</span></p></p>
<p class="itemText"><p class="p2"><span class="s1"></span><br></p></p>
<p class="itemText"><p class="p1"><span class="s1">Training for fine-tuning the pre-trained model was carried out in standard 5-f...</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="topLevelItemTitle">Result and Discussion</p>
</li>
<hr class="afterTitle"/>
<ul>
<li>
<p class="itemTitle">Pre-training the model</p>
<p class="itemText">Loss curves in Masked LM and Binding Affinity Prediction training epochs</span></p></p>
<p class="itemText"><p class="p1"><span class="s1"></p>
</li>
<hr class="afterText"/>
<ul>
<li>
<p class="itemTitle">Loss curve in Masked LM</p>
</li>
<li>
<p class="itemTitle">Loss curve in Binding Affinity Prediction</p>
</li>
</ul>
<hr class="afterTitle"/>

<li>
<p class="itemTitle">Fine-tuning pre-trained model</p>
<p class="itemText">Training for fine-tuning the pre-trained model was carried out in standard 5-fold cross-validation. </p>
</li>
<hr class="afterText"/>
<ul>
<li>
<p class="itemTitle">Five-fold CV results</p>
</li>
</ul>
<hr class="afterTitle"/>

<li>
<p class="itemTitle">Independent validation results</p>
<p class="itemText">Fine-tuned 된 최종 모델은 11 patient-derived tumor cell lines으로부터 detected ligands를 포함한external dataset을 사용하여 independently evaluated 되었다.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">이 데이터셋을 사용한 NetMHCpan, MHCflurry와의 예측 성능 비교를 통하여 fine-tuned 최종 예측 모델의 독립 검증을 수행하였다.<span class="Apple-converted-space"> </span></span></p></p>
<p class="itemText"><p class="p1"><span class="s1">&lt;성능비교 Table 1&gt;</span></p></p>
<p class="itemText"><p class="p1"><span class="s1">Table1에서 전체적인 성능은 our model이 좋았다. 특히, HLA-A*31:01 alllele에 대한 예측 성능은 다른 방법들보다 outperformed하였다.</p>
</li>
</ul>
<hr class="afterText"/>

<li>
<p class="topLevelItemTitle">Conclusions</p>
</li>
</ul>

</td>
<td width="55px">
</td>
</tr>

<!-- Bottom margin -->
<tr><td height="15px"></td></tr>

</table>

</td>
</tr>
</table>

</body>
</html>
