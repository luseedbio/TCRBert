{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset129 AppleMyungjo;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 pMHC-I \'b0\'e1\'c7\'d5 \'be\'f0\'be\'ee\'b8\'f0\'b5\'a8\'c0\'ba were pre-trained using two tasks, such as masked language model, adding prior knowledge and binding affinity prediction, in semi-supervised manner as did in BERT model, \
Masking task\'bf\'a1\'bc\'ad\'c0\'c7 loss\'b4\'c2 cross entropy loss\'c0\'cc\'b0\'ed binding affinity prediction task\'b4\'c2 mean square loss\'b0\'a1 \'bb\'e7\'bf\'eb\'b5\'c7\'be\'fa\'b0\'ed total loss\'b4\'c2 \'b5\'ce loss\'c0\'c7 \'c7\'d5\'c0\'cc \'bb\'e7\'bf\'eb\'b5\'c7\'be\'fa\'b4\'d9.}