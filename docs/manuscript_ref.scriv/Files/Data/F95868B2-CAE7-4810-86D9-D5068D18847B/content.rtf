{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Palatino-Roman;\f2\fnil\fcharset129 AppleMyungjo;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs24 \cf0 <$Scr_Ps::0>
\f1\fs26 Predictive model
\f2 \'c0\'c7
\f1  
\f2 \'b1\'b8\'c1\'b6\'b4\'c2
\f1  BERT base model
\f2 \'c0\'bb
\f1  
\f2 \'c2\'fc\'c1\'b6\'c7\'cf\'bf\'a9
\f1  
\f2 \'b1\'b8\'bc\'ba\'b5\'c7\'be\'fa\'b4\'d9
\f1 [\{Devlin:2018uk\}]. \
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M). \

\f0\fs24 <!$Scr_Ps::0>
\f1\fs26 \
}