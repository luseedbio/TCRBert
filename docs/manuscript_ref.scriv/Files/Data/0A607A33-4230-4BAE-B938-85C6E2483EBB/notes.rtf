{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset129 AppleSDGothicNeo-Regular;\f1\fnil\fcharset0 AppleSymbols;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue109;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c50196;}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 - Token: 20\'b0\'b3 \'be\'c6\'b9\'cc\'b3\'eb\'bb\'ea\
- Sentence: \'c6\'e9\'c5\'b8\'c0\'cc\'b5\'e5 \'bc\'ad\'bf\'ad + MHC \'b0\'e1\'c7\'d5\'bb\'e7\'c0\'cc\'c6\'ae\'c0\'c7 residue\'b5\'e9\'bf\'a1 \'b4\'eb\'c7\'d1 pseudo-sequence\
- Position Encoding that used in BERT model\
\
\
\pard\pardeftab720\sl340\sa240\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., 
\f1 \uc0\u10216 
\f0  Question, Answer 
\f1 \uc0\u10217 
\f0 ) in one token sequence. Throughout this work, a \'a1\'b0sentence\'a1\'b1 can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \'a1\'b0sequence\'a1\'b1 refers to the in- put token sequence to BERT, which may be a single sentence or two sentences packed together. \
We use WordPiece embeddings (\cf3 Wu et al.\cf2 , \cf3 2016\cf2 ) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure \cf3 1\cf2 , we denote input embedding as E, the final hidden vector of the special [CLS] token as C \'a1\'f4 R\up10 H\up0 , and the final hidden vector for the i\up10 th \up0 input token asT\dn6 i \up0 \'a1\'f4R\up10 H\up0 . \
For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure \cf3 2\cf2 . \
}