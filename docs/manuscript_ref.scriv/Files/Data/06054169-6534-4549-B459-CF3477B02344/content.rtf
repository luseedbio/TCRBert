{\rtf1\ansi\ansicpg949\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset129 AppleSDGothicNeo-Regular;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;\red16\green16\blue16;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c7451\c7843\c7451;\cssrgb\c0\c0\c0;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf0 \'bc\'b1\'c7\'e0\'c7\'d0\'bd\'c0\'b5\'c8
\f1  
\f0 \'be\'f0\'be\'ee\'b8\'f0\'b5\'a8\'c0\'ba
\f1  MS-identified EL 
\f0 \'b5\'a5\'c0\'cc\'c5\'cd\'bc\'c2\'c0\'bb
\f1  
\f0 \'bb\'e7\'bf\'eb\'c7\'cf\'bf\'a9
\f1  fine-tuning 
\f0 \'b5\'c7\'be\'fa\'b4\'d9
\f1 . \strike \strikec0 MS-identified EL 
\f0 \'b5\'a5\'c0\'cc\'c5\'b8\'b4\'c2
\f1  
\f2\i positive-high, positive, positive-intermediate, positive-low, and negative
\f0\i0 \'b7\'ce
\f2\i  
\f1\i0 label 
\f0 \'b5\'c8
\f2\i  
\f0\i0 \'c1\'a4\'bc\'ba\'c0\'fb
\f1  affinity 
\f0 \'b5\'a5\'c0\'cc\'c5\'cd\'c0\'cc\'b9\'c7\'b7\'ce
\f1  pre-trained 
\f0 \'be\'f0\'be\'ee\'b8\'f0\'b5\'a8\'c0\'c7
\f1  last output layer for binding affinity prediction
\f0 \'b8\'a6
\f1  4
\f0 \'b0\'b3\'c0\'c7
\f1  label
\f0 \'bf\'a1
\f1  
\f0 \'b4\'eb\'c7\'d1
\f1  one-hot encoding layer
\f0 \'b7\'ce
\f1  replace  
\f0 \'c7\'d1
\f1  
\f0 \'c8\'c4
\f1  fine-tuning
\f0 \'c0\'bb
\f1  
\f0 \'bc\'f6\'c7\'e0\'c7\'cf\'bf\'b4\'b4\'d9
\f1 .\
\strike0\striked0 Decoy(negative) 
\f0 \'b5\'a5\'c0\'cc\'c5\'cd\'b4\'c2
\f1  
\f0 \'c0\'d3\'c0\'c7\'b7\'ce
\f1  
\f0 \'bb\'fd\'bc\'ba\'c7\'cf\'bf\'b4\'b4\'d9
\f1 (?). NetMHCpan, MHCflurry 
\f0 \'b5\'ee\'bf\'a1\'bc\'ad
\f1  random
\f0 \'c0\'b8\'b7\'ce
\f1  negative 
\f0 \'b5\'a5\'c0\'cc\'c5\'cd\'b8\'a6 \'bb\'fd\'bc\'ba\'c7\'df\'b4\'f8 \'b9\'e6\'b9\'fd\'b4\'eb\'b7\'ce \'c7\'cf\'bf\'b4\'b4\'d9?\

\f1 \
Training for fine-tuning
\f0  the pre-trained model was carried out in 
\f1 standard 5-fold cross-validation. The training dataset were split into 5 equal sized subset in 5-fold cross-validation. For each CV round, a single subset was retained as the validation data for testing current model, and the remaining subsets were used for training current model. 
\fs26\fsmilli13333 \cf2 \expnd0\expndtw0\kerning0
In a single cross-validation round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the 
\fs26 validation loss had not been decreased for 15 consecutive epochs [\{Prechelt:2012ct\}]. \cf3 Training proceeds with the Adam optimizer[\{Kingma:2014us\}] using a minibatch size of 128.\
}