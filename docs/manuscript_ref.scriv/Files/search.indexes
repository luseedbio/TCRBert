<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="ECE93083-6494-448F-9274-7E28C927375D">
            <Title>Introduction</Title>
        </Document>
        <Document ID="B6C91DB2-8FC4-45CC-8695-8AC6F5F8DD23">
            <Title>전체 학습과정</Title>
            <Synopsis>￼
</Synopsis>
            <Text>#


2) BA 데이터셋을 semi-supervised하게 pretraining 하여 펩타이드-MHC 결합에서의 아미노산 상호작용 패턴에 대한contextual language model을 구축한다. 이 때 언어모델의 범용성(generality)을 위해 입력  펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 삭제 또는 치환하여 선행학습을 수행한다. 펩타이드와 MHC 서열에서의 아미노산 삭제 또는 치환은 임의로 특정 비율의 서열 단편에 대해 수행하되 펩타이드 서열의 경우는 알려진 위치특이적 점수행렬(PSSM: Position-Specific Scoring Matrix)에 기반한 편향성을 주고, MHC 서열의 경우는 MHC 결합사이트의 진화적 보존성에 기반한 편향성을 부여한다. 
3) The pre-trained model을 특정 MS-identified natural ligand 데이터셋을 사용한 fine-tuning을 통해 최종 (Neo-) 펩타이드 예측 모델을 구축하고 cross-validation을 통해 성능을 검증한다.</Text>
        </Document>
        <Document ID="AF59BA5F-BC3F-48C5-8575-E3638F32A08C">
            <Title>MS-based immunopeptidome studies</Title>
            <Text>Abelin, Bassni의 MS-based direct identification of presented peptides

Thus, identification of MHC-bound peptides by MS holds great promise for the generation of large-scale data sets characterizing the peptidome specific for individual MHC molecules (15, 17) and potentially for the identification of T cell epitopes (18). However, it is clear that, within the foreseeable future, the number of MHC molecules characterized by such MS studies will remain limited. In this context, significant efforts over the last decades have been dedicated to experimentally characterizing the peptide-binding space of MHC molecules using semi–high-throughput MHC– peptide BA assays (19, 20), enabling binding-specificity characterization of a large set of MHC molecules from different species. 

MS 방법의 단점들
데이터 수의 한계,allele coverage가 낮다
시간과 비용 cost?

Deconvolution for assigning HLA allele
a key obstacle is the ambiguity that arises from the co-expression of multiple HLA alleles. 

The majority of LC-MS/MS studies of the HLA peptidome have used cells expressing multiple HLA molecules, which requires peptides to be assigned to one of up to six class I alleles through the use of pre-existing bioinformatics predictors, or ‘‘deconvolu- tion’’ (Bassani-Sternberg and Gfeller, 2016). Thus, peptides that do not closely match known motifs cannot confidently be reported as binders to a given HLA allele. By contrast, we used a rapid approach to generate a high-quality LC-MS/MS dataset of &gt;24,000 endogenous peptides whose assignment to specific HLA alleles was unambiguous. Because we knew the allele assignment a priori, we greatly enhanced our analyses depth. 
 
However, historically liquid chromatography-tandem mass spectrometry (LC- MS/MS) methods have required large cellular input, which limits throughput, and the multi-allelic nature of the data complicates productive motif learning. 

the potential impact of any biases associated with ligands identified by MS, such as depletion of cysteines 
We note that certain biases (Klont et al., 2018; Mahoney et al., 2011) are known to exist in the sequences identified from mass spectrometry, which could limit the power of computational models trained on such datasets. 

The predicted MS observability of the HLA peptides and frequencies of individual amino acids between MS and IEDB peptides were highly similar, aside from underrepresentation of cysteine (Figures 1E and 1F). Free cysteine, which interferes with precursor fragmentation during LC-MS/MS, is underrepre- sented in other MS-based HLA-peptide datasets (Bassani-Stern- berg et al., 2015; Trolle et al., 2016) (Figure S1F). We recovered cysteine-containing peptides when a third round of database search accounted for cysteinylation (Table S1C). </Text>
        </Document>
        <Document ID="D1ED545E-FA3B-4A3F-A4D1-3F1F4E6E6E13">
            <Title>백신개발을 위한 T-cell epitope 식별의 중요성</Title>
            <Text>대부분의 백신 후보 물질 개발은 중화항체를 생성하는 B cell에 촛점을 맞추고 있지만, 기본적인 접근법은 T cell을 기반으로 하고 있다. 혈액에서 순환하는 T 세포는 종종 증상이 나타나기 전에 감염된 세포를 탐지하고 면역 반응을 일으키거나 감염된 세포를 직접 제거하는 적응 면역 체계의 모든 바이러스에 대한 첫 번째 반응을 주도한다. SARS-CoV에서 T 세포가 효과적인 초기 면역대응과 바이러스 사멸에 중요하다는 사실이 최근의 연구에서 밝혀진 바 있다[{Oh:2019hx}, {Channappanavar:2014gj}, {Channappanavar:2014we}, {Yang:2007ei}]. 따라서, 암 면역치료와 효과적인 백신 개발에 있어서 T cell 면역 반응을 효과적으로 유도할 수 있는 면역원성을 갖는 에피토프를 식별하는 일은 매우 중요한 일이다.

Beyond neutralizing antibodies produced by B cells, cytotoxic CD8 T cells and the helper CD4 T cells are essential to clear viruses. Circulating in the blood, T cells are leading the first response to any virus in adaptive immune system that detects infected cells and mount an immune response or directly clear the infected cells, often before symptoms appear. Several studies have shown that T cells were essential for effective early immune response and virus clearance[{Oh:2019hx}, {Channappanavar:2014gj}, {Channappanavar:2014we},{Yang:2007ei}]. Therefore, it is important to identify the T cell epitopes that can induce T cell immune responses in effective COVID-19 vaccine development.
</Text>
        </Document>
        <Document ID="9736EDCF-B577-4C0D-ABB3-66C1EAE01F7C">
            <Title>Hi-TIDe : Immunopeptidomics - DOF</Title>
            <Text>By continuing your navigation on the sites and applications "unil.ch", you accept the use of cookies allowing us to optimize your user experience. Read the legal information
OK
Department of Oncology

You are here:  UNIL  &gt; Department of Oncology &gt; Our research groups &gt; Bassani (Hi-TIDe)
 Our research groups 
Hi-TIDe : Immunopeptidomics

| Research interest | Research group projects | Selected publications 
 

 
Michal BASSANI-STERNBERG
Group leader
Human integrated tumor immunology discovery engine (Hi-TIDe)

Department of oncology UNIL CHUV
Ludwig Institute for Cancer Research Lausanne

Head of clinical mass spectrometry unit
Center of experimental therapeutics

Phone +41 79 900 55 30
Email Michal.bassani@chuv.ch



 
 

Research interest
Our main goal is to identify clinically relevant cancer specific Human Leukocyte Antigen (HLA) ligands that will guide the development of personalized cancer immunotherapy using mass-spectrometry (MS), currently the only methodology to unbiasedly identify HLA binding peptides that are presented in vivo to cytotoxic T cells.

Research group projects
Proteogenomics and MS-based immunopeptidomics approaches to identify HLA ligands derived from tumor-associated proteins, mutated neoantigens, non-canonical ORFs and post translationally modified peptides.



We developed a high-throughput and in depth MS-based immunopeptidomics pipeline that now enables robust and reproducible sample preparation and measurement of HLA/MHC class I and class II peptides. We are currently applying this methodology to identify tumor-associated ligands extracted from cell lines and tumor tissues from humans and nice models. 
We have initiated fundamental discovery work to elucidate how tumor cells present antigens and what are the bases of tumor immunogenicity.
We are investigating the differences between tumor types in terms of antigen presentation and how drugs modulate the immunopeptidome.
In collaboration with the Vital-IT group (SIB), we have established a continuous bio-informatics pipeline enabling direct identification of neoantigens by combining genomic information derived from exome-seq analysis with measured immunopeptidomics data.
In collaboration with Pr David Gfeller, we are improving the performance of HLA class I and class II binding prediction tools by training them with our measured immunopeptidomics data.
We apply proteogenomics approaches to identify personalized neo-antigens from patient tumor samples. These tumor-specific antigens will be further developed into personalized cancer vaccines or to enrich tumor-reactive and antigen-specific T cells for adoptive T cell-based therapies.
Selected publications
Julien Racle, Justine Michaux, Georg Alexander Rockinger, Marion Arnaud, Philippe Guillaume, Sara Bobisse, George Coukos, Alexandre Harari, Camilla Jandus, Michal Bassani-Sternberg* and David Gfeller* (2019). Deep motif deconvolution of HLA-II peptidomes for robust epitope predictions. BioRxiv * Co-corresponding authors.

Bassani-Sternberg, M. *, Digklia, A., Huber, F., Wagner, D., Sempoux, C., Stevenson, B. J., Thierry, A.-C., Michaux, J., Pak, H., Racle, J., Boudousquie, C., Balint, K., Coukos, G., Gfeller, D., Martin Lluesma, S., Harari, A., Demartines, N., and Kandalaft, L. E.* (2019). A phase Ib study of the combination of personalized autologous dendritic cell vaccine, aspirin and standard of care adjuvant chemotherapy followed by nivolumab for resected pancreatic adenocarcinoma - a proof of antigen discovery feasibility in three patients. Frontiers in Immunology doi:10.3389/fimmu.2019.01832. *Co-corresponding authors

Ebrahimi-Nik, H., Michaux, J., Corwin, W.L., Keller, G.L., Shcheglova, T., Pak, H., Coukos, G., Baker, B.M., Mandoiu, II, Bassani-Sternberg, M.* and Srivastava, P.K* (2019). Mass spectrometry driven exploration reveals nuances of neoepitope-driven tumor rejection. JCI Insight 5, doi:10.1172/jci.insight.129152. *Co-corresponding authors.

Chong, C., Marino, F., Pak, H. S., Racle, J., Daniel, R. T., Muller, M., Gfeller, D., Coukos, G., &amp; Bassani-Sternberg, M. (2018). High-throughput and sensitive immunopeptidomics platform reveals profound IFNgamma-mediated remodeling of the HLA ligandome. Mol Cell Proteomics. doi:10.1074/mcp.TIR117.000383

*Bassani-Sternberg, M., *Braunlein, E., Klar, R., Engleitner, T., Sinitcyn, P., Audehm, S., Straub, M., Weber, J., Slotta-Huspenina, J., Specht, K., Martignoni, M. E., Werner, A., Hein, R., D, H. Busch, Peschel, C., Rad, R., Cox, J., Mann, M., &amp; Krackhardt, A. M. (2016). Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry. Nat Commun, 7, 13404. doi:10.1038/ncomms13404

Group members
Chloe Chong
Post doctoral fellow
Humberto Ferreira
Post doctoral fellow
Elodie Lauret Marie Joseph
Post doctoral fellow
Justine Michaux
Laboratory Technician
HuiSong Pak
MS operator
Florian Huber
Bioinformatician
 
Markus Müller 
Senior bioinformatician (SIB)
Brian Stevenson 
Senior bioinformatician (SIB)
 

Share:    
Ch. des Boveresses 155 - CH-1066 Epalinges
Switzerland
Tel. +41 21 692 59 92 
Fax +41 21 692 59 95
Contact 
Directories
Legal information
Sitemap
Edition
 
  
 </Text>
        </Document>
        <Document ID="D12B2365-D2DE-46C4-8888-189EE79914C4">
            <Title>Deep learning methods</Title>
            <Text>ANN
ConvNet

DeepLigand
</Text>
        </Document>
        <Document ID="C5CAFA7A-9BA8-408E-8306-C5640C771104">
            <Title>NetMHCPan40</Title>
            <Text>#

NetMHCpan4.0[{Jurtz:2017bw}] is a modified affinity model that includes an additional output node for training on natural ligand observations from mass spectrometry data. However, it assumes that affinity and uncorrelated display selection processes share the same sequence features[{Zeng:2019ds}]. 
NetMHCpan4.0은 전형적인 Multi-Task learning 방법을 사용하여 BA와 EL 데이터의 동시에 저레벨의 sequence 패턴을 공유하며 예측 모델을 학습시킨다. 이를 통하여 BA 데이터에 편향되지 않고(오버피팅 되지 않고) EL 데이터의 feature가 가미된(반영된) 좀 더 범용적인 예측 모델을 만들어 낼 수 있다.
그러나, 문제는 BA 데이터를 학습한 예측 모델의 false positive이다. 따라서 BA 데이터로부터 세포 표면으로 제시되는 후보 신항원을 selectively하게 선별해내는 예측 모델을 구축하는데에 한계가 있을 수 있다.</Text>
        </Document>
        <Document ID="3AF7D8EA-0A64-417D-8A1D-432B6272B8AB">
            <Title>Fine-tuning and evaluation datasets</Title>
            <Text>임상학적 신생항원 샘플들에 대한 선행학습 된 모델의 미세튜닝과 예측 성능을 평가하기 위해 [{Sarkizova:2019fu}]의 예측 모델 성능 평가에서 사용된 데이터셋을 utility 하였다. The evaluation dataset consisted of  51,531(n) HLA-bound ligands identified by LC-MS/MS from 11 patient-derived tumor cell lines and 999 x n random decoys from the human proteome. To estimate prediction power, we used the fraction of correctly predicted binders in the top 0.1% of the dataset(that is, PPV = true positive call / all positive call). We advocated for the PPV evaluation metric over the commonly used area under the receiver operating characteristic curve(AUROC), because it is better suited for the HLA presentation prediction where a relatively small number of true binders within an excess of non binders. A MHC molecule is expected to present approximately only 0.1% peptides among the 9-mer peptides in the human proteome[{Abelin:2017cn}, {BassaniSternberg:2015js}, {Sarkizova:2019fu].  
</Text>
            <Notes>The MS model selection dataset consists of 226,684 ligands formed by combining 186,415 ligands from IEDB with 39,741 additional ligands from SysteMHC Atlas (Shao et al., 2018) and 530 additional ligands from ref. (Abelin et al., 2017). The unprocessed SysteMHC and Abelin et al. datasets are much larger but most entries are already present in IEDB, are duplicates, or report on alleles for which training was not attempted. The ligands from SysteMHC Atlas were first filtered to remove entries with low confidence (prob &lt; 0.99). Of the 112 alleles supported by the predictor, 57 had at least 100 MS ligands available for model selection (Table S1). 
</Notes>
        </Document>
        <Document ID="DC1448AA-AC33-4695-BAEB-8E8BE85AA8DA">
            <Title>Google AI Blog: Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</Title>
            <Text>Blog
The latest news from Google AI
Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing
Friday, November 2, 2018
Posted by Jacob Devlin and Ming-Wei Chang, Research Scientists, Google AI Language


One of the biggest challenges in natural language processing (NLP) is the shortage of training data. Because NLP is a diversified field with many distinct tasks, most task-specific datasets contain only a few thousand or a few hundred thousand human-labeled training examples. However, modern deep learning-based NLP models see benefits from much larger amounts of data, improving when trained on millions, or billions, of annotated training examples. To help close this gap in data, researchers have developed a variety of techniques for training general purpose language representation models using the enormous amount of unannotated text on the web (known as pre-training). The pre-trained model can then be fine-tuned on small-data NLP tasks like question answering and sentiment analysis, resulting in substantial accuracy improvements compared to training on these datasets from scratch. 

This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. The release includes source code built on top of TensorFlow and a number of pre-trained language representation models. In our associated paper, we demonstrate state-of-the-art results on 11 NLP tasks, including the very competitive Stanford Question Answering Dataset (SQuAD v1.1). 

What Makes BERT Different?
BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).

Why does this matter? Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” However, BERT represents “bank” using both its previous and next context — “I accessed the ... account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.

A visualization of BERT’s neural network architecture compared to previous state-of-the-art contextual pre-training methods is shown below. The arrows indicate the information flow from one layer to the next. The green boxes at the top indicate the final contextualized representation of each input word:

BERT is deeply bidirectional, OpenAI GPT is unidirectional, and ELMo is shallowly bidirectional.
The Strength of Bidirectionality
If bidirectionality is so powerful, why hasn’t it been done before? To understand why, consider that unidirectional models are efficiently trained by predicting each word conditioned on the previous words in the sentence. However, it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model. 

To solve this problem, we use the straightforward technique of masking out some of the words in the input and then condition each word bidirectionally to predict the masked words. For example:

While this idea has been around for a very long time, BERT is the first time it was successfully used to pre-train a deep neural network.

BERT also learns to model relationships between sentences by pre-training on a very simple task that can be generated from any text corpus: Given two sentences A and B, is B the actual next sentence that comes after A in the corpus, or just a random sentence? For example:

Training with Cloud TPUs
Everything that we’ve described so far might seem fairly straightforward, so what’s the missing piece that made it work so well? Cloud TPUs. Cloud TPUs gave us the freedom to quickly experiment, debug, and tweak our models, which was critical in allowing us to move beyond existing pre-training techniques. The Transformer model architecture, developed by researchers at Google in 2017, also gave us the foundation we needed to make BERT successful. The Transformer is implemented in our open source release, as well as the tensor2tensor library.

Results with BERT
To evaluate performance, we compared BERT to other state-of-the-art NLP systems. Importantly, BERT achieved all of its results with almost no task-specific changes to the neural network architecture. On SQuAD v1.1, BERT achieves 93.2% F1 score (a measure of accuracy), surpassing the previous state-of-the-art score of 91.6% and human-level score of 91.2%:

BERT also improves the state-of-the-art by 7.6% absolute on the very challenging GLUE benchmark, a set of 9 diverse Natural Language Understanding (NLU) tasks. The amount of human-labeled training data in these tasks ranges from 2,500 examples to 400,000 examples, and BERT substantially improves upon the state-of-the-art accuracy on all of them:

Making BERT Work for You
The models that we are releasing can be fine-tuned on a wide variety of NLP tasks in a few hours or less. The open source release also includes code to run pre-training, although we believe the majority of NLP researchers who use BERT will never need to pre-train their own models from scratch. The BERT models that we are releasing today are English-only, but we hope to release models which have been pre-trained on a variety of languages in the near future. 

The open source TensorFlow implementation and pointers to pre-trained BERT models can be found at http://goo.gl/language/bert. Alternatively, you can get started using BERT through Colab with the notebook “BERT FineTuning with Cloud TPUs.”

You can also read our paper "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" for more details.
   
    
 
 Google  Privacy  Terms</Text>
        </Document>
        <Document ID="C796E5F0-8538-4CBA-ADE6-26BB129254FB">
            <Title>NetMHCpan4.0</Title>
            <Text>
	

	
Services are gradually being migrated to https://services.healthtech.dtu.dk/.
In the near future, cbs.dtu.dk will be retired. Please try out the new site.


Home


Supplementary material

Here, you will find the data set used for training and testing of the NetMHCpan-4.0 method.

Training data

The training binding data are partitioned in 5 files to be used for cross-validation. For instance do the f000_ba and f000_el files contain the binding affinity and eluted ligand training data, and the c000_ba and c000_el files the binding affinity and eluted ligand test data for the first cross-validation partitioning. It is critical that this data partitioning is maintained.

The format for each of the files is

ARWLASTPL 0.589395 BoLA-D18.4 85.0
ASYAAAAAY 0.496594 BoLA-D18.4 232.0
GMMGGLWKY 0.439136 BoLA-D18.4 432.0
KMFHGGLRY 0.898463 BoLA-D18.4 3.0
KMLEASTIY 0.75609 BoLA-D18.4 14.0
KQLEYSWVL 0.481554 BoLA-D18.4 273.0
KQWSWFSLL 0.451477 BoLA-D18.4 378.0
MMFDAMGAL 0.935937 BoLA-D18.4 2.0
MMMSTAVAF 0.762939 BoLA-D18.4 13.0
MTFPVSLEY 0.485003 BoLA-D18.4 263.0
where the first column gives the peptide, the second column the log50k transformed binding affinity (i.e. 1 - log50k( aff nM)) or 1/0 for the eluted ligangd data, and the third column the class I allele.

When classifying BA peptides into binders and non-binders for calculation of the AUC values for instance, a threshold of 500 nM is used. This means that peptides with log50k transformed binding affinity values greater than 0.426 are classified as binders.

BA data

f000_ba (Train data) c000_ba (Test data)
f001_ba (Train data) c001_ba (Test data)
f002_ba (Train data) c002_ba (Test data)
f003_ba (Train data) c003_ba (Test data)
f004_ba (Train data) c004_ba (Test data)
EL data

f000_el (Train data) c000_el (Test data)
f001_el (Train data) c001_el (Test data)
f002_el (Train data) c002_el (Test data)
f003_el (Train data) c003_el (Test data)
f004_el (Train data) c004_el (Test data)
References

NetMHCpan, a method for MHC class I binding prediction beyond humans 
Ilka Hoof, Bjoern Peters, John Sidney, Lasse Eggers Pedersen, Ole Lund, Soren Buus, and Morten Nielsen 
Immunogenetics 61.1 (2009): 1-13 
PMID: 19002680   Full text 

This file was last modified Sunday 27th 2019f October 2019 13:08:50 GMT


</Text>
        </Document>
        <Document ID="3B20F4A6-3400-4B9C-8591-A326AF8F616B">
            <Title>Result and Discussion</Title>
        </Document>
        <Document ID="0F61C527-EEA6-4954-BC9D-5527562FE504">
            <Title>Datasets</Title>
            <Notes>￼</Notes>
        </Document>
        <Document ID="95384325-4873-420F-AE94-3C7E0A8FF5DD">
            <Title>The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time</Title>
            <Text>
Jay Alammar
Visualizing machine learning one concept at a time
Blog About
The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)

Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) 
Translations: Chinese (Simplified), Persian

The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).


(ULM-FiT has nothing to do with Cookie Monster. But I couldn’t think of anything else..)

One of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component – saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch.


The two steps of how BERT is developed. You can download the model pre-trained in step 1 (trained on un-annotated data), and only worry about fine-tuning it for step 2. [Source for book icon].
BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).

There are a number of concepts one needs to be aware of to properly wrap one’s head around what BERT is. So let’s start by looking at ways you can use BERT before looking at the concepts involved in the model itself.

Example: Sentence Classification

The most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:



To train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT.

For people not versed in the topic, since we’re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a labele (“spam” or “not spam” for each message).


Other examples for such a use-case include:

Sentiment analysis
Input: Movie/Product review. Output: is the review positive or negative?
Example dataset: SST
Fact-checking
Input: sentence. Output: “Claim” or “Not Claim”
More ambitious/futuristic example:
Input: Claim sentence. Output: “True” or “False”
Full Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, by with ML later, hopefully).
Video: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.
Model Architecture

Now that you have an example use-case in your head for how BERT can be used, let’s take a closer look at how it works.



The paper presents two model sizes for BERT:

BERT BASE – Comparable in size to the OpenAI Transformer in order to compare performance
BERT LARGE – A ridiculously huge model which achieved the state of the art results reported in the paper
BERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model – a foundational concept for BERT and the concepts we’ll discuss next.



Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).

Model Inputs



The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.

Just like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.



In terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge.

Model Outputs

Each position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).



That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.



If you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.

Parallels with Convolutional Nets

For those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network.



A New Age of Embedding

These new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let’s recap how those are used before pointing to what has now changed.

Word Embedding Recap

For words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).

The field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)


The GloVe word embedding of the word "stick" - a vector of 200 floats (rounded to two decimals). It goes on for two hundred values.
Since these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:

 
ELMo: Context Matters

If we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born.


Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams
Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.


ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.

What’s ELMo’s secret?

ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.


A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.

We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done.

ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.


Great slides on ELMo
ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).


ULM-FiT: Nailing down Transfer Learning in NLP

ULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.

NLP finally had a way to do transfer learning probably as well as Computer Vision could.

The Transformer: Going beyond LSTMs

The release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs.

The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).

OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling

It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.


The OpenAI Transformer is made up of the decoder stack from the Transformer
The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).

With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.


The OpenAI Transformer is now ready to be trained to predict the next word on a dataset made up of 7,000 books.
Transfer Learning to Downstream Tasks

Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):


How to use a pre-trained OpenAI transformer to do sentence clasification
The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.


Isn’t that clever?

BERT: From Decoders to Encoders

The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?

“Hold my beer”, said R-rated BERT.

Masked Language Model

“We’ll use transformer encoders”, said BERT.

“This is madness”, replied Ernie, “Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.”

“We’ll use masks”, said BERT confidently.


BERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.
Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).

Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.

Two-sentence Tasks

If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).

To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?


The second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT actually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.
Task specific-Models

The BERT paper shows a number of ways to use BERT for different tasks.


BERT for feature extraction

The fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.


Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):


Take BERT out for a spin

The best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.

The next step would be to look at the code in the BERT repo:

The model is constructed in modeling.py (class BertModel) and is pretty much identical to a vanilla Transformer encoder.
run_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file.

Several pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.

BERT doesn’t look at words as tokens. Rather, it looks at WordPieces. tokenization.py is the tokenizer that would turns your words into wordPieces appropriate for BERT.
You can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model.

Acknowledgements

Thanks to Jacob Devlin, Matt Gardner, Kenton Lee, Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post.

Written on December 3, 2018
Subscribe to get notified about upcoming posts by email

Email Address




This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 
Attribution example: 
Alammar, Jay (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/ 

Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.
  </Text>
        </Document>
        <Document ID="C8F98E66-7AC6-4300-B250-87783EE31C61">
            <Title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</Title>
            <Text>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
Google AI Language
{jacobdevlin,mingweichang,kentonl,kristout}@google.com
Abstract
We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be fine- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
1 Introduction
Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).
There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as addi- tional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.
We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the fine-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.
In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked
arXiv:1810.04805v2 [cs.CL] 24 May 2019

word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair representations. The contributions of our paper are as follows:
• We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.
• We show that pre-trained representations reduce the need for many heavily-engineered task- specific architectures. BERT is the first fine- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-specific architectures.
• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert.
2 Related Work
There is a long history of pre-training general lan- guage representations, and we briefly review the most widely-used approaches in this section.
2.1 Unsupervised Feature-based Approaches
Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013).
These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016).
ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els.
2.2 Unsupervised Fine-tuning Approaches
As with the feature-based approaches, the first works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008).
More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-

           NSP Mask LM Mask LM
MNLI NER SQuAD
Start/End Span
                       C T1 ... TN T[SEP] T1’ ... TM’ BERT
C T1 ... TN T[SEP] T1’
... TM’
...
          E
[CLS]
 E
1
 E
N
 E
[SEP]
 E’ 1
  E
[CLS]
 E
1
    E’ M
E
N
E
[SEP]
E’ 1
                            [CLS]
...
Tok 1 ... Tok N
Masked Sentence A
[SEP]
...
Tok 1 ...
TokM
[CLS]
BERT BERT ...
Tok 1 ... Tok N [SEP]
Tok 1 ... TokM
        Masked Sentence B
Question
Question Answer Pair
Fine-Tuning
Paragraph
E’ M
    Unlabeled Sentence A and B Pair Pre-training
  Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers).
ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).
2.3 Transfer Learning from Supervised Data
There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014).
3 BERT
We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and fine-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine- tuning, the BERT model is first initialized with the pre-trained parameters, and all of the param- eters are fine-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate fine-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.
A distinctive feature of BERT is its unified ar- chitecture across different tasks. There is mini-
mal difference between the pre-trained architec- ture and the final downstream architecture.
Model Architecture BERT’s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4
1 https://github.com/tensorflow/tensor2tensor
2 http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/filter size to be 4H,
i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-
 
Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one token sequence. Throughout this work, a “sentence” can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special clas- sification token ([CLS]). The final hidden state corresponding to this token is used as the ag- gregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ∈ RH, and the final hidden vector for the ith input token asTi ∈RH.
For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2.
3.1 Pre-training BERT
Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1.
Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly “see itself”, and the model could trivially predict the target word in a multi-layered context.
former is often referred to as a “Transformer encoder” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation.
In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input.
Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace “masked” words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.
Task #2: Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6
5The final model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.
  
           Input
[CLS]
my
dog
is
cute
[SEP]
he
likes
play ##ing
[SEP]
             Token Embeddings
Segment Embeddings
Position Embeddings
E
B
E
8
E[CLS]
Emy
Edog
Eis
Ecute
E[SEP]
Ehe
Elikes
Eplay
E##ing
E[SEP]
              EA
 EA
 EA
 EA
 EA
 EA
 EB
 EB
 EB
 EB
              E0
 E1
 E2
 E3
 E4
 E5
 E6
 E7
 E9
 E10
Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings.
The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters.
Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.
3.2 Fine-tuning BERT
Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences.
For each task, we simply plug in the task- specific inputs and outputs into BERT and fine- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and
(4) a degenerate text-∅ pair in text classification or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as en- tailment or sentiment analysis.
Compared to pre-training, fine-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-specific details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5.
4 Experiments
In this section, we present BERT fine-tuning re- sults on 11 NLP tasks.
4.1 GLUE
The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.
To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hid- den vector C ∈ RH corresponding to the first input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during fine-tuning are classification layer weights W ∈ RK ×H , where K is the number of labels. We com- pute a standard classification loss with C and W , i.e., log(softmax(C W T )).
7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.
8See (10) in https://gluebenchmark.com/faq.
 
 System
MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average
 Pre-OpenAI SOTA BiLSTM+ELMo+Attn OpenAI GPT BERTBASE BERTLARGE
392k 80.6/80.1 76.4/76.1 82.1/81.4 84.6/83.4 86.7/85.9
363k 108k 67k 8.5k 5.7k 66.1 82.3 93.2 35.0 81.0 64.8 79.8 90.4 36.0 73.3 70.3 87.4 91.3 45.4 80.0 71.2 90.5 93.5 52.1 85.8 72.1 92.7 94.9 60.5 86.5
3.5k 2.5k - 86.0 61.7 74.0 84.9 56.8 71.0 82.3 56.0 75.1 88.9 66.4 79.6 89.3 70.1 82.1
  Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The “Average” column is slightly different thantheofficialGLUEscore,sinceweexcludetheproblematicWNLIset.8 BERTandOpenAIGPTaresingle- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.
We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that fine- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different fine-tuning data shuffling and clas- sifier layer initialization.9
Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.
We find that BERTLARGE significantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.
4.2 SQuAD v1.1
The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from
9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.
10 https://gluebenchmark.com/leaderboard
Wikipedia containing the answer, the task is to predict the answer text span in the passage.
As shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- torS ∈ RH andanendvectorE ∈ RH during fine-tuning. The probability of word i being the start of the answer span is computed as a dot prod-
uct between Ti and S followed by a softmax over eS·Ti
all of the words in the paragraph: Pi = 􏰀j eS·Tj .
 The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as S ·Ti + E ·Tj , and the maximum scoring span where j ≥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.
Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.
Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA fine-
11QANet is described in Yu et al. (2018), but the system has improved substantially after publication.
  
  System Dev Test EM F1 EM F1
Top Leaderboard Systems (Dec 10th, 2018)
System Dev Test
ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0
BERTBASE 81.6 -
  Human
#1 Ensemble - nlnet #2 Ensemble - QANet
Published BiDAF+ELMo (Single)
- -
- -
- -
82.3
86.0 91.7 84.5 90.5
- 85.8 82.3 88.5
- - - -
91.2
 86.6 86.3
formance is measured with 100 samples, as reported in the SWAG paper.
sˆ = max S·T + E·T . We predict a non-null i,j j≥i i j
answer when sˆ &gt; s + τ , where the thresh- i,j null
old τ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.
The results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system.
4.4 SWAG
The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices.
When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer.
We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors’ baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%.
5 Ablation Studies
In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional
BERTLARGE Human (expert)†
Human (5 annotations)†
Table 4: SWAG Dev and Test accuracies. †Human per-
  R.M. Reader (Ensemble) Ours
BERTBASE (Single) BERTLARGE (Single) BERTLARGE (Ensemble) BERTLARGE (Sgl.+TriviaQA) BERTLARGE (Ens.+TriviaQA)
- 85.6 81.2 87.9
80.8 88.5 84.1 90.9 85.8 91.8 84.2 91.1 86.2 92.2
- 85.0 - 88.0
  Human
#1 Single - MIR-MRC (F-Net) #2 Single - nlnet
86.3 89.0
- -
- -
86.9 89.5 74.8 78.0 74.2 77.1
71.4 74.9 71.4 74.4
80.0 83.1
-
-
85.1 91.8 87.4 93.2
 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and fine-tuning seeds.
 System
Top Leaderboard Systems (Dec 10th, 2018)
Dev Test EM F1 EM F1
  unet (Ensemble) SLQA+ (Single)
BERTLARGE (Single)
Published
Ours
- - -
 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.
tuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12
4.3 SQuAD v2.0
The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic.
We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = S·C + E·C to the score of the best non-null span
12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.
78.7 81.9
  
 Tasks
BERTBASE
No NSP
LTR &amp; No NSP
+ BiLSTM
Dev Set
MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1)
results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.
We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.
5.2 Effect of Model Size
In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously.
Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such significant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters.
It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suffi- ciently pre-trained. Peters et al. (2018b) presented
 84.4 88.4 86.7 83.9 84.9 86.5 82.1 84.3 77.5 82.1 84.1 75.7
92.7 88.5 92.6 87.9
92.1
91.6 84.9
77.8
 Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. “No NSP” is trained without the next sentence prediction task. “LTR &amp; No NSP” is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. “+ BiLSTM” adds a ran- domly initialized BiLSTM on top of the “LTR + No NSP” model during fine-tuning.
ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks
We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, fine-tuning scheme, and hyperpa- rameters as BERTBASE:
No NSP: A bidirectional model which is trained using the “masked LM” (MLM) but without the “next sentence prediction” (NSP) task.
LTR &amp; No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our fine-tuning scheme.
We first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring “No NSP” to “LTR &amp; No NSP”. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the

 mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach — we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.
5.3 Feature-based Approach with BERT
All of the BERT results presented so far have used the fine-tuning approach, where a simple classifi- cation layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a down- stream task. However, the feature-based approach, where fixed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.
In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF
Hyperparams Dev Set Accuracy
System
ELMo (Peters et al., 2018a)
CVT (Clark et al., 2018) CSE (Akbik et al., 2018)
Fine-tuning approach BERTLARGE BERTBASE
Feature-based approach (BERTBASE) Embeddings
Second-to-Last Hidden
Last Hidden
Weighted Sum Last Four Hidden Concat Last Four Hidden Weighted Sum All 12 Layers
Dev F1
95.7 -
-
96.6 96.4
91.0 95.6 94.9 95.9 96.1 95.5
Test F1
92.2 92.6 93.1
92.8 92.4
- - - - - -
      #L
3 6 6
12 12 24
#H #A
LM(ppl) MNLI-m MRPC SST-2
Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.
layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.
To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without fine-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classification layer.
Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine- tuning and feature-based approaches.
6 Conclusion
Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architec- tures. Our major contribution is further general- izing these findings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks.
 768 12 5.84 768 3 5.24 768 12 4.68 768 12 3.99
1024 16 3.54 1024 16 3.23
77.9 79.8 88.4
80.6 82.2
81.9 84.8 91.3 84.4 86.7 92.9 85.7 86.9 93.3
86.6 87.8
93.7
90.7
 Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. “LM (ppl)” is the masked LM perplexity of held-out training data.

References
Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.
Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120–128. Association for Computa- tional Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancou- ver, Canada. Association for Computational Lin- guistics.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005.
Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.
Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL.
Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914– 1925.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160–167. ACM.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ̈ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670–680, Copen- hagen, Denmark. Association for Computational Linguistics.
Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079–3087.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.
William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).
William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the . arXiv preprint arXiv:1801.07736.
Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics.
Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics.
Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI.
Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557.
 
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.
Quoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188–1196.
Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.
Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence represen- tations. In International Conference on Learning Representations.
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS.
Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.
Andriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081–1088. Curran As- sociates, Inc.
Ankur P Parikh, Oscar Ta ̈ckstro ̈m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP.
Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532– 1543.
Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL.
Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499–1509.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383–2392.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.
Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638.
Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433.
Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL ’10, pages 384–394.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.
Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform

for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353–355.
Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics.
Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv:1805.12471.
Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.
Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”
We organize the appendix into three sections:
• Additional implementation details for BERT are presented in Appendix A;
• •
A A.1
Additional details for our experiments are presented in Appendix B; and
Additional ablation studies are presented in Appendix C.
We present additional ablation studies for BERT including:
– Effect of Number of Training Steps; and – Ablation for Different Masking Proce-
dures.
Additional Details for BERT Illustration of the Pre-training Tasks
We provide examples of the pre-training tasks in the following.
Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by
• 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy → my dog is [MASK]
• 10% of the time: Replace the word with a randomword,e.g.,my dog is hairy → my dog is apple
• 10% of the time: Keep the word un- changed,e.g.,my dog is hairy → my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.
The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.
Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model

       OpenAI GPT
T T ... T 12N
Trm Trm ... Trm
Trm Trm ... Trm
...
                       Lstm Lstm
Lstm Lstm
ELMo
T T ... T 12N
... Lstm Lstm Lstm ... Lstm ... Lstm Lstm Lstm ... Lstm
...
                                             E
1
E
2
E
N
E
1
E
2
E
N
  BERT (Ours)
T T ... T 12N
Trm Trm ... Trm
Trm Trm ... Trm
...
                  E
1
 E
2
 E
N
Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.
to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.
Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples.
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
Label = IsNext
Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
A.2 Pre-training Procedure
To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as “sentences” even though they are typ- ically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the “next sentence pre- diction” task. They are sampled such that the com- bined length is ≤ 512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces.
We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40
epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.
Training of BERTBASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete.
Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.
A.3 Fine-tuning Procedure
For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:
• Batch size: 16, 32
13 https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html
 
• Learning rate (Adam): 5e-5, 3e-5, 2e-5 • Number of epochs: 2, 3, 4
We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.
A.4 Comparison of BERT, ELMo ,and OpenAI GPT
Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are fine- tuning approaches, while ELMo is a feature-based approach.
The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:
• GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).
• GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only in- troduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training.
• GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.
• GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.
To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.
A.5 Illustrations of Fine-tuning on Different Tasks
The illustration of fine-tuning BERT on different tasks can be seen in Figure 4. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classification out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.
B B.1
Detailed Experimental Setup
Detailed Descriptions for the GLUE Benchmark Experiments.
Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a):
MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classifi- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the first one.
QQP Quora Question Pairs is a binary classifi- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018).
QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classification task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.

   Class Label
C T ... T T T’ ... T’
Class Label
[CLS] [CLS]
                    1
Tok 1N1M
Sentence 1 Sentence 2
Start/End Span
...
N [SEP] 1
M
BERT
        E
[CLS]
E
1
E
N
E
[SEP]
E’ 1
...
E’ M
                           [CLS]
...
Tok
[SEP]
Tok
...
Tok
Tok 1
O
Tok 1
Tok 2 ...
Single Sentence
B-PER ...
Tok 2 ...
Single Sentence
Tok N
O
Tok N
                                        C T ... T T T’ ... T’
1
N [SEP] 1 M
... ...
C T1 T2 ... TN
    BERT
         E
[CLS]
E
1
E
N
E
[SEP]
E’ 1
E’ M
E
[CLS]
E
1
BERT
 E
2
...
E
N
                       [CLS]
Tok ...
[CLS]
Tok [SEP] Tok ... Tok 1N1M
Question Paragraph
        Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
SST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).
CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not (Warstadt et al., 2018).
STS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.
MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations
for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005).
RTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14
WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that’s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-
14Note that we only report single-task fine-tuning results in this paper. A multitask fine-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.
15 https://gluebenchmark.com/faq
C T1 T2   TN
...
      E
[CLS]
E
1
BERT
 E
2
...
E
N
 
jority class.
C Additional Ablation Studies
C.1 Effect of Number of Training Steps
Figure 5 presents MNLI Dev accuracy after fine- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions:
Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never ap- pears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based ap- proaches, as we expect the mismatch will be am- plified for the feature-based approach as the model will not have the chance to adjust the representa- tions.
Masking Rates Dev Set Results
MASK SAME RND MNLI NER
Fine-tune Fine-tune Feature-based
1.
2.
Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high fine-tuning accuracy?
Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps.
Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word?
Answer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.
Ablation for Different Masking Procedures
80% 100% 80% 80% 0% 0%
10% 10% 84.2 95.4 94.9 0% 0% 84.3 94.9 94.0 0% 20% 84.1 95.2 94.6
20% 0% 84.4 95.2 94.7 20% 80% 83.7 94.8 94.6 0% 100% 83.6 94.9 94.6
       C.2
Table8: Ablationoverdifferentmaskingstrategies.
The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.
The numbers in the left part of the table repre- sent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.
From the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.
In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.
         84 82 80 78 76
                   200 400 600 800
Pre-training Steps (Thousands)
1,000
BERTBASE (Masked LM) BERTBASE (Left-to-Right)
      Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k.
MNLI Dev Accuracy
</Text>
        </Document>
        <Document ID="FACD0E37-678A-4AD1-9779-CBDE56EDB804">
            <Title>딥러닝 3단계: 머신러닝 프로젝트 구조화하기 &gt; 다중 작업 학습(Multitask Learning) : edwith</Title>
            <Text>로그인 바로가기
하위 메뉴 바로가기
본문 바로가기
전체강좌 부스트코스 파트너
강좌만들기
 로그인 / 회원가입
딥러닝 3단계: 머신러닝 프로젝트 구조화하기
 커넥트재단  edwith 좋아요 51 수강생 1208 
http://www.edwith.org/deeplearningai3/lecture/34892/
강의목록
공지게시판
다중 작업 학습(Multitask Learning)

학습목표
다중 작업 학습을 배운다.
핵심키워드
다중 작업 학습 (multi-task learning)
저레벨 특성 (low-level feature)

Multitask Learning 원본보기
학습내용
다중 작업 학습은 하나의 신경망이 여러작업을 동시에 할 수 있도록 학습하는 것입니다.
이미지 다중 분류 학습은 신경망 초기 특성들은 여러 물체에서 공유가 가능하기 때문에, 하나의 신경망을 학습시키는 것이 여러 신경망을 개별 학습시키는 것 보다 효율적입니다.
다중 작업 학습은 아래의 상황에서 많이 쓰입니다.
여러 문제들의 하나의 저레벨 특성을 공유할 때
데이터가 비슷할때 (항상 만족하는 것은 아닙니다.)
거대한 작업 세트들을 하나의 큰 신경망으로 한번에 학습 시키려고 할 때
다중 작업 학습보다는 전이학습이 더 많이 쓰이고 있습니다.
공유하기    2 수강완료
전이학습(Transfer Learning)End-to-End Deep Learning 은 무엇인가요?
목록
댓글

이미지 첨부 파일 첨부  수식 비공개   저장
서비스 소개 도움말 제휴 문의 서비스 문의
한국어

 이용약관   개인정보처리방침
상호: 재단법인 커넥트소재지: 경기도 성남시 분당구 불정로 6 NAVER 그린팩토리 16층대표자명 : 조규찬사업자정보확인사업자 등록번호: 129-82-12249유선 번호: 1522-9182통신판매신고 번호: 제2015-경기성남-0754호고객센터: inquiries@edwith.org
구글플레이앱스토어
© CONNECT All Rights Reserved. Powered by NAVER
</Text>
        </Document>
        <Document ID="A4DA0DF3-3A08-4501-B2F8-F256AFDE0392">
            <Title>MHCflurry</Title>
            <Text>
#

MHCflurry[{ODonnell:2018fv}]는 펩타이드 결합 예측에서의 FP를 줄이기 위해 IEDB 결합데이터셋으로 학습한 모델을 MS elution 데이터셋을 사용하여 성능을 평가하여 높은 성능을 보이는 예측 모델들을 ensembl하여 최종 예측 모델을 구축하였다. 
MHCflurry only considers the natural ligands as peptides that also have high binding affinity to the MHCs, and thus it remains an affinity model that doesn’t consider other peptide features that influence peptide presentation. </Text>
        </Document>
        <Document ID="A69C96B8-4902-42F8-B1CF-586004916454">
            <Title>BERT-based 전이학습을 통해 데이터 수 한계 극복이 가능</Title>
            <Text>또한, BERT의 전이학습 접근 방식을 따라서, BA 데이터를 사용하여 선행 학습된 언어모델을 MS로 식별된 eluted ligand 데이터를 사용한 fine-tuning을 통하여 범용적이면서 동시에 정확한 reliable한 예측 모델을 구축해 낼 수 있을 것이다.
</Text>
            <Notes>

- 기존의 기계학습 또는 딥러닝 기반 신항원 예측 기술은 신뢰성 높은 예측 모델 구축을 위해 가공(레이블)된 많은 수의 학습데이터가 필요하나, 암 특이적 신항원 펩타이드 데이터 수가 적기 때문에 예측 정확도의 한계가 있음
- Allele-specific/Pan-specific 방법의 정화도/범용성에서의 Trade-off
- 본 발명은 이러한 신항원 펩타이드 데이터 수의 한계를 극복하기 위해 최근의 자연어 처리 분야에서 최고의 예측성능을 보이고 있는 양방향 언어모델을 기반으로 기존에 누적된 펩타이드 결합 데이터를 사용하여 선행 학습된 펩타이드 언어 모델(Pre-trained Peptide Language Model)을 구축하고 한국인 종양 특이적 신항원 펩타이드 데이터를 사용한 미세튜닝(Fine-tuning)을 통해 정확도 높은 신항원 펩타이드 예측 모델을 구축하는 기술임 
</Notes>
        </Document>
        <Document ID="135DBF03-0E4F-46B1-A343-3BC09A032B78">
            <Title>Abelin2017</Title>
            <Text>MS-identified peptides eluted from MHC class I
 - {Abelin:2017cn}: Abelin, J. G. et al. Mass Spectrometry Profiling of HLA-Associated Peptidomes in Mono-allelic Cells Enables More Accurate Epitope Prediction. Immunity 46, 315–326 (2017).

 - MS identified HLA-bound peptides; presented on the cell
 - HLA-bound peptides can be directly identified via immunopurification and LC-MS/MS. We processed class I HLA-deficient cell lines (30 million–90 million B721.221-derived cells), each stably transduced to express one of 16 different class I HLA alleles
 - Data: /Users/hym/projects/nplm/.data/mhcflurry/curated_training_data.with_mass_spec.csv</Text>
        </Document>
        <Document ID="F25E8FE3-01A8-4ADB-9DDC-D28576DC73AF">
            <Title>Pre-training datasets</Title>
            <Notes># Binding affinity datasets
The affinity measurement dataset used for training and model selection was assembled from a snapshot of the Immune Epitope Database (IEDB) MHC ligands downloaded on Dec. 1, 2017 augmented with the BD2013 dataset (Kim et al., 2014). IEDB entries with non-class I, non-specific, mutant, or unparseable allele names were dropped, as were those with peptides identified by MS or containing post-translational modifications or noncanonical amino acids. This yielded an IEDB dataset of 143,898 quantitative and 43,978 qualitative affinity measurements. Of 179,692 measurements in the BD2013 dataset (Kim et al., 2014), 57,506 were not also present in the IEDB dataset. After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the combined dataset consists of 230,735 measurements across 130 alleles. 
# Output value of binding prediction model and Loss
As in the NetMHC tools, MHCflurry internally transforms binding affinities to values between 0.0 and 1.0, where 0.0 is a non-binder and 1.0 is a strong binder. The neural networks are trained using the transformed values and the inverse transform is used to return prediction results as nanomolar affinities. The transform is given by 1 log50000(x) where x is the nanomolar affinity. Affinities are capped at 50,000 nM.
 Training is attempted for all alleles with at least 25 affinity measurements. Ten percent of the training data is set aside for model selection. Each neural network is trained on a different 90% sample of the remaining data, with the other 10% used as a test set for early stopping. Training proceeds with the RMSprop optimizer using a minibatch size of 128 until the accuracy on the test set has not improved for 20 epochs. At each epoch, 25 synthetic negative peptides for each length 8–15 are randomly generated. These random negative peptides are sampled so as to have the same amino acid distribution as the training peptides and are assigned affinities &gt;20,000 nM. For the MHCflurry (train-MS) variant, the number of random peptides for each length is 0.2n + 25 where n is the number of training peptides. 
A modified mean squared error (MSE) loss function that supports data with inequalities is used for both the training loss and test set accuracy metric. For this loss function, measurements are associated with an inequality: (&lt;), (&gt;), or (=). The loss L is defined as: 
￼

where n is the total number of measurements, and ybi and yi are the predicted and measured values for measurement i, respectively. Quantitative affinity data is associated with an inequality of (=). For qualitative affinity data, we assigned the following inequalities and measurement values: positive-high, &lt; 100 nM; positive, &lt; 500 nM, positive-intermediate, &lt; 1,000 nM; positive-low, &lt; 5,000 nM; negative, &gt; 5,000 nM. In the MHCflurry (train-MS) variant, MS-identified ligands are assigned the value ‘‘&lt; 500 nM.’’ 
</Notes>
        </Document>
        <Document ID="BC47ED2C-FDA7-4D0E-ACC8-067D2B69AE86">
            <Title>Independent validation on external datassets</Title>
            <Text>Fine-tuned 된 최종 모델은 11 patient-derived tumor cell lines으로부터 detected ligands를 포함한external dataset을 사용하여 independently evaluated 되었다. 
The standalone version of NetMHCpan4.0 was downloaded from http://www.cbs.dtu.dk/services/NetMHCpan/. MHCflurry version 1.2.2 was installed as instructed on https://github.com/openvax/ mhcflurry and the pre-trained ‘models_class1_trained_with_mass_s- pec’ model (https://github.com/openvax/mhcflurry/releases/down load/pre-1.2.1/models _class1_trained_with_mass_spec.20180228. tar.bz2) was downloaded by ‘mhcflurry-downloads fetch model- s_class1_rained_with_mass_spec’. </Text>
        </Document>
        <Document ID="E5480327-0D44-468F-81EF-780EAE5982A4">
            <Title>Exploring datasets</Title>
        </Document>
        <Document ID="FAE4035D-2240-4160-A635-5AFC41FCC887">
            <Title>BassaniSternberg2016</Title>
            <Text>- {BassaniSternberg:2016kt}: Bassani-Sternberg, M. et al. Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry. Nat Commun 7, 185–16 (2016).
- Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry
- Data: https://www.nature.com/articles/ncomms13404#Sec32</Text>
        </Document>
        <Document ID="1AEBABF5-F65C-436C-BA17-638A66B51D0C">
            <Title>Source</Title>
            <Text>- The dataset for pretraining the model consisted of BA measurement data and MS-identified EL ligand data, excluding tumor-associated neopeptides used in fine-tuning the model.
- The binding affinity measurement dataset for pretraining the model was compiled from a snapshot of the Immune Epitope Database (IEDB)[{Vita:2015bh}] MHC ligands downloaded on Jan. 28, 2019(http://www.iedb.org/doc/mhc_ligand_full.zip) and the BA dataset augmented with the BD2013 dataset[{Kim:2014jg}](http://tools.iedb.org/static/main/binding_data_2013.zip).
- The MS-identified EL dataset  for pretraining the model consisted of the MS-identified ligands from IEDB, SysteMHC Atlas [{Shao:2017hb}] and 186,464 eluted peptides from 95 HLA-A, —B, -C and -G alleles[{Sarkizova:2019fu}].  </Text>
            <Notes>The affinity measurement dataset used for training and model selection was assembled from a snapshot of the Immune Epitope Database (IEDB) MHC ligands downloaded on Dec. 1, 2017 augmented with the BD2013 dataset (Kim et al., 2014).

Here, we expand our initial dataset of &gt;24,000 peptides from 16 cell lines and identify and characterize 186,464 eluted peptides from 95 HLA-A, -B, -C and -G alleles. We included HLA-G peptidomes because this HLA is impli- cated in maternal–fetal tolerance and is also upregulated in many cancers10,11. These data allow us to compare peptide length pref- erences and the spectrum of distinct and shared submotifs across HLA class I alleles, revealing the diversity and complexity of endog- enous HLA ligands. Using this information, we trained allele- and-length-specific and pan-allele-pan-length predictors, which identify 1.5-fold more peptides than conventional prediction tools when evaluating ligands directly detected by LC–MS/MS from 11 patient-derived tumor cell lines. The datasets of HLA binding pep- tides from mono-allelic cells and patient-derived tumors, as well as the prediction models (HLAthena) and interactive web tools, are all made publicly available. 

 </Notes>
        </Document>
        <Document ID="5F1DFC67-3910-462D-8AD4-E971113A7409">
            <Title>양방향 자기주목을 통한 pMHC 결합패턴 학습</Title>
            <Text>양방향 자기주목 기반의 언어모델인 BERT를 도용함으로써, 펩타이드-MHC 결합과 펩타이드 제시에서의 아미노산 간의 상호작용 패턴- 장단거리 상호작용, 집합적 상호작용, 일반적 상호작용 등-을 펩타이드 길이와 상관없이 효과적으로 반영하는 contextual representation(language model)을 사전 훈련 시킬 수 있다.
</Text>
        </Document>
        <Document ID="F0504B0E-A123-437F-80A5-4566C5C7925B">
            <Title>Fine-tuning</Title>
        </Document>
        <Document ID="FB3AE06C-29B3-4579-AEC2-24B6B5E10875">
            <Title>The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time</Title>
            <Text>
Jay Alammar
Visualizing machine learning one concept at a time
Blog About
The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)

Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) 
Translations: Chinese (Simplified), Persian

The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).


(ULM-FiT has nothing to do with Cookie Monster. But I couldn’t think of anything else..)

One of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component – saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch.


The two steps of how BERT is developed. You can download the model pre-trained in step 1 (trained on un-annotated data), and only worry about fine-tuning it for step 2. [Source for book icon].
BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).

There are a number of concepts one needs to be aware of to properly wrap one’s head around what BERT is. So let’s start by looking at ways you can use BERT before looking at the concepts involved in the model itself.

Example: Sentence Classification

The most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:



To train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT.

For people not versed in the topic, since we’re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a labele (“spam” or “not spam” for each message).


Other examples for such a use-case include:

Sentiment analysis
Input: Movie/Product review. Output: is the review positive or negative?
Example dataset: SST
Fact-checking
Input: sentence. Output: “Claim” or “Not Claim”
More ambitious/futuristic example:
Input: Claim sentence. Output: “True” or “False”
Full Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, by with ML later, hopefully).
Video: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.
Model Architecture

Now that you have an example use-case in your head for how BERT can be used, let’s take a closer look at how it works.



The paper presents two model sizes for BERT:

BERT BASE – Comparable in size to the OpenAI Transformer in order to compare performance
BERT LARGE – A ridiculously huge model which achieved the state of the art results reported in the paper
BERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model – a foundational concept for BERT and the concepts we’ll discuss next.



Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).

Model Inputs



The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.

Just like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.



In terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge.

Model Outputs

Each position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).



That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.



If you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.

Parallels with Convolutional Nets

For those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network.



A New Age of Embedding

These new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let’s recap how those are used before pointing to what has now changed.

Word Embedding Recap

For words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).

The field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)


The GloVe word embedding of the word "stick" - a vector of 200 floats (rounded to two decimals). It goes on for two hundred values.
Since these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:

 
ELMo: Context Matters

If we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born.


Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams
Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.


ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.

What’s ELMo’s secret?

ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.


A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.

We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done.

ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.


Great slides on ELMo
ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).


ULM-FiT: Nailing down Transfer Learning in NLP

ULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.

NLP finally had a way to do transfer learning probably as well as Computer Vision could.

The Transformer: Going beyond LSTMs

The release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs.

The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).

OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling

It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.


The OpenAI Transformer is made up of the decoder stack from the Transformer
The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).

With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.


The OpenAI Transformer is now ready to be trained to predict the next word on a dataset made up of 7,000 books.
Transfer Learning to Downstream Tasks

Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):


How to use a pre-trained OpenAI transformer to do sentence clasification
The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.


Isn’t that clever?

BERT: From Decoders to Encoders

The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?

“Hold my beer”, said R-rated BERT.

Masked Language Model

“We’ll use transformer encoders”, said BERT.

“This is madness”, replied Ernie, “Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.”

“We’ll use masks”, said BERT confidently.


BERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.
Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).

Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.

Two-sentence Tasks

If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).

To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?


The second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT actually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.
Task specific-Models

The BERT paper shows a number of ways to use BERT for different tasks.


BERT for feature extraction

The fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.


Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):


Take BERT out for a spin

The best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.

The next step would be to look at the code in the BERT repo:

The model is constructed in modeling.py (class BertModel) and is pretty much identical to a vanilla Transformer encoder.
run_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file.

Several pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.

BERT doesn’t look at words as tokens. Rather, it looks at WordPieces. tokenization.py is the tokenizer that would turns your words into wordPieces appropriate for BERT.
You can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model.

Acknowledgements

Thanks to Jacob Devlin, Matt Gardner, Kenton Lee, Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post.

Written on December 3, 2018
Subscribe to get notified about upcoming posts by email

Email Address




This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 
Attribution example: 
Alammar, Jay (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/ 

Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.
  </Text>
        </Document>
        <Document ID="88903A84-890C-4361-B7C3-9EE59018EC3E">
            <Title>BassaniSternberg2015</Title>
            <Text>Mass spectrometry of HLA-I peptidomes
 - {BassaniSternberg:2015js}: Bassani-Sternberg, M. &amp; Pletscher-Frankild, S. Mass Spectrometry of Human Leukocyte Antigen Class I Peptidomes Reveals Strong Effects of Protein Abundance and Turnover on Antigen Presentation. Molecular &amp; Cellular … 14, 658–673 (2015).
 - We here present a rich and high confidence HLA-I peptidome, established by applying state-of-the-art mass-spectrometric techniques on a collection of seven cell lines. We investigate how abundance affects the propensity of proteins to be presented as measurable HLA peptides and whether or not there are specific protein classes that are overrepresented even independent of abundance. Likewise, we explore how to use in silico immunogenicity tools on the set of identified HLA peptides from cancer-associated proteins, with a view to select vaccine candidates.
#

- Data: https://www.mcponline.org/content/suppl/2015/01/09/M114.042812.DC1</Text>
        </Document>
        <Document ID="276EF430-6B2B-4746-86BD-CDCB7B06F16F">
            <Title>An Overview of Multi-Task Learning in Deep Neural Networks∗ – TensorMSA</Title>
            <Text>
TENSORMSA
TOGGLE NAVIGATION
AN OVERVIEW OF MULTI-TASK LEARNING IN DEEP NEURAL NETWORKS∗
BY TMDDNO1@NAVER.COM | AUGUST 28, 2018 | NO COMMENTS | PAPER STUDY
An Overview of Multi-Task Learning in Deep Neural Networks (Paper, Blog)
1. 서론

보통 우리가 머신러닝으로 무언가를 할때 통상적으로 하나의 모델을 훈련하거나 복수의 모델을 훈련하여 Ensemble 하는 형태로 우리가 추구하고자 하는 목표를 잘 해석할 수 있는 모델을 만들고자 노력한다. 그 이후에는 Fine 튜닝 등의 방법으로 성능을 끌어올리고자 노력하다가 더 이상 모델의 성능의 개선되지 않으면 모델 개발을 완료하는 형태로 진행한다. 이러한 방법으로도 어지간한 경우에는 원하는 성능을 달성할 수 있다. 하지만 이렇게만 모델 개발을 종료하게 되면 어쩌면 우리의 모델의 도움이 되었을 수도 있는 연관된 데이터들을 활용할 수가 없다. 이러한 문제를 해결하고자 Multi Tasking 이라는 방법이 연구되었으며, 이 논문에서는 몇 가지 대표적인 Multi Tasking 방법과 응용 방법을 소개하고자 한다.

2. 대표적인 두 가지 방법

(1) Hard Parameter Sharing 

Hard Parameter Sharing 은 가장 흔히 사용되는 방법으로 Hidden Layer 를 공유하고 Task 별로 일부 개별적인 Layer 를 가지고 가는 형태로 활용된다. 이러한 방법의 활용은 당연히 Over Fitting 을 방지하는데 효과가 있다. (N 개의 Task 에 전부 Over Fit 되기는 어려움으로.. )



(2) Soft Parameter Sharing 

Soft Parameter Sharing 는 조금 다르게 각각의 Task 별로 별도의 Layer 를 가지고 있다. 다만, 각각의 Layer 가 비슷해 질 수 있도록 L2 Distance 를 사용한다. (아마도 LOSS 함수 구성시 Classification Loss 와 각각의 Layer 간의 거리를 최소화 하는 Loss 를 조합하는 형태일 것이라고 예상함)



3. Multi Task Learning 이 동작하는 이유 

(1) Implicit Data Augmentation  

어짜피 딥러닝은 High Dimension Data 를 Deep Learning 을 통해 Low Dimension 에서 Representation 하는 것에 목적이 있는데 , 특정한 데이터 셋에 종속적으로 모델을 훈련하는 것보다 다양한 데이터를 활용하여 더욱더 범용적인 Representation 을 만들어 낼 수 있다면, Over fitting 을 회피할 수 있다.

(2) Attention Focusing   

Task 가 만약 매우 지저분하거나 데이터가 제한적인 경우 모델이 관련이 있는 것과 관련이 없는 것을 구분하는 것이 쉽지 않을 것이다. 이러한 경우에 MTL 기법은 모델에 관련있는 것과 관련 없는 것을 구분하기 위한 추가적인 정보를 제공하여 줄 것이다.

(3) Eavesdropping  

어떤 Feature G 가 있다고 하자, Task A 에서는 이러한 Feature 를 학습하게 어려운데 Task B 에서는 학습하기가 용이하다고 하자, 이러한 문제를 해결하기에 가장 쉬운 방법은 Task B 를 통해서 Feature G 를 학습하기에 중요한 포인트를 전달 받아서 Task A 를 훈련하는 것일 것이다.

(4) Representation Bias   

하나의 Task 뿐만 아니라 다른 Task 에서도 선호되는 Representation 을 만들 수 있도록 Bias 를 줄 수 있다. 이를 통해서 모델의 Generalization 을 달성하는데 도움을 줄 수 있다.

(5) Regularization    

MTL 은 Regularizer 의 역할도 수행을 하는데, 귀납적인 Bias 를 제공하여 준다. 결론적으로 Over fitting 의 위험등을 감소 시킬 수 있다.

4. 최근 MTL 에 관한 연구  

(1) “Learning Multiple Tasks with Deep Relationship Networks” NIPS 2017 (링크)

Pretrained 된 CNN(Alex Net) 을 활용하여 Fine Tune 개념으로 CNN Layer 를 활용하며, Multi Tasking 까지 적용하는 형태로 뒤의 FCC 레이어는 각각의 Task 별로 존재하는 형태로 자세한 사항은 직접 논문을 참조하기를 바란다.



(2) Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in
Person Attribute Classification (링크)

Bottom up 방식으로  Thin 아키택쳐로 시작해서 복잡한 아키택쳐로 확대해 가는 형태로 훈련을 진행하면서 Dynamic 하게 Branch 를 추가해 가는 형태를 제안하고 있는다.

 

(3) Cross-stitch Networks for Multi-task Learning (링크)

일반적인 Soft Parameter Sharing 과 비슷한 형태로 구성되어 Shared CNN Layer 에 대해서 각 Task 별 별도의 Hidden Layer 를 구성하고 각 Layer 간의 거리를 최소화하는 방향으로 훈련하는 개념으로 구성되어 있다. 다만, Cross-Stitch 라는 개념이 추가되어 있는데, Task A 와 Task B 가 있다고 했을 때,  각 Task 에서 다음 Layer 의 Input 을 계산할 때, 아래와 같이 서로간에 Linear 한 관계로 값을 Combine 하는 구조가 추가되어 있다.





(4) A Joint Many-Task Model: Growing a NN for Multiple NLP Tasks (링크)

NLP 에는 POS, Chunking, Dependency Parsing, entailment 등 다양한 Task 들이 존재하는데,  보통은 각각의 목적별로 별도의 아키택쳐를 구성하지만, 여기에서는 모든 Task 를 하나의 아키택쳐로 구성하고, 훈련하고자 하는 대상별로 다른 LOSS Function 을 구성하여 하나의 아키택쳐에서 같이 훈련시키고 있다.

 



 

 

(5) Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics (링크)

Share 하는 Layer 을 훈련하는 형태가 아닌 복수의 목적을 갖는 Loss Function 을 설계하고 이를 Sum 하여 한번에 훈련하는 형태를 제시하고 있다.



(3) Learning what to share between loosely related tasks (링크)

지금까지 나온 Hard Parameter Sharing, Cross-Stitch , NLP의 Task Hierarchy 등 다양한 기법들을 복합적으로 적용한, MTL 아키택쳐이다.



5. Examples with Tensorflow   

(1) Linear Transformation 

간단한 Linear Regression 을 Tensorflow 로 구현한 모습이다. MTL 을 설명하기 전에 가장 간단한 구조를 한번 설명하고 있다.



&lt;code class="language-python" data-lang="python"&gt;# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np
# ======================
# Define the Graph
# ======================
# Create Placeholders For X And Y (for feeding in data)
X = tf.placeholder("float",[10, 10],name="X") # Our input is 10x10
Y = tf.placeholder("float", [10, 1],name="Y") # Our output is 10x1
# Create a Trainable Variable, "W", our weights for the linear transformation
initial_W = np.zeros((10,1))
W = tf.Variable(initial_W, name="W", dtype="float32")
# Define Your Loss Function
Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name="Loss")
with tf.Session() as sess: # set up the session
    sess.run(tf.initialize_all_variables())
    Model_Loss = sess.run(
                Loss, # the first argument is the name of the Tensorflow variabl you want to return
                { # the second argument is the data for the placeholders
                  X: np.random.rand(10,10),
                  Y: np.random.rand(10).reshape(-1,1)
                })
    print(Model_Loss)&lt;/code class="language-python" data-lang="python"&gt;
(2) Simple Hard Parameter Sharing 

Hard Parameter Sharing 의 간단한 예가 되겠다. 공유되는 Shared Layer 를 가지고 있는 상태에서 각각의 Task 가 별도의 Layer 를 가지고 있으며, 두개의 Task 는 각각 다른 X,Y Set 으로 Loss 구해서 BackPropagation 을 진행하고 있다.



#  GRAPH CODE
# ============
# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np
# ======================
# Define the Graph
# ======================
# Define the Placeholders
X = tf.placeholder("float", [10, 10], name="X")
Y1 = tf.placeholder("float", [10, 20], name="Y1")
Y2 = tf.placeholder("float", [10, 20], name="Y2")
# Define the weights for the layers
initial_shared_layer_weights = np.random.rand(10,20)
initial_Y1_layer_weights = np.random.rand(20,20)
initial_Y2_layer_weights = np.random.rand(20,20)
shared_layer_weights = tf.Variable(initial_shared_layer_weights, name="share_W", dtype="float32")
Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name="share_Y1", dtype="float32")
Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name="share_Y2", dtype="float32")
# Construct the Layers with RELU Activations
shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))
Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))
Y2_layer = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))
# Calculate Loss
Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)
Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)
# optimisers
Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)
Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss)
(3) Joint Loss 

각각 따로 따로 모델을 훈련하는 것이 아닌 Joint Loss 를 활용하여 훈련하는 예제이다



#  GRAPH CODE
# ============
# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np
# ======================
# Define the Graph
# ======================
# Define the Placeholders
X = tf.placeholder("float", [10, 10], name="X")
Y1 = tf.placeholder("float", [10, 20], name="Y1")
Y2 = tf.placeholder("float", [10, 20], name="Y2")
# Define the weights for the layers
initial_shared_layer_weights = np.random.rand(10,20)
initial_Y1_layer_weights = np.random.rand(20,20)
initial_Y2_layer_weights = np.random.rand(20,20)
shared_layer_weights = tf.Variable(initial_shared_layer_weights, name="share_W", dtype="float32")
Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name="share_Y1", dtype="float32")
Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name="share_Y2", dtype="float32")
# Construct the Layers with RELU Activations
shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))
Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))
Y2_layer = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))
# Calculate Loss
Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)
Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)
Joint_Loss = Y1_Loss + Y2_Loss
# optimisers
Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)
Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)
Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss)
# Joint Training
# Calculation (Session) Code
# ==========================
# open the session
with tf.Session() as session:
    session.run(tf.initialize_all_variables())
    _, Joint_Loss = session.run([Optimiser, Joint_Loss],
                    {
                      X: np.random.rand(10,10)*10,
                      Y1: np.random.rand(10,20)*10,
                      Y2: np.random.rand(10,20)*10
                      })
    print(Joint_Loss)
6. Real Examples    

(1) VoC Example  

고객의 STT 데이터를 가지고 현업에서 실제 사용할 모델을 만들 때 응용했던 결과를 보여주고 있다. 현업에서의 문제는 Inbound Call 이 들어올 때, 고객이 선택한 카테고리 정보 외에 우리가 분류하고 싶은 형태의 정답지(Labeled Data)가 없거나 매우 적다는 것이 문제다. 이러한 문제를 해결하기 위해서 Transfer Learning, Multi Tasking, Learning by Association 3가지 기법을 적용한 결과이다. 전체적으로 데이터가 적어서 발생하는 Over Fitting 을 회피하기 위한 기법들이 위주로 적용되었으며, 아래와 같이 Walker Loss, Classification Loss 와 더불어 우리가 목적하였던 고객의 감정 분석 모델의 Loss 와 Accuracy 도 향상됨을 볼 수 있다. 실제 테스트 결과 완전 별도로 작업한 Test Set 1,000건 기준으로 79.5 F1 Score 를 달성 하였다.



Post navigation
← SCORING
WAVENET: A GENERATIVE MODEL FOR RAW AUDIO  →
LEAVE A REPLY

Your email address will not be published. Required fields are marked *

Comment


Name *


Email *


Website


 Notify me of follow-up comments by email.

 Notify me of new posts by email.




BLOG STATS

180,269 hits
ShopIsle powered by WordPress

</Text>
        </Document>
        <Document ID="E2DAB33D-D631-4965-AF2C-57DD005088AF">
            <Title>Binding Affinity Prediction</Title>
            <Text>In training the binding affinity prediction model with mean square error loss, the pMHC-I binding affinity values were transformed using 1-log(a)/log(50,000), where a is the measured binding affinity to scale between 0 and 1[Nielsen, 2003]. </Text>
            <Notes>1.	Nielsen, M. et al. Reliable prediction of T-cell epitopes using neural networks with novel sequence representations. Protein Sci. 12, 1007–1017 (2003).

the output values used in the training and testing of the neural networks on a scale between 0 and 1. The transformation is defined as 1-log(a)/log(50,000), where a is the measured binding affinity. In this transformation high binding peptides, with a mea- sured affinity stronger than 50 nM, are assigned an output value above 0.638, intermediate binding peptides, with an affinity stron- ger than 500 nM, an output value above 0.426, and peptides, with an affinity weaker than 500 nM, an output value below 0.426. Peptides that have an affinity weaker than 50,000 nM are assigned an output value of 0.0.</Notes>
        </Document>
        <Document ID="AE43CA13-604F-4FE8-BCA5-A78AD76A79D6">
            <Title>말하고자 하는 연구내용</Title>
            <Text>본 논문에서는  MHC-bound 후보 (neo-)peptide의 예측을 위한 BERT를 도용한 전이학습(semi-supervised?) 방법을 제안한다. 
MHC-펩타이드 binding affinity 데이터를 사용한 선행학습을 통해 MHC-peptide의 결합에 대한 contextual language model을 construct한다. The pre-trained model을 특정 MS-identified natural ligand 데이터셋을 사용한 fine-tuning을 통해 최종 (Neo-) 펩타이드 예측 모델을 구축하고 cross-validation을 통해 성능을 검증한다. 또한, final predictive 모델을 external dataset을 사용하여 독립 성능 검증을 수행하고 다른 예측 방법들 보다 범용성과 정확도가 우수하다는 것을 증명한다.
 


</Text>
            <Notes>￼


1)  펩타이드-주조직적합성복합체(MHC: Major Histocompatibility Complex) 결합데이터셋(IEDB 결합데이터, Mass Spectrometry 펩타이드 데이터 등)으로 부터 펩타이드 아미노산 서열과 MHC 아미노산 서열을 20개 아미노산이 단어로 표현되는 문장으로 변환하여 펩타이드-MHC 결합 사전(Peptide Corpus)를 구축한다.
2) 펩타이드-MHC 결합데이터에 대응하는 각 펩타이드 문장(Peptide Sentence) 데이터를 사용하여 양방향 언어모델(BERT: Bidirectional Encoder Representations from Transformer)을 구축하고 선행 학습을 통해 펩타이드-MHC 결합 언어 모델(Peptide Language Model)을 구축한다. 이 때 언어모델의 범용성(generality)을 위해 입력  펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 삭제 또는 치환하여 선행학습을 수행한다. 펩타이드와 MHC 서열에서의 아미노산 삭제 또는 치환은 임의로 특정 비율의 서열 단편에 대해 수행하되 펩타이드 서열의 경우는 알려진 위치특이적 점수행렬(PSSM: Position-Specific Scoring Matrix)에 기반한 편향성을 주고, MHC 서열의 경우는 MHC 결합사이트의 진화적 보존성에 기반한 편향성을 부여한다. 
3) 특정 인종 또는 지역에 집중된 HLA 유전형에 대한 신항원 펩타이드-MHC 결합데이터셋을 사용한 선행학습된 펩타이드-MHC 결합 언어모델의 미세튜닝을 통하여 최종  신항원 펩타이드-MHC 결합 언어모델을 구축한다


Neopeptide language model에게 입력 펩타이드-MHC 서열들을 선행 학습시킬 때 prior knowledge를 가미하기 위해  펩타이드 와 MHC 서열 문장의 일부 단어(아미노산)를 임의로 치환한 서열들을 추가로 학습시킨다. 


펩타이드-MHC 결합에서의 서열을 20개 아미노산이 단어로 표현되는 문장으로 인코딩하여 </Notes>
        </Document>
        <Document ID="8435D019-B9B9-4615-8BDC-F7783789006D">
            <Title>ML/DL using MS peptidome data</Title>
            <Text>NetMHCpan4.0
MHCflurry 1.2.0
DeepLigand
</Text>
        </Document>
        <Document ID="F95868B2-CAE7-4810-86D9-D5068D18847B">
            <Title>Model Architecture</Title>
            <Text>Predictive model의 구조는 BERT base model을 참조하여 구성되었다[{Devlin:2018uk}]. 
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M). 

</Text>
            <Notes>We will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017)[{Vaswani:2017ul}] as well as excellent guides such as “The Annotated Transformer.”[https://nlp.seas.harvard.edu/2018/04/03/attention.html]
In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). 
BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4 

</Notes>
        </Document>
        <Document ID="4429E692-D772-4F9B-BBCA-D39AC2CA5791">
            <Title>Independent validation results</Title>
            <Text>Fine-tuned 된 최종 모델은 11 patient-derived tumor cell lines으로부터 detected ligands를 포함한external dataset을 사용하여 independently evaluated 되었다. 
이 데이터셋을 사용한 NetMHCpan, MHCflurry와의 예측 성능 비교를 통하여 fine-tuned 최종 예측 모델의 독립 검증을 수행하였다. 
&lt;성능비교 Table 1&gt;
Table1에서 전체적인 성능은 our model이 좋았다. 특히, HLA-A*31:01 alllele에 대한 예측 성능은 다른 방법들보다 outperformed하였다.</Text>
        </Document>
        <Document ID="BC3FDD75-96C6-4303-93EA-8C9D155FD44D">
            <Title>Immunogenic T-cell epitope 예측의 중요성</Title>
            <Text>바이러스에서 유래한 펩타이드의 약 1 % 미만이 면역 반응을 유도하기에 충분한 강도로 MHC 클래스 I 분자에 결합 할 것으로 추정되었다.[{Yewdell:uq}]. 펩타이드의 MHC 분자에 대한 결합 특이성은 MHC 분자의 residue들의 분자 구조와 물리화학적 구조에 의해 결정된다. MHC 단백질을 인코딩하는 유전자는 매우 높은 다형성을 갖고 있다. 인간 MHC, 즉 huma n leukocyte antigen(HLA) 단백질은 에 대해 3개의 HLA-I 형 주요 유전좌(HLA-A, -B, 그리고 -C)와 9개의 HLA-II 형 주요 유전자((HLA-DPA1, -DPB1, -DQA1, -DQB1, -DRA, -DRB1, -DRB3, -DRB4, and -DRB5)로부터 인코딩 된다. 현재 IMGT/HLA 데이터베이스(https://www.ebi.ac.uk/ipd/imgt/hla/stats.html)에 등록된  HLA-I 유전형은 250,597개, HLA-II 유전형은 7,723개  등록되어 있다[{Robinson:2015ca}]. 
또한 MHC 분자와 결합하는 펩타이드 중  5% 미만만이 T cell의 면역반응을 유도할 수 true immunogenic epitope이며[있다[{Vitiello:2017dm}, {Ott:2017ft}, {Wells:2020br}] 면역원성의 펩타이드를 선별하기 위해서는 MHC와 결합한 이후 수반되는 TAP transport와 ERAP trimming 등의 항원 processing 과정({Tenzer:2005bt}과 펩타이드-MHC 복합체의 안정성{Harndahl:2012jj}, 그리고 T-Cell receptor와의 상호작용 등을 고려해야만 한다[{Dash:2017go}, {Gfeller:2018em}, {Ogishi:2019en}, {Fischer:2020hv},{Wells:2020br}].
이렇게 HLA 유전형별 펩타이드의 결합 특이성이 다르고 매우 적은 수의 펩타이드 만이 T-cell 면역반응 유도할 수 있는 true immunogenic epitope이기 때문에 실험적으로 T cell epitope를 식별하는 것은 시간과 비용 측면에서 매우 어려운 일이다. 이런 까닭에 컴퓨터 프로그램을 사용하여 잠재적인 T cell 에피토프를 예측하는 다양한 방법들이 개발되어 왔다[{Peters:2020jn}] 

It has been estimated that less than about 1% of peptides derived from the virus will bind to MHC class I molecules with sufficient strength to elicit an immune response. The peptide binding specificity is determined by the molecular and physicochemical properties of the MHC residues. The genes encoding MHC proteins are highly polymorphic. The human MHC, human leukocyte antigen(HLA), proteins are encoded from three major loci (HLA-A, -B, and -C) for HLA class I and nine major loci(HLA-DPA1, -DPB1, -DQA1, -DQB1, -DRA, -DRB1, -DRB3, -DRB4, and -DRB5) for HLA class II. Currently, 20,597 alleles for HLA-I and 7,723 alleles for HLA- II are registered in the IMGT/HLA database [[{Robinson:2015ca}]] (https://www.ebi.ac.uk/ipd/imgt/hla/stats.html). Since the binding specificity of peptides is different for each HLA allele and only a very small portion of peptides can mount an T cell immune response,  the experimental identification of immunogenic T cell epitope is time-consuming and laborious. Therefore, computer-assisted epitope predictions can be a cost-effective and practical alternative; many methods have been developed[{Peters:2020jn}].
</Text>
        </Document>
        <Document ID="77D2C7EC-771E-4AFA-92A2-46597EFD9F8F">
            <Title>자연어처리에서의 언어모델 발전과정</Title>
            <Text>N-gram 통계기반
딥러닝 기반 word embedding
RNN-based RM, Attention mechanism
Transformer

</Text>
        </Document>
        <Document ID="7574218D-766E-40BB-A6A5-45FA31BA9A98">
            <Title>IEDB binding affinity</Title>
            <Text> -  {Vita:2015bh} : Vita, R. et al. The immune epitope database (IEDB) 3.0. Nucleic Acids Research 43, D405–12 (2015).

 - 정성: positive-high, positive, positive-intermediate, positive-low, negative, 정량: nM
 - Data: http://www.iedb.org/doc/mhc_ligand_full.zip 
</Text>
        </Document>
        <Document ID="11E2714D-70B1-4ED2-8E9C-68CA5123FC7B">
            <Title>Datasets</Title>
            <Text>{Devlin:2018uk}은 BooksCorpus (800M words)와 English Wikipedia (2,500M words) 같은  대용량의 Corpus의 텍스트를 사용하여 unsupervised한 manner로 pretraining을 수행하여 corpus에서의 contextual 한 word representations(embedding vector로 구현되는)를 통한 언어모델을 구축한 후 이를 11개의 NLP task에서의 specific한 task dataset을 사용하여 fine-tuning을 수행하여 SOTA 성능을 보이는 최종 예측 모델을 구축하였다. 

MHC와 결합하는 모든 펩타이드가 Tumor-specific 신항원 펩타이드는 아니다. [{Biotechnol:fe}]. MHC 분자와 결합하는 펩타이드 중에 오직 30% 미만이 tumor cell에서 process 되고 이중 일부만이 T-cell을 유도하고 clinically-relevant immunogenicity를 갖는 neopeptide 일 수 있다[{Vitiello:2017dm}, {Ott:2017ft}].
따라서, BERT에서의 pretraining에서 fine-tuning에 걸친 전이학습 전략을 employing하면, 먼저 최대한 많은 수의 펩타이드-MHC 결합데이터를 사용하여 펩타이드-MHC 결합에 대한 언어모델을 pre-training한 후, pretrained 언어 모델을 HLA allele 별로 tumor-specific한 신항원 펩타이드 결합데이터셋을 사용하여 fine-tuning을 수행하여 최종 학습 모델을 구축한다.
 
</Text>
            <Notes>NetMHCPan4.0
One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this binding to MHC is the most selective step in peptide Ag presentation. However, other factors, including Ag processing (12) and the sta- bility of the peptide–MHC complex (13), could influence the likelihood of a given peptide to be presented as an MHC ligand. Similarly, the length distribution of peptides available for binding to MHC molecules is impacted by other steps in the processing and presentation pathway, such as TAP transport and ERAP trimming, which are not reflected in binding data in itself (6). 

MHCflurry

We used the MHC class I dataset curated by O’Donnell et al. (2018). This dataset consists of 525 672 binding affinity and mass spectrometry measurements collected from IEDB, Abelin et al. (2017) and Kim et al. (2014). This dataset also contains 2 541 370 non-ligand sequences (decoys) sampled from the protein-coding transcripts that also contained the mass spectrometry-identified pep- tides (hits) based on protein sequences in the UCSC hg19 proteome and transcript quantification from RNA sequencing of the relevant cell line (B721.221). As described in Abelin et al., for an allele with n hits, 100n decoys of length 8–15 were sampled, weighting tran- scripts by the number of hits. 

</Notes>
        </Document>
        <Document ID="4E75867E-0DCE-4D8A-817E-C298ABE9B6DE">
            <Title>SysteMHC</Title>
            <Text>SysteMHC Atlas
 - {Shao:2017hb}: Shao, W. et al. The SysteMHC Atlas project. Nucleic Acids Research 46, D1237–D1247 (2017).

 - Immunopeptidomics datasets used for building the first version of the SysteMHC Atlas. Data from 23 projects that collectively generated 1184 raw MS files constitute the initial contents of the SysteMHC Atlas.
In May 2017, ∼29.5 million MS/MS spectra were searched using a uniform and well-tested computational pipeline and yielded 250,768 and 1,458,698 distinct peptides with iProphet probability P ≥ 0.9 and P &gt; 0.0, respectively. After applying strict confidence filters for the identification of class I and class II peptides, 119,073 high-confidence HLA class I peptides (peptide FDR 1%, 8–12 amino acids) were identified and annotated to specific HLA-A, -B or -C alleles using an automated annotation strategy as described. For class II molecules, 73,465 high-confidence peptides were identified (peptide FDR 1%, 10–25 amino acids, belonging to groups of two or more peptides with an overlap of at least four amino acids). Of note, the assignment of peptides to specific HLA class II alleles will be considered in the future as soon as robust bioinformatics tools for class II peptide annotation become openly available (26). The high-confidence class I and class II peptides were mapped onto 13,132 and 7,704 of the human UniProtKB/Swiss-Prot proteins, respectively.
  -Data:  https://systemhcatlas.org/Builds_for_download/180409_master_final.tgz
</Text>
        </Document>
        <Document ID="D2957F8B-5049-48F0-AC42-2615A8CF0591">
            <Title>Sarkizova2019</Title>
            <Text>Fine-tuned 된 최종 모델은 11 patient-derived tumor cell lines으로부터 detected ligands를 포함한 external dataset을 사용하여 independently evaluated 되었다[{Sarkizova:2019fu}]. 
</Text>
        </Document>
        <Document ID="C8FC9692-FCA8-4825-8D02-F04A419AF4BA">
            <Title>BERT, transfer learning</Title>
            <Text>BERT의 학습방법은 그림에서 보듯이 전형적인 전이학습 방법이다.
Pre-training: 대용량의 Corpus를 unsupervised manner로 pretraining 하여 문장에서의 word간의 context-based representation을 알아낸 후, 
Fine-tuning: 특정 Task 데이터셋을 사용하여 supervised manner로 pretrained model을 미세튜닝하여 최종 모델을 구축한다
#
</Text>
        </Document>
        <Document ID="5BD49B16-AD30-420E-8E6E-C0A281FFB93D">
            <Title>BERT, Bidirectional Attention</Title>
            <Text>Attention based LM: Transfomer, BERT, ELMo, OpenGPT…
Bidirectionality

Transfomer와 같은 양방향 자기주목 학습을 사용한 BERT는 대용량의 Corpus의 unsupervised 사전학습과 특정 학습을 통해 11개의 자연어처리 Task에서 SOTA를 달성하면서 최근에 크게 주목받고 있다.

</Text>
        </Document>
        <Document ID="3A58E4B4-D4C2-4AB4-9BF4-3C7D9376D4C8">
            <Title>Exploring datasets</Title>
            <Text>Final dataset은 구성되었다. 총 0000, 000 alleles, % binders</Text>
            <Notes>IEDB entries with non-class I, non-specific, mutant, or unparseable allele names were dropped, as were those with peptides identified by MS or containing post-translational modifications or noncanonical amino acids. This yielded an IEDB dataset of 143,898 quantitative and 43,978 qualitative affinity measurements. Of 179,692 measurements in the BD2013 dataset (Kim et al., 2014), 57,506 were not also present in the IEDB dataset. After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the combined dataset consists of 230,735 measurements across 130 alleles. </Notes>
        </Document>
        <Document ID="00D1435E-14D6-4C46-86C6-62CC7E292D44">
            <Title>MS ligands</Title>
            <Text>추가적인 MS ligand dataset은 SysteMHC Atlas [{Shao:2017hb}]와 Abelin 데이터셋으로부터 확장된 {Sarkizova:2019fu}로부터 compiled 되었다. 
The MS ligands from SysteMHC Atlas were filtered to remove entries with low confidence (prob &lt; 0.99).
</Text>
        </Document>
        <Document ID="D2C79734-E7F5-49A2-92B4-201EE3B39FE8">
            <Title>Data preprocessing</Title>
            <Text>The unprocessed MS-EL datasets are much larger but most entries are already present in IEDB, are duplicates, or report on alleles for which training was not attempted. The ligands from SysteMHC Atlas were first filtered to remove entries with low confidence (prob &lt; 0.99).</Text>
        </Document>
        <Document ID="79DE2CBE-A5F2-4DCD-AF2B-2DE5B7FECBD5">
            <Title>기존의 예측방법들</Title>
        </Document>
        <Document ID="CD00FFE0-BB2D-4BD4-8B53-C899C3A0A22E">
            <Title>Ott2017</Title>
            <Text>An immunogenic personal neoantigen vaccine for patients with melanoma 
 
- {Ott:2017ft}
#

#

- Data: N/A
</Text>
        </Document>
        <Document ID="C04A207C-2408-4EDE-8C62-9937BD8577FE">
            <Title>DeepLigand</Title>
        </Document>
        <Document ID="E3957CE4-B02A-4243-BD36-CB9CAFAFDC7A">
            <Title>Data preprocessing</Title>
            <Text>&lt;IEDB&gt;
The IEDB entries were filtered to MHC allele class = I, Epitope Object Type = Linear peptide and Allele Name consistent with human HLA class I nomenclature with four-digit typing (that is, regex: “^HLA-[ABC]\\*[0–9]{2}:[0–9]{2}$”), resulting in a dataset with 0000 quantitative or qualitative measurements. Redundant ligand data with the same ligand sequence and MHC molecule were removed. A peptide was considered a binder if it had a quantitative affinity of &lt;500 nM or qualitative label of ‘Positive’, ‘Positive-High’, ‘Positive-Intermediate’ or ‘Positive-Low’.

After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, , the curated IEDB dataset contained 0000 measurements for 0000 HLA-I alleles.   
We excluded the subsets grouped by allele and peptide length, which have fewer than 25 entries.
The length of the ligands ranged from 4 to 37 aa. All lengths that were associated with &gt;= 0.5% of total ligands were selected for further analysis; this included lengths 8–15 aa and included 99% of the assay entries.
This yielded an IEDB dataset of 143,898(?) quantitative and 43,978(?) qualitative affinity measurements. 정성적 데이터는 [{ODonnell:2018fv}]에서 한 방법으로 정량데이터로 변환되었다; For qualitative affinity data, we assigned the following inequalities and measurement values: positive-high, &lt; 100 nM; positive, &lt; 500 nM, positive-intermediate, &lt; 1,000 nM; positive-low, &lt; 5,000 nM; negative, &gt; 5,000 nM. 

&lt;Kim2014&gt;
The Kim et al dataset contained quantitative half maximal inhibitory concentration(IC50) measurements for 186,281 peptides, where a peptide was denoted as a binder if it had measurement value &lt; 500 nM.

&lt;MS ligands&gt;
The additional MS ligand dataset contained 0000 ligands(binder peptides) formed by combining 0000 ligands from SysteMHC[{Shao:2017hb}] and 0000 ligands from [{Sarkizova:2019fu}] that was extended from [{Abelin:2017cn}], where  the list of MS ligands from SysteMHC Atlas were filtered to remove entries with low confidence (prob &lt; 0.99). 
Lower cases of a peptide in [{Sarkizova:2019fu}] indicates variable modification

&lt;Final&gt;
The final combined dataset used for pretraining our model contained 0000 quantitative or qualitative measurements that were labeled as binder(1) or non-binder(0), after selecting peptides of length 8-15, dropping alleles with fewer than 25 measurements, and selecting only one entry of the major label in duplicated cases removing duplicated records with the same {allele, peptide} pair. 

This yielded an IEDB dataset of 0000 binding affinity measurements and 0000 qualitative MS ligands labeled as Positive-High, Positive-Intermediate, Positive, Positive-Low and Negative.
The 186,281 quantitative half-max? (IC50) measurements in {Kim:2014jg} were labeled as Positive if a &lt; 500 nM, otherwise Negative. 
The MS ligands from SysteMHC Atlas were filtered to remove entries with low confidence (prob &lt; 0.99) and then labeled as Positive.
- The MS-identified EL dataset  for pretraining the model consisted of the MS-identified ligands from IEDB, SysteMHC Atlas [{Shao:2017hb}] and the eluted ligands from 95 HLA-I -allelic cell lines[{Sarkizova:2019fu}].  
Pretraining을 위해 사용된 최종 dataset은 중복된 ligands 제거하면서, 두 데이터셋을 combine한 최종 데이터셋을
The final dataset with  ? measurements that combines two datasets, removing redundant entries with the same peptide sequence and MHC allele, was used for pretraining the model.

</Text>
            <Notes>MHCflurry
IEDB entries with non-class I, non-specific, mutant, or unparseable allele names were dropped, as were those with peptides identified by MS or containing post-translational modifications or noncanonical amino acids. This yielded an IEDB dataset of 143,898 quantitative and 43,978 qualitative affinity measurements. Of 179,692 measurements in the BD2013 dataset (Kim et al., 2014), 57,506 were not also present in the IEDB dataset. After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the combined dataset consists of 230,735 measurements across 130 alleles. 

NetMHCPan4.0
Data on all class I MHC ligand elution assays available in the IEDB database (http://www.iedb.org) were collected, including the ligand sequence, details of the source protein, position of the ligand in the source protein, and the restricting allele of the ligand. There were 160,527 distinct assays in total, and the length of the ligands ranged from 4 to 37 aa. All lengths that were associated with $0.5% of total ligands were selected for further analysis; this included lengths 8–15 aa and included 99% of the assay entries. 
The restricting MHC molecule of the ligands was analyzed, and entries with alleles listed unambiguously were selected. For example, some entries for which the HLA alleles are listed as just the gene name, as well as alleles from chicken, horse, cow, and mouse for which we did not have binding prediction algorithms, were excluded. Representative alleles were assigned for entries where only supertypes were listed (e.g., HLA-A*26:01 for HLA-A26). Thus, there were 127 class I molecules from human and mouse in the selected data set. Redundant entries with the same ligand sequence and MHC molecule were removed, and MHC molecules with $50 ligand entries were selected. This included 55 class I molecules, and the number of available ligands per molecule varied widely from 50 to 9500.

The IEDB entries were filtered to MHC allele class = I, Epitope Object Type = Linear peptide and Allele Name consistent with human HLA class I nomenclature with four-digit typing (that is, regex: “^HLA-[ABC]\\*[0–9]{2}:[0–9]{2}$”). Peptides with quantitative measurements in units other than nM were removed and so were the following three assay types due to detected inconsistency between predicted (NetMHC 3.0) and actual affinity: ‘purified MHC/direct/radioactivity/dissociation constant KD’, ‘purified MHC/direct/fluorescence/half maximal effective concentration (EC50)’ and ‘cellular MHC/direct/fluorescence/ half maximal effective concentration (EC50)’. A peptide was considered a binder if it had a quantitative affinity of &lt;500 nM or qualitative label of ‘Positive’, ‘Positive-High’, ‘Positive-Intermediate’ or ‘Positive-Low’. In cases where multiple records are available for the same {peptide, allele} pair, we either took the mean affinity or removed the peptide when the difference between the maximum and minimum log-transformed affinities (1 − log(nM)/log(50,000)) was &gt;0.2. Similarly, peptides with multiple qualitative, were removed if the same number of positive and negative labels were found or kept otherwise Major 라벨의 엔트리 하나만 남겨두고 나머지는 제거하였다.
Our previously published data for 16 HLA-A and -B alleles were removed from the analysis of IEDB counts (PubMedID = 28228285). 
After selecting peptides of length 8-15 and dropping alleles with fewer than 25 measurements, the curated IEDB dataset consisted of 0000 measurements for 0000 HLA-I alleles.  
</Notes>
        </Document>
        <Document ID="B9FA15E2-3288-4BA5-B3E1-A85D3789FFD7">
            <Title>Pre-training the model</Title>
            <Text>Loss curves in Masked LM and Binding Affinity Prediction training epochs
</Text>
        </Document>
        <Document ID="DD6D04A2-8976-4EC5-8DE0-E2069D05BD51">
            <Text>{Devlin:2018uk}</Text>
        </Document>
        <Document ID="45F6EB93-C4E9-4497-B7C9-F29817494DFF">
            <Title>BD2013</Title>
            <Text> - {Kim:2014jg}: Kim, Y. et al. Dataset size and composition impact the reliability of performance benchmarks for peptide-MHC binding predictions. 15, 241 (2014).

 - Peptide-MHC quantitative binding affinity with inequality(&gt;, &lt;, =)
 - Data: http://tools.iedb.org/main/datasets/, http://tools.iedb.org/static/main/benchmark_mhci_reliability.tar.gz


</Text>
        </Document>
        <Document ID="9C3C3F8E-5925-4B65-A744-0C2551B9E41C">
            <Title>ML/DL 방법의 한계점</Title>
        </Document>
        <Document ID="2E308F2D-52E7-4FE3-BDCA-B7295AAF4D17">
            <Title>MHCflurrry</Title>
            <Text>MS Benchmark Dataset 
The MS benchmark was derived from 23,651 sequences of MHC-displayed ligands eluted from a B cell line expressing a single MHC I allele [{Abelin:2017cn}]. We excluded one allele (HLA-A*02:04) not supported by MHCflurry or NetMHC due to insufficient representation in the training dataset (fewer than 25 measurements) and discarded peptides with post-translational modifications or lengths outside the supported range (8-15 residues). We sampled unobserved sequences (decoys) from the protein-coding transcripts that contained the identified peptides (hits) based on protein sequences in the UCSC hg19 proteome and transcript quantification from RNA sequencing of the relevant cell line (B721.221) downloaded from the Gene Expression Omnibus (accession GSE93315). For an allele with n hits, we sampled 100n decoys, weighting transcripts by the number of hits and sampling an equal number of decoys of each length 8-15. After removing any entries also present in the training dataset, this yielded a benchmark of 23,651 hits and 2,377,042 decoys. 
</Text>
        </Document>
        <Document ID="E66782ED-6E85-453B-A16B-53D3BB790792">
            <Title>DeepLigand</Title>
            <Text>#

Fig. 1. Schematics of DeepLigand. DeepLigand consists of a binding affinity prediction module and a peptide embedding module. The affinity prediction module takes as input a pair of MHC and peptide sequence and predicts the mean and variance their binding affinity using a deep residual network. The peptide embedding module is a deep language model (ELMo) that embeds each peptide into a vector representation. The outputs from the two modules are concatenated and provided as input to one fully connected (FC) layer with sigmoid activation to predict whether the input peptide is a natural ligand of the input MHC. As a pre-training step, the embedding module is trained on the natural ligands in the training set for a given CV split. Then the affinity prediction module and the combining layer is jointly trained on all training examples taking as input the MHC and peptide sequences as well as the peptide embeddings produced by the pre-trained embedding module 

DeepLigand는 peptide 서열(‘sentence’)에서의 각 아미노산(‘word’)의 contextual한 vector representation(‘language model’)을 구하기 위해 ELMo 기반의 peptide embedding module을 통하여 MS-eluted natural ligands 데이터셋을 사전 학습하였다.

ELMo는 LSTM 기반 bi-directional language model이다. 문장에서의 단어간의 long-term dependancies를 self-attention 기반의 Transformer[{Vaswani:2017ul}]가 LSTMs 보다 효과적으로 다룰 수 있다. 

ELMo를 기반으로 한 DeepLigand는 peptide의 vector representation을 사전 학습할 때 펩타이드 아미노산과 MHC contact residue 간의 상호작용을 고려하지 않았다(아마도 LSTM의 long-term relationship(AA interactions)을 다루는 것에서의 한계 때문일 수 있다)

</Text>
        </Document>
        <Document ID="4B3ACD70-F7D5-4C83-834B-227BAD730C73">
            <Title>BERT 논문정리</Title>
            <Text>tmax.ai
ABOUT ARCHIVES CATEGORIES TAGS SEARCH
BERT 논문정리
● 23 Feb 2019

Written by Minho Park
BERT: Pre-trainig of Deep Bidirectional Transformers for Language Understanding
최근에 NLP 연구분야에서 핫한 모델인 BERT 논문을 읽고 정리하는 포스트입니다.
구성은 논문을 쭉 읽어나가며 정리한 포스트기 때문에 논문과 같은 순서로 정리하였습니다.
Abstract

BERT : Bidirectional Encoder Representations form Transformer
논문의 제목에서 볼 수 있듯이, 본 논문은 “Attention is all you need(Vaswani et al., 2017)”(arxiv)에서 소개한 Transformer 구조를 활용한 Language Representation에 관한 논문입니다.
Transformer에 대한 자세한 구조를 알고 싶은 분은 위 논문을 읽어보시거나, 다음 블로그 MChromiak’s blog를 참고하시면 좋을 듯 합니다.
BERT는 기본적으로, wiki나 book data와 같은 대용랑 unlabeled data로 모델을 미리 학습 시킨 후, 특정 task를 가지고 있는 labeled data로 transfer learning을 하는 모델입니다.
BERT이전에 이러한 접근 방법을 가지는 모델이 몇가지 있었습니다. 대용량 unlabeld corpus를 통해 language model을 학습하고, 이를 토대로 뒤쪽에 특정 task를 처리하는 network를 붙이는 방식(ELMo, OpenAI GPT…)
하지만 BERT 논문에서는 이전의 모델의 접근 방식은 shallow bidirectional 또는 unidirectional하므로 language representation이 부족하다고 표현하였습니다.
게다가 BERT는 특정 task를 처리하기 위해 새로운 network를 붙일 필요 없이, BERT 모델 자체의 fine-tuning을 통해 해당 task의 state-of-the-art를 달성했다고합니다.
1. Introduction

Introduction에서는 BERT와 비슷한 접근 방식을 가지고 있는 기존 model에 대한 개략적인 소개를 합니다.
Language model pre-training은 여러 NLP task의 성능을 향상시키는데에 탁월한 효과가 있다고 알려져 있습니다. (Dai and Le, 2015; Peters et al., 2018, 2018; Radford et al., 2018; …)
이러한 NLP task는 token-level task인 Named Entity Recognition(NER)에서부터 SQuAD question answering task와 같은 task까지 광범위한 부분을 커버합니다
이런 pre-trained language representation을 적용하는 방식은 크게 두가지 방식이 있습니다. 하나는 feature-based 또 다른 하나는 fine-tuning 방식입니다.
feature-based approach : 특정 task를 수행하는 network에 pre-trained language representation을 추가적인 feature로 제공. 즉, 두 개의 network를 붙여서 사용한다고 보면 됩니다. 대표적인 모델 : ELMo(Peters et al., 2018)
fine-tuning approach : task-specific한 parameter를 최대한 줄이고, pre-trained된 parameter들을 downstream task 학습을 통해 조금만 바꿔주는(fine-tuning) 방식. 대표적인 모델 : Generative Pre-trained Transformer(OpenAI GPT) (Radford et al., 2018)
앞에 소개한 ELMo, OpenAI GPT는 pre-training시에 동일한 objective funtion으로 학습을 수행합니다, 하지만 BERT는 새로운 방식으로 pre-trained Language Representation을 학습했고 이것은 매우 효과적이었습니다.
BERT의 pre-training 방법론


그림1. BERT, GPT, ELMo (출처 : BERT 논문)

BERT pre-training의 새로운 방법론은 크게 2가지로 나눌 수 있습니다. 하나는 Masked Language Model(MLM), 또 다른 하나는 next sentence prediction이다.
기존 방법론 : 앞에 소개한 ELMo, OpenAI GPT는 일반적인 language model을 사용하였습니다. 일반적인 language model이란, 앞의 n 개의 단어를 가지고 뒤의 단어를 예측하는 모델을 세우는 것입니다(n-gram). 하지만 이는 필연적으로 unidirectional할 수 밖에 없고, 이러한 단점을 극복하기 위해 ELMo에서는 Bi-LSTM으로 양방향성을 가지려고 노력하지만, 굉장히 shallow한 양방향성 (단방향 concat 단방향)만을 가질 수 밖에 없었습니다(그림1).
Masked Language Model(MLM) : MLM은 input에서 무작위하게 몇개의 token을 mask 시킵니다. 그리고 이를 Transformer 구조에 넣어서 주변 단어의 context만을 보고 mask된 단어를 예측하는 모델입니다. OpenAI GPT도 Transformer 구조를 사용하지만, 앞의 단어들만 보고 뒷 단어를 예측하는 Transformer decoder구조를 사용합니다(그림1). 이와 달리 BERT에서는 input 전체와 mask된 token을 한번에 Transformer encoder에 넣고 원래 token 값을 예측하므로(그림1) deep bidirectional 하다고 할 수 있습니다. BERT의 MLM에 대해서는 뒷장의 Pre-training Tasks에서 더 자세히 설명하겠습니다.
next sentence prediction : 이것은 간단하게, 두 문장을 pre-training시에 같이 넣어줘서 두 문장이 이어지는 문장인지 아닌지 맞추는 것입니다. pre-training시에는 50:50 비율로 실제로 이어지는 두 문장과 랜덤하게 추출된 두 문장을 넣어줘서 BERT가 맞추게 시킵니다. 이러한 task는 실제 Natural Language Inference와 같은 task를 수행할 때 도움이 됩니다.
2. Related Work

ELMo, OpenAI GPT와 같은 모델이 존재하고, 앞에서 충분히 소개하였기 때문에 생략하도록 하겠습니다. 자세한 내용에서는 BERT 논문을 참고 바랍니다.
3. BERT

BERT의 아키텍처는 Attention is all you need에서 소개된 Transformer를 사용하지만, pre-training과 fine-tuning시의 아키텍처를 조금 다르게하여 Transfer Learning을 용이하게 만드는 것이 핵심입니다.
3.1 Model Architecture
BERT는 transformer 중에서도 encoder 부분만을 사용합니다. 이에 대한 자세한 내용은 Vaswani et al (2017) 또는 tensor2tensor를 참고 바랍니다.
BERT는 모델의 크기에 따라 base 모델과 large 모델을 제공합니다.
BERT_base : L=12, H=768, A=12, Total Parameters = 110M
BERT_large : L=24, H=1024, A=16, Total Parameters = 340M
L : transformer block의 layer 수, H : hidden size, A : self-attention heads 수, feed-forward/filter size = 4H
여기서 BERT_base 모델의 경우, OpenAI GPT모델과 hyper parameter가 동일합니다. 여기서 BERT의 저자가 의도한 바는, 모델의 하이퍼 파라미터가 동일하더라도, pre-training concept를 바꾸어 주는 것만으로 훨씬 높은 성능을 낼 수 있다는 것을 보여주고자 하는 것 같습니다.
OpenAI GPT모델의 경우 그림1에서 볼 수 있듯, next token 만을 맞추어내는 기본적인 language model 방식을 사용하였고, 그를 위해 transformer decoder를 사용했습니다. 하지만 BERT는 MLM과 NSP를 위해 self-attention을 수행하는 transformer encoder구조를 사용했음을 알 수 있습니다.
실제로 대부분의 NLP task SOTA는 BERT_large모델로 이루어 냈습니다.
3.2 Input Representation


그림2. bert input representation (출처: BERT 논문)

BERT의 input은 그림 2와 같이 3가지 embedding 값의 합으로 이루어져 있습니다.
WordPiece embedding을 사용합니다. WordPiece(Wu et al., 2016)에 대한 자세한 내용은 논문 링크를 참고 하시거나, lovit님의 블로그글을 참고 바랍니다. BERT english의 경우 30000개의 token을 사용하였습니다.
그림 2에서 볼 수 있듯이, Position embedding을 사용합니다. 이는 Transformer에서 사용한 방식과 같으며, jalammer의 블로그 글을 참고하시면 position embedding 뿐만 아니라 transformer의 전체적인 구조를 이해 하실 수 있습니다.
모든 sentence의 첫번째 token은 언제나 [CLS](special classification token) 입니다. 이 [CLS] token은 transformer 전체층을 다 거치고 나면 token sequence의 결합된 의미를 가지게 되는데, 여기에 간단한 classifier를 붙이면 단일 문장, 또는 연속된 문장의 classification을 쉽게 할 수 있게 됩니다. 만약 classification task가 아니라면 이 token은 무시하면 됩니다.
Sentence pair는 합쳐져서 single sequence로 입력되게 됩니다. 각각의 Sentence는 실제로는 수 개의 sentence로 이루어져 있을 수 있습니다(eg. QA task의 경우 [Question, Paragraph]에서 Paragraph가 여러개의 문장). 그래서 두 개의 문장을 구분하기 위해, 첫째로는 [SEP] token 사용, 둘째로는 Segment embedding을 사용하여 앞의 문장에는 sentence A embedding, 뒤의 문장에는 sentence B embedding을 더해줍니다.(모두 고정된 값)
만약 문장이 하나만 들어간다면 sentence A embedding만을 사용합니다.
3.3 Pre-training Tasks
기존의 ELMO나 GPT는 left to right or right to left Language Model을 사용하여 pre-training을 하지만, BERT는 이와 다르게 2가지의 새로운 unsupervised prediction task로 pre-training을 수행합니다.
3.3.1 Task #1: Masked LM
Introduction의 pre-training 방법론에서 설명한 내용과 동일한 내용입니다.
이번 장에서는 MLM이 구체적으로 어떤 식으로 수행되는 지에 대해서 설명하겠습니다.


그림3. BERT Masked Language Model (출처: rani horev’s blog : BERT explained)

그림 3에서 볼 수 있듯, 일단 단어 중의 일부를 [MASK] token 으로 바꾸어 줍니다. 바꾸어 주는 비율은 15% 입니다.
그리고 plain text를 tokenization하는 방법은 input representation에서 설명한 바와 같이 WordPiece(Wu et al., 2016)를 사용합니다.
이를 통하여 LM의 left-to-right (혹은 r2l)을 통하여 문장 전체를 predict하는 방법론과는 달리, [MASK] token 만을 predict하는 pre-training task를 수행합니다.
이 [MASK] token은 pre-training에만 사용되고, fine-tuning시에는 사용되지 않습니다. 해당 token을 맞추어 내는 task를 수행하면서, BERT는 문맥을 파악하는 능력을 길러내게 됩니다.
15%의 [MASK] token을 만들어 낼 때, 몇가지 추가적인 처리를 더 해주게 됩니다. 그것은 다음과 같습니다.
80%의 경우 : token을 [MASK]로 바꿉니다. eg., my dog is hairy -&gt; my dog is [MASK]
10%의 경우 : token을 random word로 바꾸어 줍니다. eg., my dog is hariy -&gt; my dog is apple
10%의 경우 : token을 원래의 단어로 그대로 놔둡니다. 이는 실제 관측된 단어에 대한 표상을 bias해주기 위해 실시합니다.
pre-trained 되는 Transformer encoder의 입장에서는 어떤 단어를 predict하라고 하는건지, 혹은 random word로 바뀌었는지 알 수 없습니다. Transformer encoder는 그냥 모든 token에 대해서 distributional contextual representation을 유지하도록 강제합니다.
또한 random word로 바꾸는 것 때문에 모델의 language understanding능력에 해를 끼친다고 생각 할 수 있지만, 바뀌는 부분이 1.5%(15%의 10%)에 불과하므로, 해를 끼치지 않습니다.
또한 MLM은 보통의 LM 보다 converge하는데에 많은 training step이 필요하지만, emperical하게는 LM보다 훨씬 빠르게 좋은 성능을 냅니다. 이는 Section 5.3에서 자세히 설명하겠습니다.
3.3.2 Task #2: Next Sentence prediction
이 task 또한 Introduction의 pre-training 방법론에서 설명한 내용입니다.
이 pre-training task 수행하는 이유는, 여러 중요한 NLP task중에 QA나 Natural Language Inference(NLI)와 같이 두 문장 사이의 관계를 이해하는 것이 중요한 것들이기 때문입니다.. 이들은 language modeling에서 capture되지 않습니다.
그래서 BERT에서는 corpus에서 두 문장을 이어 붙여 이것이 원래의 corpus에서 바로 이어 붙여져 있던 문장인지를 맞추는 binarized next sentence prediction task를 수행합니다.
50% : sentence A, B가 실제 next sentence
50% : sentence A, B가 corpus에서 random으로 뽑힌(관계가 없는) 두 문장
예를 들어
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] LABEL = IsNext

Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext

pre-training이 완료되면, 이 task는 97~98%의 accuracy를 달성했습니다. 이러한 간단한 task를 부여해도, QA와 NLI에 굉장히 의미있는 성능 향상을 이루어 냈습니다. 이는 section5. Ablation Studies에서 자세히 설명이 되어있습니다.
3.4 Pre-training Procedure
pre-training의 기본적인 절차는 LM에서 수행하는 것과 같습니다.
BERT_english의 경우 BookCorpus (Zhu et al., 2015) (800M words)와 English Wikipedia (2,500M words)를 사용하였습니다. Wikipedia 데이터에서는 text passage만 추출하여 사용했다고 합니다. 이유는, long contiguous sequence만을 학습시키고 싶어서입니다.
input pre-processing
먼저, NSP를 위해 sentence를 뽑아서 embedding A, B를 먹여줍니다. 물론 50%는 진짜 next sentence, 나머지는 random sentence를 사용합니다.
이 모든 토큰이 합쳐진 길이는 512개 이하여야 합니다. (OOM 때문)
이후 Masking 작업을 해줍니다.
pre-training 시의 Hyper Parameters
batch size : 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps -&gt; 3.3 billion word corpus의 40 epochs
Adam optimizer, learning rate : 1e-4, β1=0.9
β
1
=
0.9
, β2=0.999
β
2
=
0.999
, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, linear decay of the learning rate
Dropout prob: 0.1 for all layers
using gelu activation Hendrycks and Gimpel, 2016
BERT_base - 4 TPUs, BERT_large - 16 TPUs를 사용하여 4일동안 학습
3.5 Fine-tuning Procedure
sequence-level classification tasks에 대해서는 BERT fine-tuning과정이 매우 straightforward합니다.
input sequence에 대해서 일정한 차원수의 representation 결과를 얻고 싶기 때문에, [CLS] token의 Transformer output값을 사용합니다.
[CLS] token의 벡터는 H차원을 가집니다. C∈ℝH
C
∈
R
H
여기서 classify하고 싶은 갯수(K)에 따라 classification layer를 붙여 줍니다. classification layer : W∈ℝK×H
W
∈
R
K
×
H
label probabilities는 standard softmax로 계산 됩니다. P=softmax(CWT)
P
=
s
o
f
t
m
a
x
(
C
W
T
)
W matrix와 BERT의 모든 파라미터가 같이 fine-tuning 됩니다.
span-level, token-level prediction tasks의 경우에는, 위의 과정에서 약간 변형시켜 fine-tuning이 진행됩니다. 자세한 내용은 Section 4에서 설명하도록 하겠습니다.
fine-tuning시의 Hyper-Parameter
몇가지를 제외하고는 pre-training때의 hyper parameter와 대부분 동일합니다.
다른점은 batch size, learning rate, trainig epochs 수 입니다.
optimal hyperparameter는 task마다 달라지지만, 다음에 제시하는 것을 사용하면 대부분 잘 학습됩니다.
Batch size: 16, 32 Learning rage (Adam): 5e-5, 3e-5, 2e-5 Number of epochs : 3, 4

fine-tuning시의 dataset의 크기가 클수록 hyperparameter에 영향을 덜 받고 잘 training 됨을 관측할 수 있었다고 합니다.
Fine-tuning은 굉장히 빠르게 학습되며(pre-training에 비해) 이로 인해 최적의 hyperparameter 탐색을 exhaustive search로 찾아내도 무방합니다.
3.6 Comparison of BERT and OpenAI GPT
OpenAI GPT와 BERT는 transformer구조를 사용한다는 점에 있어서 공통점을 갖지만, BERT가 훨씬 좋은 성능을 갖습니다. 이 차이는 앞에 설명한 MLM task, NSP task를 수행하는 것과 별개로 또 더 다른점을 갖기에 생깁니다.
GPT의 경우 BookCorpus(800M words)만을 pre-training에 사용; BERT는 거기에 + Wikipedia(2500M words) 사용
GPT는 [SEP]과 [CLS] token을 fine-tuning시에만 추가하여 학습; BERT는 pre-training시에도 학습 (NSP task가 존재하기 때문에 가능)
GPT : 32,000 words/batch for 1M steps ; BERT : 128,000 words/batch ofr 1M steps
GPT는 모든 fine-tuning을 수행할 때 learning rate : 5e-5를 사용; BERT는 task-specific하게 조절하여 사용
위에 나열된 차이점에 대한 효과는 ablation experiments가 수행된 Section 5.1에 설명되어 있습니다.
4. Experiments

이번 섹션에서는, 11개의 NLP tasks에 대한 BERT fine-tuning에 대해서 설명하겠습니다.


그림 4. BERT experiments result (출처: BERT 논문)

그림 4를 참고하면, 각 fine-tuning 유형마다 어떻게 학습하는지를 알아 볼 수 있습니다.
(a), (b)는 sequence-level task, (c)와 (d)는 token-level task입니다. sequence-level task의 경우는 3.5 Fine-tuning Procedure에서 설명을 드렸습니다.
(c)의 QA task경우는, Question에 정답이 되는 Paragraph의 substring을 뽑아내는 것이므로, [SEP] token 이후의 token들에서 Start/End Span을 찾아내는 task를 수행합니다.
(d)의 경우는 Named Entity Recognition(NER)이나 형태소 분석과 같이 single sentence에서 각 토큰이 어떤 class를 갖는지 모두 classifier 적용하여 정답을 찾아냅니다.
4.1 GLUE Datasets
해당 섹션은 The General Language Understanding Evaluation(GLUE) benchmark (Wang et al., 2018)에 대한 설명이 이어져있습니다. 본 포스트에서는 생략하고 넘어가도록 하겠습니다.
4.1.1 GLUE Results


그림5. GLUE results (출처: BERT 논문)

쉽게 말해, 모든 task에 대해 SOTA를 달성합니다.
특히 인상적인 것은 데이터 크기가 작아도 fine-tuning 때는 좋은 성능을 낼 수 있다는 것입니다.
그리고 BERT_large가 BERT_base에 비해 훨씬 좋은 성능을 냅니다.(dataset이 작은 task의 경우에도!) BERT의 모델 크기에 따른 성능은 Section 5.2에서 살펴보겠습니다.
4.2 SQuAD v1.1
기존 SQuAD dataset의 경우에는 GLUE dataset을 BERT에 fine-tuning할 때와 방식이 좀 다릅니다. GLUE dataset의 경우에는 task가 sequence classification이지만, SQuAD는 질문과 지문이 주어지고, 그 중 substring인 정답을 맞추는 task입니다.
이러한 차이를 BERT는 아주 간단한 방법론으로 극복합니다.
일단 질문을 A embedding, 지문을 B embedding 으로 처리하고, 지문에서 정답이 되는 substring의 처음과 끝을 찾는 task로 문제를 치환 합니다.
start vector S∈ℝH
S
∈
R
H
와 end vector E∈ℝH
E
∈
R
H
를 fine-tuning중에 학습히여, 지문의 각 token들과 dot product하여 substring을 찾아냅니다. (그림 4 (c) 참조)
이런 접근법으로 BERT_large의 경우 기존의 모든 시스템을 wide margin을 두고 최고성능을 달성 합니다.
4.3 Named Entity Recognition
token tagging task를 평가해보기 위해서 CoNLL 2003 Named Entity Task를 fine-tuning 해보았습니다.
해당 데이터셋은 200k개의 words로 이루어져 있고, 각각의 단어들은 Person, Organization, Location, Miscellaneous, Other로 태깅되어 있습니다.
각 토큰마다 classifier를 붙여서 위의 제시된 class에 대해 판별을 합니다.
각각의 prediction은 주위의 prediction에 영향을 받지 않습니다. (CRF나 autoregressive를 사용하지 않음!)
이 task또한 SOTA를 달성합니다.
4.4 SWAG
The Situations With Adversarial Generations(SWAG) 데이터셋은 113k sentence-pair로 이루어져 있으며, grounded common-sense inference를 측정하기 위해 사용합니다.
앞 문장이 주어지고, 보기로 주어지는 4 문장중에 가장 잘 이어지는 문장을 찾는 task 입니다.
A girl is going across a set of monkey bars. She (i) jumps up across the monkey bars. (ii) struggles onto the bars to grab her head. (iii) gets to the end and stands on a wooden plank. (iv) jumps up and does a back flip.

이 데이터셋을 BERT에 적용하는 것은 GLUE dataset 적용법과 비슷합니다. 4개의 가능한 input sequences를 구성합니다. 이는 given sentence(sentence A)와 possible continuation(sentence B)을 concat한 것들입니다.
해당 task specific한 벡터 V∈ℝH
V
∈
R
H
를 학습시키고, input sequences의 값들을 합산한 Ci∈ℝH
C
i
∈
R
H
를 dot product하여 softmax 합니다.
이 또한 사람을 능가하는 SOTA를 달성합니다.
5. Ablation Studies

해당 챕터에서는 이전 챕터에서 중요한 요소라고 설명했던 부분을을 하나씩 제거하며 요소들의 중요함을 파악해 보고있습니다.
개인적으로 논문에서 가장 중요한 챕터라고 생각합니다.
5.1 Effect of Pre-training Tasks
5.1에서는 이전 3.3 Pre-training Tasks에서 소개한 2가지 task를 하나씩 제거하면서 각각의 task의 효과를 알아봅니다.
BERT_base와 동일한 hyperparameter로 실험을 진행하지만 ablation한 두가지 다른 모델로 실험을 진행합니다.
No NSP: MLM은 사용하지만, 다음 문장 예측 (NSP)를 없앤 모델
LTR &amp; No NSP : MLM 대신 Left-to-Right (LTR) 을 사용하고, NSP도 없앤 모델, 이는 OpenAI GPT모델과 완전히 동일하지만, 더 많은 트레이닝 데이터를 사용하였습니다.

그림6. pre-training task ablation result (출처: BERT 논문)

표에서 볼 수 있듯, pre-training task를 하나라도 제거하면 성능이 굉장히 떨어지는 것을 볼 수 있습니다.
No NSP의 경우에는 NLI계열의 task에서 성능이 많이 하락하게 되는데, 이는 NSP task가 문장간의 논리적인 구조 파악에 중요한 역할을 하고 있음을 알 수 있습니다.
MLM대신 LTR을 쓰게 되면 성능하락은 더욱더 심해지게 됩니다. BiLSTM을 더 붙여도, MLM을 쓸 때보다 성능이 하락하는 것으로 보아, MLM task가 더 Deep Bidirectional한 것임을 알 수 있습니다.
5.2 Effect of Model Size

간단하게 말해서, 측정한 데이터셋에서는 모두 모델이 커질수록, 정확도가 상승함을 볼 수 있습니다.

그림7. Ablation over BERT model size (출처: BERT 논문)

수 년동안 알려져 왔듯, 번역 task나 language modeling과 같은 large-scale task는 모델 사이즈가 클 수록 성능은 계속 상승합니다.
특히 BERT의 경우에는, downstream task를 수행하는 dataset의 크기가 작아도, pre-training덕분에, model의 크기가 클 수록 정확도는 상승함을 볼 수 있습니다.
5.3 Effect of Number of Training Steps


그림8. Ablation over number of training steps (출처: BERT 논문)

그림 8에서는 MNLI Dev accuracy를 pre-training step에 따라 정리하였습니다. 세부사항은 문답형식으로 진행하겠습니다.
Question 1. fine-tuning 단계에서 높은 정확도를 얻으려면, pre-training 단계에서 많은 training step이 필요합니까?
A : 그렇습니다. 0.5M step에 비해 1M step때 accuracy가 거의 1.0% 상승함을 볼 수 있습니다.
Question 2. 3.3.1 Task #1: Masked LM의 마지막 부분에서 시사한 바와 같이, MLM으로 학습하면 15%의 단어만 맞추는 것으로 학습을 진행하기 때문에, LTR보다 수렴속도가 훨씬 느리지않습니까?
A : 수렴속도가 조금 느린 것은 사실이지만, LTR보다 훨씬 먼저 out-perform성능이 나오게 됩니다.
5.4 Feature-based Approach with BERT

지금까지 BERT는 pre-training을 진행 한 후, downstream task를 학습 할 때, 간단한 classifier를 부착해서 모든 layer를 다시 학습시키는 fine-tuning 방법을 사용하는 것만을 설명드렷습니다.
하지만 BERT를 ELMO와 같이 feature based approach로도 사용을 할 수 있습니다.
Feature-based Approach는 몇가지 이점이 존재합니다.
Transformer encoder는 모든 NLP task를 represent하지는 못하므로, 특정 NLP task를 수행할 수 있는 Network를 부착하여 쓸 수 있습니다.
Computational benefit을 얻을 수 있습니다.
해당 section에서는 BERT를 ELMO와 같이 마지막 레이어에 Bi-LSTM을 부착시켜, 해당 레이어만 학습 시키는 방법론을 사용해보았습니다.

그림9. Ablation using BERT with a feature-based approach (출처: BERT 논문)

그림 9에서 볼 수 있듯이, Concat Last Four Hiddem값을 사용하면, Finetune All과 단지 0.3 F1 score차이밖에 나지 않습니다.
이를 통해, BERT는 Feature-based Approach에서도 효과적이라고 할 수 있겠습니다.
6. Conclusion
읽으면서 계속해서 느꼈지만, 굉장히 놀라운 논문이라고 생각합니다. 특히 MLM과 NSP task를 생각해 낸것이 굉장히 놀라웠습니다. 이러한 간단한 intuition을 가지고 NLP의 전분야를 아우르는 SOTA를 달성해 낸 것에 대해서 놀랍고, 또 부럽다는 생각을 했습니다.
마치며…
논문을 읽은 것이 10월 말 경이었던 것 같은데, 조금씩 글로 정리하다 보니 벌써 2월 말이 되었습니다. 그 사이에 BERT를 토대로 다시 SOTA를 달성한 새로운 논문(Multi-Task Deep Neural Networks for Natural Language Understanding)을 MS에서 발표하였습니다. 해당 논문도 빠른 시일내에 review해 보도록 하겠습니다. 감사합니다.
NLP Language Representation transformer 고급
© 2019 Tmaxdata. All right reserved.
 </Text>
        </Document>
        <Document ID="EDBBFFF7-D55D-4894-BACA-EBA8D8E76836">
            <Title>Conclusions</Title>
        </Document>
        <Document ID="E310FC17-2D4D-44CE-A230-875564BC8D32">
            <Title>Fine-tuning pre-trained model</Title>
            <Text>Training for fine-tuning the pre-trained model was carried out in standard 5-fold cross-validation. </Text>
        </Document>
        <Document ID="160621EC-768A-4D29-9A63-4C30CF5351B5">
            <Title>선행학습에서 prior knowledge 학습시키기</Title>
        </Document>
        <Document ID="DE8931AE-4127-4DC0-88EA-461B5027C118">
            <Title>full_ms_model(EDGE)</Title>
        </Document>
        <Document ID="0886F03E-89E1-4BC8-929A-E22A5358E7CF">
            <Title>Materials and Methods</Title>
        </Document>
        <Document ID="25E226DF-1E4D-42B8-8FE9-3AFFCB744D7D">
            <Title>BERT?</Title>
            <Text>Transformer의 encoder를 사용하여 self-attention 기반의 BERT(Bidirectional Encoder Representations from Transformers)[{Devlin:2018uk}]는 대용량 corpus의 semi-supervised 사전학습을 통해 contextual word representation(language model)을 구축하고 이를 specific한 task의 비교적 적은 양의 데이터에 대한 supervised fine-tuning을 통해 최종 예측 모델을 구축하는 방법으로 11개의 자연어 처리 task(SQuAD1.1 등)에서 SOTA를 달성하면서 최근에 크게 주목받고 있다. </Text>
        </Document>
        <Document ID="82400414-9ECB-454D-8F62-CF43D1F96576">
            <Title>Not all binding peptides are presented on the cell</Title>
            <Text>MHC와 결합하는 모든 펩타이드가 Tumor-specific 신항원 펩타이드는 아니다. [{Biotechnol:fe}]. MHC 분자와 결합하는 펩타이드 중에 오직 30% 미만이 tumor cell에서 process 되고 이중 일부만이 T-cell을 유도하고 clinically-relevant immunogenicity를 갖는 neopeptide 일 수 있다[{Vitiello:2017dm}, {Ott:2017ft}].

One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this binding to MHC is the most selective step in peptide Ag presentation. However, other factors, including Ag processing [{Tenzer:2005bt}] and the stability of the peptide–MHC complex [{Harndahl:2012jj}], could influence the likelihood of a given peptide to be presented as an MHC ligand. 
Similarly, the length distribution of peptides available for binding to MHC molecules is impacted by other steps in the processing and presentation pathway, such as TAP transport and ERAP trimming, which are not reflected in binding data in itself[{Trolle:2016jw}]. 
</Text>
            <Notes>Some of the most well-documented and applied methods for predicting peptide binding to MHC class I include NetMHC (4, 7) and NetMHCpan (1, 8). Over the last years, these tools have garnered increasing interest because of the recent focus on neoantigen identification within the field of personalized immunotherapy (9, 10). However, as underlined in several studies, including the recent Nature Biotechnology Editorial (11), “neoantigen discovery and validation remains a daunting problem,” primarily as a result of the relatively high false positive rate of predicted epitopes. 
One potential cause for this relatively high rate of false positive epitope predictions is the fact that most methods are trained on binding affinity (BA) data and, as a consequence, only model the single event of peptide–MHC binding. As stated above, this binding to MHC is the most selective step in peptide Ag presentation. However, other factors, including Ag processing (12) and the sta- bility of the peptide–MHC complex (13), could influence the likelihood of a given peptide to be presented as an MHC ligand. 
Similarly, the length distribution of peptides available for binding to MHC molecules is impacted by other steps in the processing and presentation pathway, such as TAP transport and ERAP trimming, which are not reflected in binding data in itself (6). </Notes>
        </Document>
        <Document ID="14C67731-9074-4B0E-9FAE-499B79D20F44">
            <Title>Encoding pMHC binding data into a sentence</Title>
            <Text>pMHC-I 결합데이터는 peptide 서열과 MHC-I 분자의 contact residue 서열을 concat하여 하나의 문장으로 encoding 될 수 있다. 여기서 concat 된 아미노산 서열에서 아미노산들은 word들로 treated 되어 질 수 있다(Figure-&gt;).
우리는 HLA-bound 사전훈련 데이터셋부터 20개의(or 22(Sec, Pyl)) 단어로 구성된 vocab을 갖는 하나의  sentence corpus를 구축할 수 있다.
</Text>
            <Notes>
By formulating protein data as standard sequence data like sentences in a text corpus, standard NLP algorithms can be readily applied. More concretely, individual peptides are treated as individual sentences and amino acids are treated as words. In this article, the skip-gram model is used with a context window of size 5, 5 negative samples, and 15-dimensional vector space embedding. Various other dimensional size were explored, however, 15-dimen- sions gave the best results on 10-fold cross-validation of HLA- A*02:01 subtype. The entire post-processed dataset by Luo et al. (2016) was used to learn this new distributed representation. The 15-dimensional vector space distributed representation, HLA-Vec, is summarized in Table 1 
</Notes>
        </Document>
        <Document ID="8038CBD4-1CDC-4D44-9880-E585378565EA">
            <Title>External test datasets</Title>
            <Text>기존 논문에서 독립 검증에서 사용되었던 외부 데이터셋을 사용한다</Text>
        </Document>
        <Document ID="D745CF75-4F0B-478C-A4A7-730300A5E22C">
            <Title>BassaniSternberg2016</Title>
            <Text>- {BassaniSternberg:2016kt}: Bassani-Sternberg, M. et al. Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry. Nat Commun 7, 185–16 (2016).
- Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry
- Data: https://www.nature.com/articles/ncomms13404#Sec32</Text>
        </Document>
        <Document ID="EAC34DAC-B9D9-4622-A2C8-A702C718BF69">
            <Title>Loss curve in Binding Affinity Prediction</Title>
            <Text>#</Text>
        </Document>
        <Document ID="8B672601-17EB-4F96-9EF3-D558685F6AB5">
            <Title>Loss curve in Masked LM</Title>
            <Text>#</Text>
        </Document>
        <Document ID="3C7848E8-8DE4-4FDC-88D7-B655EAC58A0A">
            <Title>Pre-traing model</Title>
            <Text>pMHC-I 결합 언어모델은 were pre-trained using two tasks, such as masked language model, adding prior knowledge and binding affinity prediction, in semi-supervised manner as did in BERT model, 
Masking task에서의 loss는 cross entropy loss이고 binding affinity prediction task는 mean square loss가 사용되었고 total loss는 두 loss의 합이 사용되었다.</Text>
            <Notes>Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1. 
Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. 
In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. 
Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. 
</Notes>
        </Document>
        <Document ID="394FE7AE-392D-48B6-A54C-AA1FBE421B44">
            <Title>전이학습</Title>
        </Document>
        <Document ID="59E9493E-5006-4A8F-ABA2-361A1FC1C3E0">
            <Title>Sarkizova(Abelin Extension)</Title>
        </Document>
        <Document ID="FC045630-47F5-4117-BD12-2A682CEE275A">
            <Title>NIHMS905283-supplement-1</Title>
        </Document>
        <Document ID="4DF57EC0-4186-4549-9E41-A4CAAB17DE4A">
            <Title>sydney_comb_smm</Title>
            <Notes>1.	Sidney, J. et al. Quantitative peptide binding motifs for 19 human and mouse MHC class I molecules derived using positional scanning combinatorial peptide libraries. Immunome Res 4, 2–14 (2008).</Notes>
        </Document>
        <Document ID="7D7AD2F1-C237-444C-99AD-BBE43F763A8D">
            <Title>Kim2014</Title>
            <Text>The 186,281 quantitative half-max? (IC50) measurements in {Kim:2014jg} were denoted as a binder if  &lt; 500 nM.
</Text>
        </Document>
        <Document ID="4FBEB562-03E9-4E9B-AE15-D0BE79F051B1">
            <Title>Immunogenic T-cell epitope 예측의 한계점</Title>
            <Text>Presented pMHC complex 중 약 5% 미만의 펩타이드 만이 TCR에 의해 인지되고 T 세포의 면역반응을 유도할 수 있다.
현재까지 알려진 예측 도구들 대부분은 BA + LP 데이터만을 학습한 것들이기 때문에 immunogenic epitope을 예측하는 데 한계가 있다.
현재까지 알려진 immunogenic peptide의 데이터 수는 수만개 정도로 신뢰성 높은 예측 모델을 구축하기에 한계가 있다.
</Text>
        </Document>
        <Document ID="D2170201-5D37-4BE8-98B7-D495653C3F95">
            <Title>Five-fold CV results</Title>
        </Document>
        <Document ID="2149A706-900B-401D-8B2A-35F27D7482D2">
            <Title>기존예측 방법의 한계</Title>
        </Document>
        <Document ID="33AE285F-A332-4989-A3FB-A8810C82BEEA">
            <Title>BulikSullivan2018</Title>
            <Text>{BulikSullivan:2018cb}: 1.	Bulik-Sullivan, B. et al. Deep learning using tumor HLA peptide mass spectrometry datasets improves neoantigen identification. Nature Biotechnology 37, 1–17 (2018).

#

- Data: N/A</Text>
        </Document>
        <Document ID="0A607A33-4230-4BAE-B938-85C6E2483EBB">
            <Title>Input Embedding</Title>
            <Text>pMHC-I 결합에서의 아미노산 서열(peptide sequence + MHC contact residue sequence)는 아미노산들에 해당하는 단어들로 구성된 하나의  ‘sentence’로 인코딩된다.  For a given ‘sentence’, the input embeddings of the BERT-based language model are the sum of the token embeddings and the position embeddings, where the position embeddings are used to inject some information about the relative or absolute position of the tokens in the sequence as used in original BERT model[{Devlin:2018uk}]. An input embedding matrix의 사이즈는 49 x emb_dim이다. 여기서 49는 문장의 최대길이, 즉 최대 peptide 길이(=15) + MHC contact residue의 서열길이(=56)[{Nielsen:2007ga} + {Luo:2016iw}]이고 emb_dim는 embedding dimension이다.</Text>
            <Notes>- Token: 20개 아미노산
- Sentence: 펩타이드 서열 + MHC 결합사이트의 residue들에 대한 pseudo-sequence
- Position Encoding that used in BERT model


Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one token sequence. Throughout this work, a “sentence” can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the in- put token sequence to BERT, which may be a single sentence or two sentences packed together. 
We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ∈ RH, and the final hidden vector for the ith input token asTi ∈RH. 
For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2. 
</Notes>
        </Document>
        <Document ID="CE130A60-033E-4A6B-8FE8-FD96F2C91CED">
            <Title>Masked LM for adding prior knowledge</Title>
            <Text>Masked LM task를 통하여 pMHC-I 결합에 대응하는 문장 내에서의 단어들(amino acids)간의 깊은 양방향의 contextual relationship을 선행 학습시킬 수 있다. 선행학습에서의 masking procedure은 입력 문장의 15%의 토큰들을 임의로 선택하여 80% 확률로 [MASK] 토큰으로 치환하거나(80% of the time: Replace the word with the [MASK] token), 10%의 확률로 임의 단어로 치환하거나(10% of the time: replace the word with a random word), 10%의 확률로 unchange(10% of the time: Keep the word unchanged). Then, a masked token will be used to predict the original token with cross entropy loss. 임의의 단어를 replace할 때, 펩타이드 서열에서의 위치별 아미노산 선호도와 MHC 결합사이트에서의 진화적 서열 보존 등과 같은 prior knowledge를 부여한다. 펩타이드 서열에서의 아미노산 치환은 PSSM(position-specific score matrix)[{Peters:2005tu}, {Kim:2009dd}]을 기반으로 하고, MHC 분자의 결합사이트에서의 아미노산 치환은 BLOSUM50을 사용한 진화적 perspective를 기반으로 한다[{Henikoff:1992jn}].
</Text>
            <Notes>Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1. 
Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. 
In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. 
Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. 

A A.1 
Additional details for our experiments are presented in Appendix B; and 
Additional ablation studies are presented in Appendix C. 
We present additional ablation studies for BERT including: 
– Effect of Number of Training Steps; and – Ablation for Different Masking Proce- 
dures. 
Additional Details for BERT Illustration of the Pre-training Tasks 
We provide examples of the pre-training tasks in the following. 
Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by 
	•	80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy → my dog is [MASK]  
	•	10% of the time: Replace the word with a randomword,e.g.,my dog is hairy → my dog is apple  
	•	10% of the time: Keep the word unchanged,e.g.,my dog is hairy → my dog is hairy. The purpose of this is to bias the representation towards the actual observed word. 
	•	 The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.  Compared to standard languge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. 

Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. 
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] 
Label = IsNext 
Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] 
Label = NotNext 
￼



</Notes>
        </Document>
        <Document ID="36A13F9E-A5A9-4F47-B0BD-127C33E40D9D">
            <Title>NetMHCPan40</Title>
            <Text>Validation on external data sets 
A data set of ELs was obtained from Pearson et al. [{Pearson:2016cm}]. Also, a set of positive CD8 epitopes was downloaded from the IEDB. The epitope set was identified using the following search criteria “T cell assays: IFN-g,” “positive assays only,” “MHC restriction Type: Class I.” Only entries with fully typed HLA restriction, peptides length in the range 8–14 aa, and with annotated source protein sequence were included. Positive entries with a predicted rank score . 10% using NetMHCpan-3.0 were excluded to filter out likely noise (6). For the T cell epitope and EL data sets, negative peptides were obtained by extracting all 8–14-mer peptides from the source proteins of the ELs and subsequently excluding peptide–MHC combinations found with an exact match in the training data (BA and EL data sets). The final eluted data set contained 15,965 positive ligands restricted to 27 HLA molecules, and the IEDB T cell epitope data set contained 1,251 positive T cell epitopes restricted to 80 HLA molecules. 
A Frank value was calculated for each positive HLA pair as the ratio of the number of peptides with a prediction score higher than the positive peptide/the number of peptides contained within the source protein. Hence, the Frank value is 0 if the positive peptide has the highest prediction value of all peptides within the source protein and a value of 0.5 in cases in which an equal amount of peptides has a higher and lower prediction value compared with the positive peptide. 
An unfiltered EL data set was obtained from Bassani-Sternberg et al.[{BassaniSternberg:2015js}]. This data set consisted of EL data from six cell lines, each with fully typed HLA-A, -B, and -C alleles. A data set was constructed for each cell line, including all 8–13-mer ligands (N) as positives, and five times N random natural negatives for each length of 8–13 aa (i.e., if a data set contained 5,000 ligands, 5 3 5,000 = 25,000 random natural peptides of length 8, 9, 10, 11, 12, and 13 aa were added as negatives, arriving at a final data set with 155,000 [5,000 + 6 3 25,000] peptides). 


#
{BassaniSternberg:2015js}:Table1

Results: NIHMS905283-supplement-1</Text>
            <Notes>These data sets were obtained from Bassani-Sternberg et al. (22) and include ELs obtained from six cell lines, each with typed HLA expression. From these data, we constructed six benchmark data sets by enriching each positive EL data set with a set of random natural negative peptides (for details see Materials and Meth- ods). After filtering out data included in the training data of NetMHCpan-4.0, we next benchmarked the predictive power of the different prediction methods. The result of the benchmark is shown in Fig. 9. 
</Notes>
        </Document>
        <Document ID="DE448CE6-C593-4EA9-88B1-E161BD26936E">
            <Title>IEDB</Title>
            <Text>

</Text>
            <Notes>The IEDB entries were filtered to MHC allele class = I, Epitope Object Type = Linear peptide and Allele Name consistent with human HLA class I nomenclature with four-digit typing (that is, regex: “^HLA-[ABC]\\*[0–9]{2}:[0–9]{2}$”). Peptides with quantitative measurements in units other than nM were removed and so were the following three assay types due to detected inconsistency between predicted (NetMHC 3.0) and actual affinity: ‘purified MHC/direct/radioactivity/dissociation constant KD’, ‘purified MHC/direct/fluorescence/half maximal effective concentration (EC50)’ and ‘cellular MHC/direct/fluorescence/ half maximal effective concentration (EC50)’. A peptide was considered a binder if it had a quantitative affinity of &lt;500 nM or qualitative label of ‘Positive’, ‘Positive-High’, ‘Positive-Intermediate’ or ‘Positive-Low’. In cases where multiple records are available for the same {peptide, allele} pair, we either took the mean affinity or removed the peptide when the difference between the maximum and minimum log-transformed affinities (1 − log(nM)/log(50,000)) was &gt;0.2. Similarly, peptides with multiple qualitative records were removed if the same number of positive and negative labels were found or kept otherwise.</Notes>
        </Document>
        <Document ID="B7EFF44C-3867-4AA1-B15B-9C6148485B03">
            <Title>datasets</Title>
        </Document>
        <Document ID="4244605B-4D50-46CB-A079-57A6B5F0A0A5">
            <Title>Source</Title>
            <Text>The MS-identified EL dataset consists of 226,684 ligands formed by combining 186,415 ligands from IEDB with 39,741 additional ligands from SysteMHC Atlas [{Shao:2017hb}] and 530? additional ligands from [{Abelin:2017cn}] and [{BassaniSternberg:2016kt}]
The large pepidome  dataset whice includes 186,464 MS-identified peptides eluted from 95 HLA-A, -B, -C and -G mono-allelic cell lines[{Sarkizova:2019fu}]
</Text>
            <Notes>1.	Abelin, J. G. et al. Mass Spectrometry Profiling of HLA-Associated Peptidomes in Mono-allelic Cells Enables More Accurate Epitope Prediction. Immunity 46, 315–326 (2017).
1.	Bassani-Sternberg, M. et al. Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry. Nat Commun 7, 185–16 (2016).
</Notes>
        </Document>
        <Document ID="211FB603-98F0-43EF-8F6C-4C6550579C7E">
            <Title>Research idea</Title>
        </Document>
        <Document ID="C7074FDC-AA73-48AE-92DF-5E3AE5305201">
            <Title>Contextual-free interactions</Title>
            <Text>펩타이드-MHC 결합에서의 아미노산 간의 상호작용 패턴- 장단거리 상호작용, 집합적 상호작용, 일반적 상호작용 등-을 펩타이드 길이와 상관없이 효과적으로 학습해 낼 수 있을 것이다</Text>
        </Document>
        <Document ID="7137F6B7-8ECF-4B97-84B6-1D2930871F1F">
            <Title>Classical methods</Title>
            <Text>Motif-based
PSSM
Machine learning-based</Text>
        </Document>
        <Document ID="22D1321D-0D74-4189-83E8-87583C00EC81">
            <Title>Abelin2017</Title>
            <Text>MS-identified peptides eluted from MHC class I
 - {Abelin:2017cn}: Abelin, J. G. et al. Mass Spectrometry Profiling of HLA-Associated Peptidomes in Mono-allelic Cells Enables More Accurate Epitope Prediction. Immunity 46, 315–326 (2017).

 - MS identified HLA-bound peptides; presented on the cell
 - HLA-bound peptides can be directly identified via immunopurification and LC-MS/MS. We processed class I HLA-deficient cell lines (30 million–90 million B721.221-derived cells), each stably transduced to express one of 16 different class I HLA alleles
 - Data: /Users/hym/projects/nplm/.data/mhcflurry/curated_training_data.with_mass_spec.csv</Text>
        </Document>
        <Document ID="06054169-6534-4549-B459-CF3477B02344">
            <Title>Fine-tuning and evaluating the model</Title>
            <Text>선행학습된 언어모델은 MS-identified EL 데이터셋을 사용하여 fine-tuning 되었다. MS-identified EL 데이타는 positive-high, positive, positive-intermediate, positive-low, and negative로 label 된 정성적 affinity 데이터이므로 pre-trained 언어모델의 last output layer for binding affinity prediction를 4개의 label에 대한 one-hot encoding layer로 replace  한 후 fine-tuning을 수행하였다.
Decoy(negative) 데이터는 임의로 생성하였다(?). NetMHCpan, MHCflurry 등에서 random으로 negative 데이터를 생성했던 방법대로 하였다?

Training for fine-tuning the pre-trained model was carried out in standard 5-fold cross-validation. The training dataset were split into 5 equal sized subset in 5-fold cross-validation. For each CV round, a single subset was retained as the validation data for testing current model, and the remaining subsets were used for training current model. In a single cross-validation round, training-validation was repeated for maximum of 200 epochs. The training and validation losses were measured for each epoch, and the training process was stopped early at the epoch in which the validation loss had not been decreased for 15 consecutive epochs [{Prechelt:2012ct}]. Training proceeds with the Adam optimizer[{Kingma:2014us}] using a minibatch size of 128.
</Text>
        </Document>
    </Documents>
</SearchIndexes>